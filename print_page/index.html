
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="AI on Power - Level 3 course">
      
      
        <meta name="author" content="Deepak C Shetty, Learning Content Developer, IBM Power">
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Print Course - AI on Power - Level 3</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"IBM Plex Sans";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings4.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings5.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings6.css">
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#helpful-tips" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AI on Power - Level 3" class="md-header__button md-logo" aria-label="AI on Power - Level 3" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI on Power - Level 3
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Course
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AI on Power - Level 3" class="md-nav__button md-logo" aria-label="AI on Power - Level 3" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AI on Power - Level 3
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../helpful-tips/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting started
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre-req/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prerequisites
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lab-setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab setup instructions
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lab1 - Deploy a LLM on Power10
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Lab1 - Deploy a LLM on Power10
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab1/lab1-overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab education
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab1/lab1-hands-on-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on guide
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lab2 - Deploy RAG on Power10
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Lab2 - Deploy RAG on Power10
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab2/lab2-overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab education
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab2/lab2-hands-on-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on guide
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lab3 - Deploy code LLM on Power10
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Lab3 - Deploy code LLM on Power10
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab3/lab3-overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab education
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab3/lab3-hands-on-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on guide
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../resources/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Additional resources
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../credits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Credits
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../support/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Support
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Print Course
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Print Course
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#helpful-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Getting started
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pre-req" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Lab setup instructions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab1-deploy-a-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab1 - Deploy a LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab1 - Deploy a LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab2-deploy-rag-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab2 - Deploy RAG on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab2 - Deploy RAG on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab3-deploy-code-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab3 - Deploy code LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab3 - Deploy code LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    <span class="md-ellipsis">
      Additional resources
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#credits" class="md-nav__link">
    <span class="md-ellipsis">
      Credits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#support" class="md-nav__link">
    <span class="md-ellipsis">
      Support
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#helpful-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Getting started
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pre-req" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Lab setup instructions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab1-deploy-a-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab1 - Deploy a LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab1 - Deploy a LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab2-deploy-rag-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab2 - Deploy RAG on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab2 - Deploy RAG on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab3-deploy-code-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab3 - Deploy code LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab3 - Deploy code LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    <span class="md-ellipsis">
      Additional resources
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#credits" class="md-nav__link">
    <span class="md-ellipsis">
      Credits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#support" class="md-nav__link">
    <span class="md-ellipsis">
      Support
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<div id="print-site-page" class="print-site-enumerate-figures">
        <section id="print-site-cover-page">
            
<div>

    
        <h1>AI on Power - Level 3</h1>
    

</div>


<table>

    
    <tr>
        <td>Description</td>
        <td>AI on Power - Level 3 course</td>
    </tr>
    

    
    <tr>
        <td>Author(s)</td>
        <td>Deepak C Shetty, Learning Content Developer, IBM Power</td>
    </tr>
    

    

    
    <tr>
        <td>Copyright</td>
        <td>Copyright &copy; 2024 IBM</td>
    </tr>
    

</table>





        </section>
        
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
</p>
<p>This page has combined all site pages into one. You can export to PDF using <b>File > Print > Save as PDF</b>.</p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="6">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index"><div><h1 id="introduction">Introduction<a class="headerlink" href="#index-introduction" title="Permanent link">¶</a></h1>
<p>Welcome to the AI on Power - Level 3 course - seller enablement demonstration.
This Level 3 course will provide hands-on enablement on how to use generative AI (gen AI) models on Power10 and will cover few use-cases to help understand the art of the possible.</p>
<p>There are 4 main parts to this demonstration as you can see on the left panel:</p>
<ul>
<li>
<p>Lab setup - How to provision and setup the lab env for the hands-on demos.</p>
</li>
<li>
<p>Lab 1 – Deploy a Large Language Model (LLM) on Power10 - Deploy a LLM on Power10 and switch to a different LLM.</p>
</li>
<li>
<p>Lab 2 – Deploy Retrieval-Augmented Generation (RAG) on Power10 - RAG chatbot for domain specific question and answers.</p>
</li>
<li>
<p>Lab 3 – Deploy code LLM Power10 - Generate python, C code and Sequential Query Language (SQL) query using natural language.</p>
</li>
</ul>
<h2 id="index-generative-ai-and-llms-overview">Generative AI and LLMs overview<a class="headerlink" href="#index-generative-ai-and-llms-overview" title="Permanent link">¶</a></h2>
<p>Large language models (LLMs) and gen AI have revolutionized industries by enabling machines to understand, generate, and interact with human language in ways never seen before. They have disrupted the market by:</p>
<ul>
<li>
<p><strong>Boosting productivity</strong>: LLMs streamline workflows by handling complex tasks, like writing, summarizing, and translating, allowing professionals to focus on more strategic, creative efforts.</p>
</li>
<li>
<p><strong>Automating content creation</strong>: From drafting articles, reports, and code to generating creative content like music and artwork, gen AI has drastically reduced time and costs in creative industries.</p>
</li>
<li>
<p><strong>Enhancing customer experience</strong>: LLM-powered chatbots and virtual assistants are offering more personalized, human-like interactions, reshaping customer service and support.</p>
</li>
<li>
<p><strong>Transforming decision-making</strong>: With advanced data processing and natural language understanding, businesses are leveraging AI for smarter, faster insights and predictive analytics. </p>
</li>
</ul>
<p>In short, LLMs and gen AI are reshaping industries by enhancing efficiency, creativity, and customer engagement, making them indispensable tools in the modern business landscape.</p>
<h2 id="index-why-ibm-power-for-generative-ai">Why IBM Power for generative AI<a class="headerlink" href="#index-why-ibm-power-for-generative-ai" title="Permanent link">¶</a></h2>
<p>IBM Power platform is a high-performance, enterprise-grade computing solution designed to handle demanding workloads such as AI, analytics, cloud, and mission-critical applications. With unmatched processing power, scalability, and reliability, IBM Power enables organizations to optimize performance, lower costs, and ensure security.</p>
<h3 id="index-key-benefits">Key benefits<a class="headerlink" href="#index-key-benefits" title="Permanent link">¶</a></h3>
<ul>
<li>
<p><strong>Unmatched performance</strong>: Optimized for data-intensive applications, AI, and large-scale analytics, delivering faster insights and better decision-making.</p>
</li>
<li>
<p><strong>Scalability</strong>: Seamlessly scale workloads across on-premises, cloud, and hybrid environments to meet evolving business needs.</p>
</li>
<li>
<p><strong>Enterprise-grade security</strong>: Built-in security features provide robust protection for sensitive data and critical operations.</p>
</li>
<li>
<p><strong>Efficiency and flexibility</strong>: Designed for diverse environments, supporting Linux, AIX, IBM i, and containerized applications, making it versatile for a range of industries.</p>
</li>
</ul>
<p>IBM Power is the platform of choice for businesses seeking to future-proof their IT infrastructure, handle complex workloads, and accelerate innovation.</p>
<h2 id="index-ibm-power10-differentiation">IBM Power10 differentiation<a class="headerlink" href="#index-ibm-power10-differentiation" title="Permanent link">¶</a></h2>
<p>IBM Power10 offers several advantages for gen AI use cases, providing the computational power, security, and efficiency required to support the demanding workloads of AI-driven applications.</p>
<h3 id="index-key-advantages">Key advantages<a class="headerlink" href="#index-key-advantages" title="Permanent link">¶</a></h3>
<ol>
<li>
<p><strong>AI acceleration:</strong> IBM Power10 comes with several features that accelerate gen AI workloads using high memory bandwith and on-chip acceleration:</p>
<ul>
<li>
<p><strong>High bandwidth data-path:</strong> As data fuels AI, a fast access to big data for training and inferencing is required. Most AI workloads are memory bandwith bound. IBM Power10's large memory capacity that, in general exceeds limited Graphics Processing Unit (GPU) memory and IBM Power10’s high memory bandwidth are optimal for such scenarios. In addition, the Double Data Rate 5 (DDR5) memory and faster Input/Output (I/O) bandwidth provide a significant performance boost, enabling large-scale gen AI models to run more efficiently.</p>
</li>
<li>
<p><strong>4 Matrix Math Accelerator (MMA) engines per core:</strong> MMA does matrix math and helps accelerate matrix multiplications, which are required for training, fine-tuning, and inferencing of AI models such as Neural Networks and foundation models. IBM is seeing strong evidence that it supersedes GPUs and improves consolidation when deploying AI at point of use. </p>
</li>
<li>
<p><strong>8 Single Instruction Multiple Data (SIMD) engines per core:</strong> SIMD does vector math. Vectors are used in most AI algorithms. Vector processing can be highly parallelized using SIMD engines. A single inference request is often “just” a vector that is “send through” the LLM using SIMD acceleration. By batching multiple inference requests together, the workload can become MMA-bound where a whole matrix is "send through" the LLM, leading to improved throughputs.</p>
</li>
</ul>
</li>
<li>
<p><strong>Optimized AI sofware:</strong> AI software with IBM Power10 is optimized down to the core.</p>
<ul>
<li>AI software running on top of IBM Power10 hardware fully leverages the above mentioned acceleration features, without requiring data scientists to alter their code – optimization comes out-of-the-box!</li>
<li>The optimized AI software portfolio spans from enterprise options to supported open-source options; even hybrid approaches that mix enterprise and supported open-source are possible.</li>
<li>This flexibility allows solution architects to adapt the AI software portfolio to the unique requirements of their company. For example, if a company’s data scientists already use a set of open-source tools, they can continue to do so while benefitting from all optimizations in IBM Power10 and paving the road to integrations with the enterprise portfolio.</li>
</ul>
</li>
<li>
<p><strong>Support for AI ecosystems:</strong> IBM Power10 works seamlessly with AI frameworks like TensorFlow, PyTorch and ONNX runtime, making it easier for developers to build, train, and deploy gen AI models without needing major infrastructure changes.</p>
<ul>
<li>These AI frameworks leverage some of the popular math libraries like OpenBLAS, libAten, Eigen and MLAS which provide reusable function for matrix multiplication.</li>
<li>IBM has already integrated Power10's hardware acceleration capabilities into these math libraries, thus allowing AI workloads using these frameworks to automatically get the IBM Power10 speed-up without any code changes.</li>
<li>Support for container orchestration platforms like OpenShift and Kubernetes allows efficient orchestration of AI workloads.</li>
</ul>
</li>
<li>
<p><strong>Supersede GPUs:</strong> Integrating GPU clusters into computing centers is a daunting and complex task. CUDA drivers need to be installed and managed. Hardware faults regularly cause systems to crash. GPUs are expensive and consume lots of energy. Using multiple GPUs and distributed training on GPUs is complicated and buggy. By consolidating on IBM Power10, these are all problems of the past while maintaining low latencies and a high throughput for AI workloads. </p>
</li>
<li>
<p><strong>Energy efficiency:</strong> Power10 processors are highly energy-efficient, offering more performance per watt compared to previous generations. This reduces operational costs and is especially beneficial for the resource-intensive training and deployment of gen AI models.</p>
</li>
<li>
<p><strong>Security:</strong> IBM Power10 comes with end-to-end encryption and advanced memory protection features, ensuring that sensitive AI workloads are secure from potential breaches. This is particularly important in industries like healthcare and finance, where gen AI applications need strong data protection. IBM's support for quantum-safe encryption helps future-proof AI workloads against quantum computing threats.</p>
</li>
<li>
<p><strong>Flexible cloud and hybrid deployments:</strong> Power10 integrates smoothly with hybrid cloud environments, allowing enterprises to run gen AI workloads both on-premises and in the cloud. This flexibility is key for scaling AI applications while maintaining control over data and infrastructure.</p>
</li>
</ol>
<h2 id="index-summary"><strong>Summary</strong><a class="headerlink" href="#index-summary" title="Permanent link">¶</a></h2>
<p>IBM Power10 is designed to meet the specific needs of gen AI use cases by offering high-performance processing, memory scalability, security, and energy efficiency. It empowers enterprises to handle complex AI workloads efficiently while maintaining flexibility and security, making it ideal for industries that rely on AI-driven innovation.</p></div></section><section class="print-page" id="pre-req"><div><h1 id="prerequisites-for-the-course">Prerequisites for the course<a class="headerlink" href="#pre-req-prerequisites-for-the-course" title="Permanent link">¶</a></h1>
<p>The following are the prerequisite(s) for this course:</p>
<ul>
<li>
<p>AI on Power - Level 2 course - <a href="https://yourlearning.ibm.com/activity/PLAN-95E47B97CBB5" target="_blank">IBM</a> | <a href="https://learn.ibm.com/course/view.php?id=16329" target="_blank">BP</a></p>
</li>
<li>
<p>Basic understanding of Generative AI (gen AI) and Large Language Models (LLM).</p>
</li>
<li>
<p>Basic understanding of OpenShift and working with Yet Another Markup Language (YAML) files. </p>
<ul>
<li>While sufficient instructions are provided through-out the course, it is highly recommended to have a basic understanding of OpenShift and working with YAML files.</li>
</ul>
</li>
<li>
<p>Laptop with Secure Shell (ssh) pre-installed to connect to lab environment.</p>
<ul>
<li>Windows: PowerShell should have ssh pre-installed. PuTTY tool can also be used.</li>
<li>Mac/Linux: ssh should be already available.</li>
</ul>
</li>
</ul></div></section><section class="print-page" id="lab-setup"><div><h1 id="lab-setup">Lab Setup<a class="headerlink" href="#lab-setup-lab-setup" title="Permanent link">¶</a></h1>
<h2 id="lab-setup-provisioning-the-environment">Provisioning the environment<a class="headerlink" href="#lab-setup-provisioning-the-environment" title="Permanent link">¶</a></h2>
<p>For the hands-on labs, we will be using the OpenShift on Power10 on-premises environment which is optimized for AI workloads and hosted on IBM TechZone.</p>
<p>Follow the steps below:</p>
<ol>
<li>
<p>Open <a href="https://techzone.ibm.com/collection/generative-ai-demos-on-ibm-power/environments" target="_blank">this</a> TechZone collection and provision the environment named "OpenShift ready for AI on IBM Power10 (Container PaaS)" by clicking on <strong>Reserve</strong> and submitting the resulting form (select <strong>Education</strong> as purpose)</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/7e141798-b557-4584-928d-7ad0ecd24c8c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/7e141798-b557-4584-928d-7ad0ecd24c8c"></a></p>
</li>
<li>
<p>Watch your email for updates from TechZone and wait for your environment to be provisioned.   </p>
</li>
<li>
<p>Once provisioned, go to <a href="https://techzone.ibm.com/my/reservations" target="_blank">my reservations</a> page to ensure its in "Ready" state.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/18e24261-143b-4912-b8ec-285fc6fce394" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/18e24261-143b-4912-b8ec-285fc6fce394"></a></p>
</li>
</ol>
<h2 id="lab-setup-accessing-the-environment">Accessing the environment<a class="headerlink" href="#lab-setup-accessing-the-environment" title="Permanent link">¶</a></h2>
<ol>
<li>As this is an on-premises environment, verify you are connected to the IBM Virtual Private Network (VPN) to access the environment. Refer to <a href="https://github.com/IBM/itz-support-public/blob/main/IBM-On-premise/IBM-On-premise-Runbooks/configure-vpn.md" target="_blank">this</a> link for more details.</li>
<li>In TechZone, open the <a href="https://techzone.ibm.com/my/reservations" target="_blank">My Reservations</a> page.</li>
<li>Click on your reservation, which will open up the details page.</li>
<li>Scroll to the "Reservation Details" section of the page which has information on how to connect to OpenShift console.
   <a class="glightbox" href="https://github.com/user-attachments/assets/9e7df820-6a8b-4cc6-9ca3-b2a8cdc7decb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9e7df820-6a8b-4cc6-9ca3-b2a8cdc7decb"></a></li>
</ol>
<h3 id="lab-setup-accessing-openshift-console">Accessing OpenShift console<a class="headerlink" href="#lab-setup-accessing-openshift-console" title="Permanent link">¶</a></h3>
<ol>
<li>In the "Reservation Details" section of the TechZone environment details page, click on the OpenShift console link.</li>
<li>This will open up a new browser tab/window and opens up the OpenShift console login page.<ul>
<li>If you encounter any security exception, navigate to the bottom of the browser page, acccept the exception under Advanced and continue. This is ok as we are in a lab/demo environment and using self-signed certificates.</li>
</ul>
</li>
<li>On the OpenShift console page, select the <strong>htpasswd</strong> login option.</li>
<li>
<p>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code></p>
<ul>
<li>TIP: Click on the copy icon provided under 'User Password' in the Reservation Details page to copy the password and paste it in the OpenShift console window.</li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/b31a361a-b69a-4872-b5a7-a71db2f8f52f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b31a361a-b69a-4872-b5a7-a71db2f8f52f"></a>
 <a class="glightbox" href="https://github.com/user-attachments/assets/2700ebb0-bf81-4f3f-938d-3ae8a48e7473" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2700ebb0-bf81-4f3f-938d-3ae8a48e7473"></a></p>
</li>
<li>
<p>You have successfully logged into the OpenShift cluster using the console. You should be able to see the dashboard (or the page you were on before logging off) of your OpenShift console. You should land in <strong>Administrator</strong> profile (or <strong>Developer</strong> profile if that was the last profile you were in when you logged off).</p>
</li>
</ol>
<p>!!! tip "Tip - Maintain 2 browser windows, one each for Administrator and Developer persona"</p>
<div class="highlight"><pre><span></span><code>   Its a good idea to have 2 browser windows (or tabs per your preference) for OpenShift console access - one with Administrator profile and another with Developer profile because in the hands-on labs we will be needing to switch between these profiles and its easier and efficient to do so with 2 browser windows. To do so, copy the URL from the browser address bar, open a new browser window (or tab) and paste the URL there. It should open up one more OpenShift console in the new window (or tab). In the new window/tab, switch to the Developer profile (also known as Persona) by going to the top left corner and clicking on **Administrator** and selecting **Developer** (or vice-versa) in the drop down menu. In short, ensure you have 2 browser windows, one each with Administrator and Developer profile (also known as Persona) and we will call this OpenShift Administrator console and Developer console respectively.
</code></pre></div>
<video style="width:100%" muted="true" autoplay="true" loop="true" controls="" alt="type:video">
      <source src="https://github.com/user-attachments/assets/a622a195-00a6-4950-b2e5-686b04fa3401" type="video/mp4">
   </source></video>

<h3 id="lab-setup-reauthenticating-for-console">Reauthenticating for console<a class="headerlink" href="#lab-setup-reauthenticating-for-console" title="Permanent link">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">REAUTHENTICATING in case you lose console access</p>
<p>In case you lose access to the OpenShift cluster and need to reauthenticate to the console, which is possible in case your reservation expires and/or your console authentication timed-out, please follow the above steps again to reauthenticate to your OpenShift console.</p>
</div>
<h3 id="lab-setup-accessing-openshift-command-line-interface-cli">Accessing OpenShift Command Line Interface (CLI)<a class="headerlink" href="#lab-setup-accessing-openshift-command-line-interface-cli" title="Permanent link">¶</a></h3>
<p>OpenShift CLIs are accessed using the <code>oc</code> command</p>
<ol>
<li>Go back to the "Reservation Details" section of the TechZone environment details page.</li>
<li>Open a terminal window and use the SSH client to connect to the Bastion node of OpenShift cluster.<ul>
<li><code>ssh -l cecuser &lt;your bastion hostname/IP as provided in Reservation Details section&gt;</code>.</li>
<li>If <code>ssh</code> gives any warning, type <code>yes</code> and continue.</li>
<li>When prompted for password, copy the password from Reservation Details page by clicking on the copy icon and pasting it in the ssh terminal window</li>
</ul>
</li>
<li>You have logged in successfully to the bastion node of your OpenShift cluster.<ul>
<li>Keep this terminal window open as you will be using it frequently to run CLI commands.</li>
</ul>
</li>
<li>
<p><code>oc</code> CLI is pre-installed on the bastion node. Verify by running <code>oc version</code> command.</p>
<ul>
<li>Ignore the error part of the <code>oc version</code> for now. Its as expected since you have not yet logged into the cluster from the CLI.</li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/0e41ba9f-9f36-41d1-89a2-2116babbacdb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/0e41ba9f-9f36-41d1-89a2-2116babbacdb"></a>
  <a class="glightbox" href="https://github.com/user-attachments/assets/576d86f0-8873-492c-8b13-9433c9f25604" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/576d86f0-8873-492c-8b13-9433c9f25604"></a>
  <a class="glightbox" href="https://github.com/user-attachments/assets/770257da-8d44-4d21-9860-7c6200afd3b6" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/770257da-8d44-4d21-9860-7c6200afd3b6"></a></p>
</li>
</ol>
<h3 id="lab-setup-logging-in-to-openshift-cluster-using-oc-cli">Logging in to OpenShift Cluster using <code>oc</code> CLI<a class="headerlink" href="#lab-setup-logging-in-to-openshift-cluster-using-oc-cli" title="Permanent link">¶</a></h3>
<p>Let's login to the OpenShift cluster via the <code>oc</code> CLI. This is needed as we will execute some CLI commands as part of the lab steps.</p>
<ol>
<li>
<p>In the OpenShift console, top right section click on <code>cecuser</code> and select <code>Copy login command</code> option.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/64cf7f76-5bb1-477b-9f2c-45451fc80fa3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/64cf7f76-5bb1-477b-9f2c-45451fc80fa3"></a></p>
</li>
<li>
<p>A new browser window (or tab, depending on your browser setting) opens up.</p>
<ul>
<li>If you encounter any security exception, navigate to the bottom of the browser page, acccept the exception under Advanced and continue. This is ok as we are in a lab/demo environment and using self-signed certificates.</li>
</ul>
</li>
<li>You will be presented with another login screen. Click <strong>htpasswd</strong> option
   <a class="glightbox" href="https://github.com/user-attachments/assets/c3151615-24ba-44ea-8d71-783d39e4ccfb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c3151615-24ba-44ea-8d71-783d39e4ccfb"></a></li>
<li>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code>.<ul>
<li>TIP: Click on the copy icon provided under 'User Password' in the Reservation Details page to copy the password and paste it in the OpenShift console window.</li>
</ul>
</li>
<li>
<p>Click <strong>Display token</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/4af10efe-22fc-4d01-86a7-44e2fbbdd10d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4af10efe-22fc-4d01-86a7-44e2fbbdd10d"></a></p>
</li>
<li>
<p>Copy the <code>oc login --token=...</code> CLI and paste it in the bastion node terminal window.
     <a class="glightbox" href="https://github.com/user-attachments/assets/75ad62a0-d0a0-45f6-8797-fedad6e5877a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/75ad62a0-d0a0-45f6-8797-fedad6e5877a"></a>
     <a class="glightbox" href="https://github.com/user-attachments/assets/a2753a4c-86d6-49ca-96c8-54f3ed7dbac5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a2753a4c-86d6-49ca-96c8-54f3ed7dbac5"></a></p>
</li>
<li>You have successfully logged into the OpenShift cluster using the <code>oc</code> CLI.</li>
<li>
<p>In case you lose access to the <code>oc</code> CLI, you will get an error as below, in which case you need to reauthenticate.<br>
   Refer to the call-out on how to reauthenticate.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/a1e8d00c-64d0-41ab-997c-540378df0544" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a1e8d00c-64d0-41ab-997c-540378df0544"></a></p>
</li>
</ol>
<h3 id="lab-setup-reauthenticating-for-cli-access">Reauthenticating for CLI access<a class="headerlink" href="#lab-setup-reauthenticating-for-cli-access" title="Permanent link">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">REAUTHENTICATING in case you lose CLI access</p>
<p>In case you lose access to the OpenShift cluster and need to reauthenticate using the CLI, which is possible in case your reservation expires and/or your CLI window terminated for some reason, please follow the above steps again to get back your <code>oc</code> CLI authenticated to the OpenShift cluster.</p>
</div>
<h3 id="lab-setup-summary">Summary<a class="headerlink" href="#lab-setup-summary" title="Permanent link">¶</a></h3>
<p>Efforts are made to keep the lab instructions simple and easy to follow to cater to audience of all skill levels.
We strive to use OpenShift console as much possible, but in some scenarios OpenShift console doesn't yet support some functionality in which case we switch to the <code>oc</code> CLI. Hence this lab will use OpenShift console and <code>oc</code> CLI both as necessary.</p></div></section>
                        <h1 class='nav-section-title' id='section-lab1-deploy-a-llm-on-power10'>
                            Lab1 - Deploy a LLM on Power10 <a class='headerlink' href='#section-lab1-deploy-a-llm-on-power10' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="lab1-lab1-overview"><div><h1 id="deploy-a-large-language-model-llm-on-ibm-power10-lab-education">Deploy a Large Language Model (LLM) on IBM Power10 - Lab Education<a class="headerlink" href="#lab1-lab1-overview-deploy-a-large-language-model-llm-on-ibm-power10-lab-education" title="Permanent link">¶</a></h1>
<p>Goal of this lab is to showcase the basics (and the ease) of deploying a LLM (also known as foundation models) on Power and how easy it is to switch to a different LLM.
But before we do that, let's understand the ecosystem around LLMs (also known as foundation models or Generative AI) in the context of IBM Power.</p>
<p>In this lab, we will deploy 2 LLMs from <a href="https://huggingface.co/" target="_blank">Hugging Face (HF)</a> on IBM Power10 using the on-premises lab environment that has been provisioned.</p>
<h2 id="lab1-lab1-overview-what-is-hugging-face">What is Hugging Face?<a class="headerlink" href="#lab1-lab1-overview-what-is-hugging-face" title="Permanent link">¶</a></h2>
<p>Hugging Face is an AI company that built an incredibly popular AI community around open source libraries, models, and data sets.
It's an open source community with a large collection of AI models and datasets that are available for sharing and collaboration.
The Hugging Face hub now hosts more than 950K models and over 210K data sets, and those numbers are growing quickly.</p>
<div class="admonition note">
<p class="admonition-title">What is Hugging Face?</p>
<p>Hugging Face is an AI company that has become a major hub for Natural Language Processing (NLP) and Machine Learning (ML) tools. It is widely known for its open-source library, Transformers, and its collaborative platform, which provides a vast collection of pre-trained models and datasets. Hugging Face has made it easy for developers, researchers, and businesses to access, fine-tune, and deploy state-of-the-art machine learning models, especially those related to NLP tasks like text classification, translation, summarization, and more.</p>
</div>
<p>We use HF to access LLMs in this lab as it has a vast collection of pre-trained models which are hosted publicly and can be accessed for free.</p>
<h2 id="lab1-lab1-overview-what-is-ibm-watsonx">What is IBM watsonx?<a class="headerlink" href="#lab1-lab1-overview-what-is-ibm-watsonx" title="Permanent link">¶</a></h2>
<p>Enterprise clients of IBM will use the <a href="https://www.ibm.com/watsonx" target="_blank">IBM watsonx platform</a>, an enterprise-ready AI and data platform.
IBM watsonx is designed for the enterprise and is targeted for business domains, empowering value creators to transform business applications into AI-first applications.</p>
<div class="admonition note">
<p class="admonition-title">What is IBM watsonx?</p>
<p>IBM watsonx is IBM’s next-generation AI and data platform, designed to help enterprises build, train, fine-tune, and deploy large-scale AI models efficiently. watsonx enables businesses to harness the power of artificial intelligence and machine learning (AI/ML) for various tasks like generative AI, predictive analytics, and decision-making. It integrates advanced capabilities for handling large language models (LLMs), machine learning workflows, and AI governance.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">IBM watsonx support on IBM Power</p>
<p><strong>NOTE</strong>: At the time of writing this lab, IBM watsonx is not available to run on IBM Power yet. However, clients can use foundation models (either hosted by watsonx on IBM Cloud or running stand-alone on-premises on IBM Power) to infuse and harness the power of (Gen)AI into their on-premises applications running on IBM Power.</p>
</div>
<h2 id="lab1-lab1-overview-ibm-watsonx-vs-hugging-face">IBM watsonx Vs Hugging Face<a class="headerlink" href="#lab1-lab1-overview-ibm-watsonx-vs-hugging-face" title="Permanent link">¶</a></h2>
<p>IBM watsonx and Hugging Face both offer AI tools, but they serve different purposes:</p>
<ul>
<li>
<p><strong>IBM watsonx</strong>: An enterprise-grade AI platform designed for large-scale AI model training, fine-tuning, and deployment. It focuses on governance, compliance, and custom AI models for business use, offering robust tools for data management and AI governance. Ideal for enterprises needing secure, scalable AI with industry-specific solutions.</p>
</li>
<li>
<p><strong>Hugging Face</strong>: A community-driven platform offering pre-trained models (especially in NLP) and datasets for rapid experimentation and development. It's known for its open-source library and easy access to state-of-the-art models, making it great for research, startups, and developers.</p>
</li>
</ul>
<p>In essence, IBM watsonx focuses on enterprise-grade AI with strong governance and scalability, while Hugging Face is designed for flexible, community-driven AI development with a focus on rapid prototyping and open access to pre-trained models.</p>
<h2 id="lab1-lab1-overview-ibm-watsonx-hugging-face">IBM watsonx &amp; Hugging Face<a class="headerlink" href="#lab1-lab1-overview-ibm-watsonx-hugging-face" title="Permanent link">¶</a></h2>
<p>With IBM watsonx, clients can run not just IBM-trained foundation models, but also open source models and models from Hugging Face as well!</p>
<p>IBM watsonx and Hugging Face can work together by combining IBM's enterprise AI capabilities with Hugging Face's vast collection of pre-trained models and tools for rapid AI development:</p>
<ul>
<li><strong>Model Access</strong>: IBM watsonx users can leverage Hugging Face's pre-trained models (like GPT, BERT, etc.) from its model hub to fine-tune or deploy in enterprise environments using watsonx’s scalable infrastructure.</li>
<li><strong>Fine-Tuning and Customization</strong>: Businesses can use Hugging Face models in watsonx to fine-tune them with proprietary data while benefiting from watsonx’s AI governance and compliance tools.</li>
<li><strong>Deployment</strong>: Hugging Face models can be integrated into IBM watsonx to deploy at scale on hybrid cloud or on-premises environments, ensuring enterprise-level security and performance.</li>
</ul>
<p>Together, Hugging Face provides the models, and watsonx offers the enterprise-ready infrastructure for secure, large-scale, compliant deployment.
Read <a href="https://developer.ibm.com/blogs/awb-hugging-face-and-ibm-working-together-in-open-source/" target="_blank">this blog</a> to understand more about how IBM and Hugging Face are working to bring open source communities together for enterprise AI.</p>
<h2 id="lab1-lab1-overview-ai-and-watsonx-with-ibm-power">AI and watsonx with IBM Power<a class="headerlink" href="#lab1-lab1-overview-ai-and-watsonx-with-ibm-power" title="Permanent link">¶</a></h2>
<p>Here is a quick 1-slider on what you can do with AI and watsonx on IBM Power, today.
<a class="glightbox" href="https://github.com/user-attachments/assets/f3e6a66d-e418-4e3c-8315-08e125ad8149" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/f3e6a66d-e418-4e3c-8315-08e125ad8149"></a></p>
<p>Clients can get started with AI and watsonx with IBM Power today. 
IBM has made it simple by aligning the common use cases around 4 key areas that we see within our pilots and client activations.</p>
<ul>
<li><strong>Pattern 1</strong>: Securely tune, deploy and manage foundation models. When it comes to task-specific use cases, it is a good idea to work with large open-source models in your own workspace that are under your control. Hugging Face is a large repository with over 950K pre-trained ML models and a platform where the AI ecosystem collaborates on models, datasets and applications. Download any model from Hugging Face and securely deploy at scale on IBM Power. Then, use best-of-breed software to help you tune, deploy and manage as many models as you need. Some examples of what enterprises can use this capability for: Customer service, knowledge workers to augment staff and fraud reporting.
 </li>
<li><strong>Pattern 2</strong>: There are many new and existing business apps that are using foundation models integrated into the workflows. Deploy your foundation models anywhere, on Power, x86, cloud, and use the watsonx.ai software development kit (SDK) available in Python and embed directly into applications. On Power, enterprises can do this quickly so that services can be released to customers faster on a resilient 24/7 environment. Some examples of clients can embed AI into apps are generating the first draft of reports, citizen services for government end-clients and knowledge management.
 </li>
<li><strong>Pattern 3</strong>: The ecosystem is important to enterprises that have long-standing investments in software that drives their core business workflows. Consume watsonx services from customers customized ecosystem apps. Enterprises can generate code for Ansible playbooks for IBM i or AIX to enhance the Ansible IT management experience. Additionally, SAP applications can be embedded with watsonx services within the SAP ABAP environments. These custom apps help clients deliver services much faster while taking advantage of existing and familiar investments, making this an attractive proposition for many. Some example use cases include asset management, code generation, accounting automation. 
 </li>
<li><strong>Pattern 4</strong>: Lastly, we are bringing a full suite of AI capabilities to the Power platform to help clients train, tune and deploy models without purchasing GPUs. The lead time for GPUs is somewhere around a year and clients will miss out on opportunity if they can’t get started today. Some of the popular use cases that enterprises will use are fraud detection, risk underwriting, and demand forecasting.</li>
</ul>
<h2 id="lab1-lab1-overview-choosing-a-foundation-model">Choosing a foundation model<a class="headerlink" href="#lab1-lab1-overview-choosing-a-foundation-model" title="Permanent link">¶</a></h2>
<p>There are many factors to consider when you choose a foundation model to use for inferencing from a generative AI project.
Determine which factors are most important for you and your organization.</p>
<ul>
<li>Tasks the model can do</li>
<li>Languages supported</li>
<li>Tuning options for customizing the model</li>
<li>License and IP indemnity terms</li>
<li>Model attributes, such as size, architecture, and context window length</li>
</ul>
<p>After you have a short list of models that best fit your needs, you can test the models to see which ones consistently return the results you want.</p>
<p>For more details on choosing a foundation model that support your use case / language / other factors, refer to <a href="https://www.ibm.com/docs/en/watsonx/saas?topic=models-choosing-model" target="_blank">this document</a>. Although this document is part of the watsonx.ai product documentation, the information provided includes both IBM-trained models and open-source models as watsonx.ai support both types of models.</p></div></section><section class="print-page" id="lab1-lab1-hands-on-guide"><div><h1 id="deploy-a-llm-on-power10-hands-on-lab-guide">Deploy a LLM on Power10 - Hands-on lab guide<a class="headerlink" href="#lab1-lab1-hands-on-guide-deploy-a-llm-on-power10-hands-on-lab-guide" title="Permanent link">¶</a></h1>
<h2 id="lab1-lab1-hands-on-guide-lab-ready-check">Lab-ready check<a class="headerlink" href="#lab1-lab1-hands-on-guide-lab-ready-check" title="Permanent link">¶</a></h2>
<p>Make sure you have the following items ready:</p>
<ul>
<li>In your browser, you have logged in as <code>cecuser</code> in the OpenShift console.</li>
<li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI <code>oc</code>.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not proceed further unless the items mentioned above are ready. Please refer to "Lab setup instructions" section (see left hand side menu) to setup your browser and terminal windows.</p>
</div>
<h2 id="lab1-lab1-hands-on-guide-lab-guide">Lab guide<a class="headerlink" href="#lab1-lab1-hands-on-guide-lab-guide" title="Permanent link">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Image zoom functionality</p>
<p>Feel free to click on the images in the lab guide below to a view larger image.</p>
</div>
<h3 id="lab1-lab1-hands-on-guide-create-project">Create project<a class="headerlink" href="#lab1-lab1-hands-on-guide-create-project" title="Permanent link">¶</a></h3>
<ol>
<li>Go to OpenShift Administrator profile, click on <strong>Home</strong> -&gt; <strong>Projects</strong> and click <strong>Create Project</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/27a3b2e6-414b-4ec9-b0b1-be90e0b3858f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/27a3b2e6-414b-4ec9-b0b1-be90e0b3858f"></a></li>
<li>Enter a project name: <strong>lab1-demo</strong> &amp; click <strong>Create</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/6e6f471c-43e5-491b-8bcb-8700dbe4b320" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/6e6f471c-43e5-491b-8bcb-8700dbe4b320"></a></li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-setup-storage">Setup storage<a class="headerlink" href="#lab1-lab1-hands-on-guide-setup-storage" title="Permanent link">¶</a></h3>
<p>Let's setup storage for this lab which is needed for storing the downloaded AI models. This environment comes with NFS (file storage) pre-configured. 
In OpenShift, you first request for the storage (also known as PersistentVolumeClaim or PVC) and the actual storage (also known as PersistentVolume or PV) gets alloted based on your request and the storage availability in the storage pool (NFS in this case).</p>
<ol>
<li>Go to OpenShift Administrator profile, click on <strong>Storage</strong> -&gt; <strong>PersistentVolumeClaims</strong> and click <strong>Create PersistentVolumeClaim</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/e8d43c1e-2174-4976-b02b-e05ebfe37cc2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e8d43c1e-2174-4976-b02b-e05ebfe37cc2"></a></li>
<li>In the resulting form, enter PVC name: <strong>model-storage</strong> and Size: <strong>20</strong> GB. Leave other fields as defaults and click <strong>Create</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/56931cb0-f697-4a11-8038-db15f451168c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/56931cb0-f697-4a11-8038-db15f451168c"></a></li>
<li>Note that it shows PVC status as bound, which means storage was allotted.
   <a class="glightbox" href="https://github.com/user-attachments/assets/ea19ae1f-899d-4ff5-a5e0-f97df1e97ea2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ea19ae1f-899d-4ff5-a5e0-f97df1e97ea2"></a></li>
<li>
<p>Navigate to <strong>Storage</strong> -&gt; <strong>PersistentVolumes</strong> and view the actual storage (PV) bound to your storage request (PVC = <strong>model-storage</strong>).
   <a class="glightbox" href="https://github.com/user-attachments/assets/9fbc41be-f900-4052-8ac0-810edf6bd17e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9fbc41be-f900-4052-8ac0-810edf6bd17e"></a></p>
<p>This completes the storage setup.</p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-setup-configmap">Setup ConfigMap<a class="headerlink" href="#lab1-lab1-hands-on-guide-setup-configmap" title="Permanent link">¶</a></h3>
<p>In OpenShift, a ConfigMap is an object used to manage configuration data for applications. It allows you to decouple configuration details from application logic, which makes your applications more portable and easier to manage across different environments. Instead of hardcoding configuration values in your application, you store them in a ConfigMap and inject them into your application at runtime.</p>
<p>We will use ConfigMap to store the model URL and model name, both of which will be used when we deploy the model. Using ConfigMap will help us switch to a different model very easily.</p>
<ol>
<li>
<p>Navigate to <strong>Workloads</strong> -&gt; <strong>ConfigMaps</strong> and select <strong>lab1-demo</strong> as the active Project.
   <a class="glightbox" href="https://github.com/user-attachments/assets/72e44e26-9f7e-4d85-a67b-47752ba6c983" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/72e44e26-9f7e-4d85-a67b-47752ba6c983"></a></p>
</li>
<li>
<p>Click <strong>Create ConfigMap</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/14b9433f-35ab-4042-8c37-5302756b8b6e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/14b9433f-35ab-4042-8c37-5302756b8b6e"></a></p>
</li>
<li>
<p>In the resulting form, Ensure <strong>Form view</strong> option is selected.
   <a class="glightbox" href="https://github.com/user-attachments/assets/5c156ef4-a314-47f5-9084-328a170f1d74" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5c156ef4-a314-47f5-9084-328a170f1d74"></a></p>
</li>
<li>
<p>Enter a name: <strong>model-params</strong>, and fill the Key and Value fields as below:</p>
<ul>
<li>
<p><strong>Key</strong>: <code>MODEL_NAME</code></p>
</li>
<li>
<p><strong>Value</strong>: <code>tinyllama-1.1b-chat-v1.0.Q8_0.gguf</code></p>
</li>
</ul>
<p>Click <strong>Add key/value</strong> which will open up one more Key/Value box.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/68614a1f-7a47-425d-8bd4-ebd291b7ee32" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/68614a1f-7a47-425d-8bd4-ebd291b7ee32"></a></p>
</li>
<li>
<p>In the newly created Key/Value box, enter the values as below:</p>
<ul>
<li>
<p><strong>Key</strong>: <code>MODEL_URL</code></p>
</li>
<li>
<p><strong>Value</strong>: <code>https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf</code></p>
</li>
</ul>
<p>Click <strong>Create</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/24be3e90-eab3-4961-8f7c-03980c95721a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/24be3e90-eab3-4961-8f7c-03980c95721a"></a></p>
<p>This completes the ConfigMap setup.</p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-deploy-first-model">Deploy first model<a class="headerlink" href="#lab1-lab1-hands-on-guide-deploy-first-model" title="Permanent link">¶</a></h3>
<ol start="2">
<li>
<p>Navigate to the OpenShift Developer profile console window.</p>
<ul>
<li><strong>Note</strong>: If you followed the TIP given in the "Lab setup instructions" page, you should already have a browser window/tab with the Developer profile. In case you didn't, go to top left corner of your console, click <strong>Administrator</strong> and select <strong>Developer</strong>.</li>
</ul>
</li>
<li>
<p><strong>IMP</strong>: Ensure you are in the <strong>lab1-demo</strong> project in the Developer profile window. If not, goto <strong>Project</strong> and select <strong>lab1-demo</strong>.</p>
<p><div class="video-container"><video style="position:relative;width:100%;height:22.172vw" controls alt="type:video"><source src="../_attachments/switch-to-lab1-demo-project.mp4" type="video/mp4"></source></video></div></p>
</li>
<li>
<p>Click on <strong>+Add</strong> and select <strong>Import YAML</strong> option.
   <a class="glightbox" href="https://github.com/user-attachments/assets/1f49bdcb-bf92-420b-993f-509a52446462" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/1f49bdcb-bf92-420b-993f-509a52446462"></a></p>
</li>
<li>
<p>In the resulting window, copy and paste the below deployment YAML into it and click <strong>Create</strong>.
   </p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-1"> 1</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-2"> 2</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-3"> 3</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-4"> 4</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-5"> 5</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-6"> 6</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-7"> 7</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-8"> 8</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-9"> 9</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-10">10</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-11">11</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-12">12</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-13">13</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-14">14</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-15">15</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-16">16</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-17">17</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-18">18</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-19">19</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-20">20</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-21">21</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-22">22</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-23">23</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-24">24</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-25">25</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-26">26</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-27">27</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-28">28</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-29">29</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-30">30</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-31">31</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-32">32</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-33">33</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-34">34</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-35">35</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-36">36</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-37">37</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-38">38</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-39">39</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-40">40</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-41">41</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-42">42</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-43">43</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-44">44</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-45">45</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-46">46</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="w"> </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="w"> </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-0-3" name="__codelineno-0-3"></a><span class="w"> </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="w">   </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-demo</span>
<a id="__codelineno-0-5" name="__codelineno-0-5"></a><span class="w"> </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6"></a><span class="w">   </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-0-7" name="__codelineno-0-7"></a><span class="w">   </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-0-8" name="__codelineno-0-8"></a><span class="w">     </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9"></a><span class="w">       </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-demo</span>
<a id="__codelineno-0-10" name="__codelineno-0-10"></a><span class="w">   </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-0-11" name="__codelineno-0-11"></a><span class="w">     </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-12" name="__codelineno-0-12"></a><span class="w">       </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-0-13" name="__codelineno-0-13"></a><span class="w">         </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-demo</span>
<a id="__codelineno-0-14" name="__codelineno-0-14"></a><span class="w">     </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-15" name="__codelineno-0-15"></a><span class="w">       </span><span class="nt">initContainers</span><span class="p">:</span>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a><span class="w">         </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fetch-model-data</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a><span class="w">           </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubi8</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a><span class="w">           </span><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-models</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="w">               </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/models</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="w">           </span><span class="nt">command</span><span class="p">:</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sh</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">'-c'</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">|</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="w">               </span><span class="no">if [ ! -f /models/$MODEL_NAME ] ; then</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="w">                 </span><span class="no">curl -L $MODEL_URL --output /models/$MODEL_NAME</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="w">               </span><span class="no">else</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="w">                 </span><span class="no">echo "model /models/$MODEL_NAME already present"</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="w">               </span><span class="no">fi</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="w">               </span><span class="no">rm -f /models/mymodel</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="w">               </span><span class="no">ln -sf /models/$MODEL_NAME /models/mymodel</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="w">           </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="w">       </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="w">         </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-cpp</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="w">           </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">quay.io/daniel_casali/llama.cpp-mma:sep2024</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="w">           </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">"-m"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"/models/mymodel"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"-c"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"4096"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"--host"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"0.0.0.0"</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="w">           </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="w">               </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="w">           </span><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-models</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a><span class="w">               </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/models</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a><span class="w">       </span><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a><span class="w">         </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-models</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a><span class="w">           </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a><span class="w">             </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-storage</span>
</code></pre></div></td></tr></table></div>
    <a class="glightbox" href="https://github.com/user-attachments/assets/84a47fac-c4a6-49bc-b69d-3b89266b4d61" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/84a47fac-c4a6-49bc-b69d-3b89266b4d61"></a>
</li>
<li>
<p>Here is a quick explanation of the deployment YAML.</p>
<details class="info">
<summary>Deployment YAML explanation</summary>
<ul>
<li><strong>Line 2</strong> says its a deployment YAML  </li>
<li><strong>Line 4</strong> shows the name of this deployment resource (<code>lab-demo</code>)         </li>
<li><strong>Lines 7-9</strong>: Defines the label selector to match Pods with the "app: lab1-demo" label         </li>
<li><strong>Lines 11-13</strong>: Specifies the labels that the Pods will have.  </li>
<li><code>spec</code> section starting on <strong>Line 14</strong> contains the pod specification. This deployment has 1 pod which contains 2 containers.   <ul>
<li><strong>initContainer</strong> named <code>fetch-model-data</code> (<strong>Lines 15-32</strong>) which fetches the model from HF and saves it in the underlying storage. The shell script embedded under the <code>command</code> section ensures that the model is downloaded only if its not already present. <code>volumeMounts</code> section (<strong>Lines 18-20</strong>) has the name of the volume <code>llama-models</code> this container will use for disk storage and the path where the storage should be mounted. This initContainer uses the standard RHEL8 UBI (Universal Base Image) image and exits after downloading the model.</li>
<li><strong>Container</strong> named <code>llama-cpp</code> (<strong>Lines 33-42</strong>) which is the main workload container that will serve the LLM. This container uses the docker image <code>quay.io/daniel_casali/llama.cpp-mma:sep2024</code> which is a custom built image for ppc64le architecture with MMA optimized libraries. This image provides a runtime environment based on the open-source <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a> project which enables LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware.<ul>
<li>You may ask... What's the need for a model runtime?                   <ul>
<li>A model runtime serves as the infrastructure needed to deploy, manage, and execute these models in production.                      </li>
<li>It helps manage system resources such as CPU, GPU, memory, and network resources that are critical for deploying models.                     </li>
<li>Advanced runtimes may include optimizations for specific hardware, including accelerators like TPUs or MMAs (Matrix Math Accelerators), for faster computation.                      </li>
<li>It allows models to be scaled for real-world applications, particularly in cloud or distributed computing environments.                      </li>
<li>It enables handling multiple requests simultaneously while keeping latency low, which is vital in production systems.                      </li>
</ul>
</li>
<li>By facilitating the efficient execution and management of machine learning models, runtimes are essential for operationalizing AI and machine learning solutions in production.   </li>
</ul>
</li>
<li><strong>NOTE</strong>: Container <code>llama-cpp</code> uses the same volume <code>llama-models</code> as initContainer for underlying storage and hence can access the model(s) downloaded by initContainer. <strong>Lines 43-46</strong> specifies the PVC (Persistent Volume Claim) <code>model-storage</code> used as the source of storage for the volume. Recall that we created this PVC at the beginning of this lab!</li>
</ul>
</li>
</ul>
</details>
</li>
<li>
<p>You should land in the Deployment details window. Click on <strong>Pods</strong> tab and you should see the Pod erroring out. This is expected as the YAML references MODEL_URL and MODEL_NAME environment variables which we haven't supplied yet! Remember we do have those in ConfigMap, so we use inject that in the next step.
   <a class="glightbox" href="https://github.com/user-attachments/assets/e25f5f53-0aa7-4f81-a51b-b3dee3bb7cf9" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e25f5f53-0aa7-4f81-a51b-b3dee3bb7cf9"></a></p>
</li>
<li>
<p>Navigate to <strong>Environment</strong> tab, select <strong>fetch-model-data</strong> container and select <strong>model-params</strong> ConfigMap and click <strong>Save</strong>.
    <a class="glightbox" href="https://github.com/user-attachments/assets/0b42cb07-97a2-4f70-82a2-9abc9c6113aa" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/0b42cb07-97a2-4f70-82a2-9abc9c6113aa"></a></p>
<div class="admonition info">
<p class="admonition-title">About LLama and tinyLLama models</p>
<p>The ConfigMap currently points to the tinyLLaMa model. The LLaMA (Large Language Model Meta AI) model is a family of state-of-the-art large language models developed by Meta (formerly Facebook), specifically designed to perform various natural language processing tasks. TinyLLaMA is a compact variant of the LLaMA (Large Language Model Meta AI) model, designed for efficiency and accessibility, especially when deployed in smaller environments. Available via Hugging Face (HF), it focuses on reducing the size of large-scale language models while retaining strong performance across various natural language processing tasks.</p>
</div>
</li>
<li>
<p>Switch back to <strong>Pods</strong> tab and you should see that a new pod has been launched by OpenShift as we changed the pod's environment, when we added ConfigMap.
    <a class="glightbox" href="https://github.com/user-attachments/assets/55ba8b7c-3760-45f8-813b-99b90e026daf" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/55ba8b7c-3760-45f8-813b-99b90e026daf"></a></p>
</li>
<li>
<p>The new pod will download the model and then start it. Since the configmap points to tinyllama model, it will be downloaded from HuggingFace and then started. When that happens the pod's status will change to Running.</p>
<div class="admonition info">
<p class="admonition-title">Model download will take time - Have patience!!</p>
<p>This process will take a few minutes (in my case it took around 1-1.5 mins) and your mileage may vary! Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV).</p>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/06801c61-7ec4-46c6-b5d7-1f9f4af660dc" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/06801c61-7ec4-46c6-b5d7-1f9f4af660dc"></a></p>
<p><strong>Congratulations!</strong>, you have successfully deployed a LLM on Power10.</p>
</li>
<li>
<p>Let's verify that the model running is tinyllama!. Click on the pod to enter the pod details view/page.
    <a class="glightbox" href="https://github.com/user-attachments/assets/b96d318e-f6e8-48ea-9b19-88ca2813d0ca" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b96d318e-f6e8-48ea-9b19-88ca2813d0ca"></a></p>
</li>
<li>In the pod details page, click on the <strong>Logs</strong> tab to see the pod logs.
    <a class="glightbox" href="https://github.com/user-attachments/assets/d01b6b9e-b1d7-4f5a-8dd1-a2bd54882aff" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d01b6b9e-b1d7-4f5a-8dd1-a2bd54882aff"></a></li>
<li>
<p>In the log window, scroll upwards to see the name of the model against the attribute <strong>llm_load_print_meta: general.name</strong>.
    <a class="glightbox" href="https://github.com/user-attachments/assets/89536022-644a-497c-9225-9a08d68de52a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/89536022-644a-497c-9225-9a08d68de52a"></a></p>
<p>This verifies that we have indeed deployed tinyllama.</p>
</li>
<li>
<p>Let's access our model and interact with it. In OpenShift, you need to create a service and a route which generates the cluster internal and publicly accessible HTTP endpoints, respectively. To do that, navigate to the OpenShift Administrator profile and click <strong>Networking</strong> -&gt; <strong>Services</strong> and click <strong>Create Service</strong>.</p>
<div class="admonition note">
<p>If you are switching browser window/tab, make sure you are in the <strong>lab1-demo</strong> project in the new window/tab.</p>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/d901813f-96f5-4c91-a5d5-1455adafff09" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d901813f-96f5-4c91-a5d5-1455adafff09"></a></p>
</li>
<li>
<p>In the resulting Create Service YAML window, select all &amp; delete everything. Then copy the below service YAML, paste it in the YAML window and click <strong>Create</strong>.
    </p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-1"> 1</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-2"> 2</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-3"> 3</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-4"> 4</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-5"> 5</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-6"> 6</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-7"> 7</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-8"> 8</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-9"> 9</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-10">10</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-11">11</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-12">12</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-13">13</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-14">14</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-15">15</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-1-2" name="__codelineno-1-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-1-3" name="__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">"lab1-service"</span>
<a id="__codelineno-1-5" name="__codelineno-1-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-1-6" name="__codelineno-1-6"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="s">"lab1-service"</span>
<a id="__codelineno-1-7" name="__codelineno-1-7"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-1-8" name="__codelineno-1-8"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="s">"ClusterIP"</span>
<a id="__codelineno-1-9" name="__codelineno-1-9"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-1-10" name="__codelineno-1-10"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-port</span>
<a id="__codelineno-1-11" name="__codelineno-1-11"></a><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<a id="__codelineno-1-12" name="__codelineno-1-12"></a><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-1-13" name="__codelineno-1-13"></a><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<a id="__codelineno-1-14" name="__codelineno-1-14"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-1-15" name="__codelineno-1-15"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="s">"lab1-demo"</span>
</code></pre></div></td></tr></table></div>
    <a class="glightbox" href="https://github.com/user-attachments/assets/6e76ac2d-bf80-41c2-895b-53050d4cbbbf" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/6e76ac2d-bf80-41c2-895b-53050d4cbbbf"></a>
</li>
<li>
<p>Here is a quick explanation of the Service YAML.</p>
<details class="info">
<summary>Service YAML explanation</summary>
<ul>
<li>
<p><strong>Line 2</strong> shows its a service YAML</p>
</li>
<li>
<p><strong>Line 4</strong> shows the name of the service (<code>lab1-service</code>)</p>
</li>
<li>
<p><strong>Line 8</strong> shows the type of this service is <code>ClusterIP</code>. ClusterIP is the default service type used to expose a set of Pods internally within the cluster. It creates a virtual IP (ClusterIP) for the service, which other services or Pods within the same Kubernetes cluster can use to access it.</p>
</li>
<li>
<p><strong>Lines 9-13</strong> has the port details. <code>port: 8080</code> refers to the port for the incoming traffic of the service. <code>targetPort: 8080</code> refers to the port on the pod. This means that incoming traffic for the service on port 8080 will be forwarded to the pod on port 8080.</p>
</li>
<li>
<p><strong>Lines 14-15</strong> specifies the pod selector label. It defines which Pods the service will route traffic to. In this case, it matches Pods that have the label <code>app: lab1-demo</code>, which is the label we assigned in the deployment YAML.</p>
</li>
</ul>
</details>
</li>
<li>
<p>You should land in service details view/page. You can see the ClusterIP which is accessible from inside the cluster only.
    <a class="glightbox" href="https://github.com/user-attachments/assets/5b8b3385-44a7-4dec-bbfa-f384c5f784fb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5b8b3385-44a7-4dec-bbfa-f384c5f784fb"></a></p>
</li>
<li>
<p>Navigate to <strong>Networking</strong> -&gt; <strong>Routes</strong> and click <strong>Create Route</strong>.
      <a class="glightbox" href="https://github.com/user-attachments/assets/7240eef0-b4af-4fde-abed-faae0234e343" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/7240eef0-b4af-4fde-abed-faae0234e343"></a></p>
</li>
<li>
<p>In the resulting Create Route window, select <strong>YAML view</strong> and clear everything from the YAML window. Copy the below YAML and paste it in the YAML window and click <strong>Create</strong>.
    </p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-1"> 1</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-2"> 2</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-3"> 3</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-4"> 4</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-5"> 5</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-6"> 6</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-7"> 7</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-8"> 8</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-9"> 9</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-10">10</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-11">11</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-12">12</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-13">13</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Route</span>
<a id="__codelineno-2-2" name="__codelineno-2-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">route.openshift.io/v1</span>
<a id="__codelineno-2-3" name="__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-route</span>
<a id="__codelineno-2-5" name="__codelineno-2-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-2-6" name="__codelineno-2-6"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-route</span>
<a id="__codelineno-2-7" name="__codelineno-2-7"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-2-8" name="__codelineno-2-8"></a><span class="w">  </span><span class="nt">to</span><span class="p">:</span>
<a id="__codelineno-2-9" name="__codelineno-2-9"></a><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-2-10" name="__codelineno-2-10"></a><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-service</span>
<a id="__codelineno-2-11" name="__codelineno-2-11"></a><span class="w">  </span><span class="nt">tls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<a id="__codelineno-2-12" name="__codelineno-2-12"></a><span class="w">  </span><span class="nt">port</span><span class="p">:</span>
<a id="__codelineno-2-13" name="__codelineno-2-13"></a><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-port</span>
</code></pre></div></td></tr></table></div>
    <a class="glightbox" href="https://github.com/user-attachments/assets/f100a3dc-1286-4c6f-bd60-b534f1e84090" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/f100a3dc-1286-4c6f-bd60-b534f1e84090"></a>
</li>
<li>
<p>Here is a quick explanation of the Route YAML. In OpenShift, a Route exposes a service to external clients by mapping an external URL to an internal OpenShift service.</p>
<details class="info">
<summary>Route YAML explanation</summary>
<ul>
<li>
<p><strong>Line 1</strong>: This defines that you're creating an OpenShift Route resource.</p>
</li>
<li>
<p><strong>Lines 3-6</strong>: The <code>name</code> field defines the name of the route (<code>lab1-route</code>), and the <code>labels</code> field associates labels with the route, which can be used for tracking or organizing the route in OpenShift.</p>
</li>
<li>
<p><strong>Lines 7-10</strong>: The <code>to</code> field under <code>spec</code> defines the target resource for the route. In this case, it targets a Service named <code>lab1-service</code>, which has already been defined and exposes Pods internally via ClusterIP.</p>
</li>
<li>
<p><strong>Line 11</strong>: TLS is set to <code>null</code>, meaning there is no SSL/TLS encryption configured for this route. For secure routes (HTTPS), you'd configure this section to specify certificates and encryption protocols.</p>
</li>
<li>
<p><strong>Lines 12-13</strong>: The <code>targetPort</code> field under <code>port</code> specifies which port on the service (or Pods) the route should forward requests to. In this case, it refers to <code>lab1-port</code>, which was defined as port 8080 in our service configuration.</p>
</li>
</ul>
</details>
</li>
<li>
<p>You should land in the route details view. The URL mentioned under <strong>Location</strong> is the externally accessible URL of your application (which hosts the tinyllama LLM).
    <a class="glightbox" href="https://github.com/user-attachments/assets/abfcc310-985e-4208-9e56-d2f39e4f2c13" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/abfcc310-985e-4208-9e56-d2f39e4f2c13"></a></p>
</li>
<li>
<p>Click on the external URL in the route details view to access your model. A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:      </p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53"></a></p>
</li>
<li>
<p>Scroll all the way down to the input field "Say something..." where you can interact with the LLM. You can ask any question you like, but keep in mind you're using a small model and there are more powerful models out there for general conversation.
    <a class="glightbox" href="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce"></a></p>
<div class="admonition note">
<p class="admonition-title">Experimenting with model parameters</p>
<p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Feel free to google and learn about them and experiment with it. You may want change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab.</p>
</div>
</li>
<li>
<p>Here are some questions I asked and the responses I got.</p>
<div class="admonition warning">
<p class="admonition-title">Accuracy of LLM responses</p>
<ul>
<li>Large Language Models (LLMs), are trained on vast amounts of text data, but that training is limited to a <strong>specific cutoff date</strong>. This means that the model can only answer questions based on the information available up to that point in time. It cannot access real-time data or understand events, trends, or new information that occurred after the cutoff date. Consequently, their ability to provide accurate answers is constrained by the knowledge they were trained on.</li>
<li>AI-generated content may vary and may not always provide consistent answers. Your response may be different than what I got.</li>
</ul>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/c0c4b3ca-dbc6-4f9f-8d29-115c11486843" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c0c4b3ca-dbc6-4f9f-8d29-115c11486843"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/c9fde420-3c8c-40bc-a48c-0ece97fc2248" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c9fde420-3c8c-40bc-a48c-0ece97fc2248"></a></p>
<p><strong>Congratulations</strong>, you were able to deploy a LLM, create a public endpoint to access it and take it for a run!
In the next step, let's learn how to deploy a different LLM.</p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-deploy-second-model">Deploy second model<a class="headerlink" href="#lab1-lab1-hands-on-guide-deploy-second-model" title="Permanent link">¶</a></h3>
<p>In this section, let's deploy IBM's granite model.</p>
<div class="admonition info">
<p class="admonition-title">About IBM's granite LLM</p>
<p>The IBM Granite model family is designed as enterprise-grade AI models tailored for business applications. Granite models are available both as open-source models on platforms like Hugging Face and through IBM's watsonx.ai for more enterprise-specific needs.</p>
</div>
<ol>
<li>Navigate to your OpenShift developer profile window. Click <strong>ConfigMaps</strong>, then click <strong>model-params</strong>. This should open up the ConfigMap details page.
   <a class="glightbox" href="https://github.com/user-attachments/assets/a26e875b-02d0-438e-84f3-9ed86347e650" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a26e875b-02d0-438e-84f3-9ed86347e650"></a></li>
<li>In the model-params ConfigMap details page, click <strong>Action</strong> and select <strong>Edit ConfigMap</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/a0d66be8-8ca3-4f18-b360-ee35a7843862" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a0d66be8-8ca3-4f18-b360-ee35a7843862"></a></li>
<li>
<p>In the resulting form, edit the key/value fields for MODEL_NAME and MODEL_URL as below and click <strong>Save</strong>.</p>
<ul>
<li>Key: MODEL_NAME</li>
<li>Value: granite-7b-lab.Q4_K_M.gguf</li>
</ul>
<hr>
<ul>
<li>Key: MODEL_URL</li>
<li>Value: https://huggingface.co/RichardErkhov/instructlab_-_granite-7b-lab-gguf/resolve/main/granite-7b-lab.Q4_K_M.gguf</li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/7347b269-c597-4570-9d5d-509474212052" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/7347b269-c597-4570-9d5d-509474212052"></a></p>
</li>
<li>
<p>The existing pod won't see the changes right away as changing values of a ConfigMap doesn't cause a deployment (and hence pod) to restart. It needs to be done manually.</p>
</li>
<li>
<p>Let's go to the deployment view. Click <strong>Topology</strong>, then click "<strong>D lab1-demo</strong>" part of the application icon, which will open up the deployment details pane (on the right hand side of the browser window). Click <strong>D lab1-demo</strong> in that pane which will then open up the deployment details view for lab1-demo deployment.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/ecbbca24-682c-4480-a989-7a72f7398958" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ecbbca24-682c-4480-a989-7a72f7398958"></a></p>
</li>
<li>
<p>Click <strong>Actions</strong> and select <strong>Restart rollout</strong>. This will restart the deployment which results in redeployment of the pod.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/51476c1a-52a2-4fbe-ae28-9dca9348ac4f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/51476c1a-52a2-4fbe-ae28-9dca9348ac4f"></a></p>
</li>
<li>
<p>Click on <strong>Pods</strong> tab, where you will see a new pod instantiated. The new pod will download the model and then start it. Since the configmap points to granite model, it will be downloaded from HuggingFace and then started. When that happens the new pod's status will change to Running and the existing pod will be terminated.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/b80de8ad-04b0-414b-bc2c-247dd83b089b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b80de8ad-04b0-414b-bc2c-247dd83b089b"></a></p>
<div class="admonition info">
<p class="admonition-title">Model download will take time - Have patience!!</p>
<p>This process will take a few minutes (in my case it took around 3-4 mins as this is a fairly large model compared to tinyllama) and your mileage may vary! Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV).</p>
</div>
</li>
<li>
<p>If all goes well, you should see just 1 pod running.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/854703df-2ca4-42ce-825c-30c49a94a050" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/854703df-2ca4-42ce-825c-30c49a94a050"></a></p>
</li>
<li>
<p>Let's verify that the model running is granite!. Click on the pod to enter the pod details view/page.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/8a50c71b-4490-4bfe-8aaf-eafa25fdc05f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/8a50c71b-4490-4bfe-8aaf-eafa25fdc05f"></a></p>
</li>
<li>
<p>In the pod details page, click on the Logs tab to see the pod logs.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/bd5aa047-9b86-4996-abb5-28c1ec091d75" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/bd5aa047-9b86-4996-abb5-28c1ec091d75"></a></p>
</li>
<li>
<p>In the log window, scroll upwards to see the name of the model against the attribute <strong>llm_load_print_meta: general.name</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/5ca4f0b1-8655-41df-85d2-8d740627989c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5ca4f0b1-8655-41df-85d2-8d740627989c"></a></p>
<p>This verifies that we have indeed deployed granite.   </p>
</li>
<li>
<p>Let's access the model now!. As we did in the previous section of this lab, we need to find the external public endpoint. The beauty of OpenShift is that the endpoint remains same inspite of the pod being restarted. So either you can refresh the earlier page (if you have it opened in the browser) or follow the steps below to access the public URL of your application via the <strong>Topology</strong> view.</p>
<div class="admonition note">
<p class="admonition-title">Multiple ways to access public URL of your application</p>
<p>If you are in Administrator profile, you can navigate to <strong>Networking</strong> -&gt; <strong>Routes</strong> to access your route resource and click on the URL to open your application, as we did in the previous section of this lab where we deployed our first model. Alternatively, if you are in Developer profile, you can go to <strong>Topology</strong> view and access the URL of your application as well. Let's use that method here:</p>
</div>
<ul>
<li>In Developer profile window, click <strong>Topology</strong> and you should see the icon representing your deployed application.
  <a class="glightbox" href="https://github.com/user-attachments/assets/a34df273-e29b-44e0-9cf3-90e94d8fdafb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a34df273-e29b-44e0-9cf3-90e94d8fdafb"></a></li>
<li>Click on the arrow (in top right corner of the icon) that says "Open URL".
  <a class="glightbox" href="https://github.com/user-attachments/assets/7366146b-cf82-4dc3-9327-d51aa5944778" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/7366146b-cf82-4dc3-9327-d51aa5944778"></a></li>
<li>A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:
  <a class="glightbox" href="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53"></a></li>
<li>Scroll all the way down to the input field "Say something..." where you can interact with the LLM. You can ask any question you like!
  <a class="glightbox" href="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce"></a></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Experimenting with model parameters</p>
<p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Feel free to google and learn about them and experiment with it. You may want change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab.</p>
</div>
</li>
<li>
<p>Here are some questions I asked and the responses I got.</p>
<div class="admonition warning">
<p class="admonition-title">Accuracy of LLM responses</p>
<ul>
<li>Large Language Models (LLMs), are trained on vast amounts of text data, but that training is limited to a <strong>specific cutoff date</strong>. This means that the model can only answer questions based on the information available up to that point in time. It cannot access real-time data or understand events, trends, or new information that occurred after the cutoff date. Consequently, their ability to provide accurate answers is constrained by the knowledge they were trained on.</li>
<li>AI-generated content may vary and may not always provide consistent answers. Your response may be different than what I got.</li>
</ul>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/4201cafb-9f11-4f82-945b-1f843713426e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4201cafb-9f11-4f82-945b-1f843713426e"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/e0a23318-e2bc-4778-8c12-84643a8f530c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e0a23318-e2bc-4778-8c12-84643a8f530c"></a></p>
<p><strong>Congratulations</strong>, in this section you learned how easy it is to switch to a different LLM, access it and take it for a run!
This completes the lab.</p>
</li>
</ol></div></section><h1 class='nav-section-title-end'>Ended: Lab1 - Deploy a LLM on Power10</h1>
                        <h1 class='nav-section-title' id='section-lab2-deploy-rag-on-power10'>
                            Lab2 - Deploy RAG on Power10 <a class='headerlink' href='#section-lab2-deploy-rag-on-power10' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="lab2-lab2-overview"><div><h1 id="deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-education">Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - Lab education<a class="headerlink" href="#lab2-lab2-overview-deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-education" title="Permanent link">¶</a></h1>
<p>Goal of this lab is to get hands-on experience in deploying a RAG pattern on IBM Power10.
Before we do that, lets understand what RAG is, its advantages, key use cases and how it fits in the IBM Power landscape.</p>
<h2 id="lab2-lab2-overview-what-is-rag">What is RAG?<a class="headerlink" href="#lab2-lab2-overview-what-is-rag" title="Permanent link">¶</a></h2>
<p>RAG (Retrieval-Augmented Generation) is an advanced technique that combines retrieval-based and generation-based approaches to improve the performance of large language models, especially in question-answering and knowledge-intensive tasks. In layman terms, clients can use RAG pattern to generate factually accurate output from LLMs, that is grounded in information in a knowledge base.</p>
<p>Key Components of RAG:</p>
<ol>
<li><strong>Retrieval</strong>: The model first retrieves relevant documents or information from a large knowledge base (like Wikipedia or custom databases) based on the user's query. This helps provide the model with factual and up-to-date information that it may not have learned during training.</li>
<li><strong>Augmented Generation</strong>: Once the relevant documents are retrieved, the model then generates a response based on the query and the retrieved documents. This ensures the generated answer is both relevant and factual, combining the reasoning power of the language model with the accuracy of external knowledge.</li>
</ol>
<h2 id="lab2-lab2-overview-how-rag-works">How RAG works?<a class="headerlink" href="#lab2-lab2-overview-how-rag-works" title="Permanent link">¶</a></h2>
<p>RAG is a technique that uses vector databases to retrieve relevant information and improve the accuracy of Large Language Models (LLMs):</p>
<ol>
<li><strong>Vector database storage</strong>: Text data is converted into vector embeddings using pre-trained models like BERT or GPT. These embeddings are then stored in a vector database.<ul>
<li>In this lab, we use <strong>all-MiniLM-L6-v2</strong> model from HF to convert text data into embeddings which are then stored in a <strong>Milvus</strong> vector DB.</li>
</ul>
</li>
<li><strong>Query conversion</strong>: When a query is posed to the AI system, it is also converted into a vector.</li>
<li><strong>Vector search</strong>: The vector database performs a vector search to find relevant embeddings from the stored dataset.</li>
<li><strong>Information retrieval</strong>: The retrieved information is then integrated into the LLM's query input.</li>
<li><strong>Response generation</strong>: The augmented query is sent to the LLM to generate an accurate answer.</li>
</ol>
<div class="admonition info">
<p class="admonition-title">Why use vector database in RAG?</p>
<p>Vector databases are used in RAG because they store data in a way that makes it easy to search and retrieve. Vector search techniques go beyond keyword matching and focus on semantic relationships, which improves the quality of the retrieved information.</p>
</div>
<h2 id="lab2-lab2-overview-need-advantages-of-rag">Need &amp; Advantages of RAG<a class="headerlink" href="#lab2-lab2-overview-need-advantages-of-rag" title="Permanent link">¶</a></h2>
<p>The need for Retrieval-Augmented Generation (RAG) arises from the limitations of current large language models (LLMs) and the growing demands for factual accuracy and knowledge scalability in AI applications. </p>
<p>Here are the key reasons why RAG is necessary and its associated advantages:</p>
<ol>
<li><strong>Handling Knowledge Gaps</strong><ul>
<li><strong>LLMs are static</strong>: Traditional language models, once trained, cannot access new or external information. They can only generate text based on the data they were trained on, which means they might miss important or up-to-date knowledge.</li>
<li><strong>RAG dynamically retrieves information</strong>: By incorporating a retrieval step, RAG can pull in relevant, up-to-date documents from external sources to complement the model's output, making it more accurate and current.</li>
</ul>
</li>
<li><strong>Reducing Hallucinations</strong><ul>
<li><strong>LLMs sometimes "hallucinate"</strong>: LLMs can generate convincing but incorrect or fabricated answers because they are predicting text based on patterns rather than verifying facts.</li>
<li><strong>RAG grounds responses in real data</strong>: Since RAG retrieves factual documents before generating a response, it ensures that the output is based on real, verifiable information, reducing the risk of false or misleading content.</li>
</ul>
</li>
<li><strong>Scalability in Knowledge</strong><ul>
<li><strong>LLMs are limited by training data</strong>: Even the largest models have limitations on how much they can remember from their training data, which might become outdated or incomplete.</li>
<li><strong>RAG scales with external data</strong>: By leveraging vast external knowledge bases or documents, RAG allows for almost unlimited knowledge expansion without retraining the model. This is particularly useful for enterprises or specific domains where continuous data updates are essential.</li>
</ul>
</li>
<li><strong>Improved Performance in Specific Domains</strong><ul>
<li><strong>Specialized knowledge is often needed</strong>: Many applications require access to niche or domain-specific information, such as legal texts, scientific papers, or proprietary company data.</li>
<li><strong>RAG retrieves domain-specific documents</strong>: The retrieval step allows RAG to pull in domain-specific or proprietary documents, making the output more relevant for specialized tasks.</li>
</ul>
</li>
<li><strong>Efficiency and Adaptability</strong><ul>
<li><strong>Model retraining is costly</strong>: Constantly retraining LLMs with new data is computationally expensive and time-consuming.</li>
<li><strong>RAG avoids retraining</strong>: By using real-time retrieval, RAG can access the latest information or new content without the need to retrain the entire model, making it more adaptable and cost-efficient.</li>
</ul>
</li>
<li><strong>Better Results for Open-Domain Question Answering</strong><ul>
<li><strong>Complex queries require precise answers</strong>: In tasks like open-domain question answering, general models might struggle to provide precise answers for complex or rare questions.</li>
<li><strong>RAG enhances accuracy</strong>: By combining retrieval with generation, RAG can provide more accurate, context-rich answers, drawing from a wide range of documents.</li>
</ul>
</li>
</ol>
<p>In summary, RAG addresses limitations in current LLMs by improving factual accuracy, scalability, and adaptability, making it particularly useful for knowledge-intensive tasks and dynamic environments.</p>
<h2 id="lab2-lab2-overview-common-use-cases-of-rag">Common use cases of RAG<a class="headerlink" href="#lab2-lab2-overview-common-use-cases-of-rag" title="Permanent link">¶</a></h2>
<p>Below are some of the common use cases for RAG:</p>
<ol>
<li><strong>Open-domain question answering</strong>: Where models need to answer questions about a wide range of topics, potentially beyond the training data.</li>
<li><strong>Customer support</strong>: Providing accurate answers by retrieving relevant documents from knowledge bases.</li>
<li><strong>Enterprise AI</strong>: Helping businesses with information retrieval, knowledge management, and research by retrieving and summarizing relevant documents.</li>
</ol>
<p>To summarize, RAG allows models to perform better in knowledge-intensive tasks by combining the strengths of both retrieval systems and generative language models.</p>
<h2 id="lab2-lab2-overview-ibm-power-and-rag">IBM Power and RAG<a class="headerlink" href="#lab2-lab2-overview-ibm-power-and-rag" title="Permanent link">¶</a></h2>
<h3 id="lab2-lab2-overview-ibm-power-systems-as-systems-of-record">IBM Power Systems as Systems of Record<a class="headerlink" href="#lab2-lab2-overview-ibm-power-systems-as-systems-of-record" title="Permanent link">¶</a></h3>
<p>IBM Power Systems are renowned for their performance, reliability, and scalability, making them ideal for handling systems of record. A system of record (SOR) refers to a trusted source of truth that stores essential business data and transactions, such as financial systems, customer data, and inventory management. IBM Power Systems are often used for mission-critical applications in industries like banking, healthcare, and government due to their ability to manage large volumes of secure, transactional data.</p>
<h3 id="lab2-lab2-overview-how-ibm-power-systems-plays-well-with-rag">How IBM Power Systems Plays Well with RAG<a class="headerlink" href="#lab2-lab2-overview-how-ibm-power-systems-plays-well-with-rag" title="Permanent link">¶</a></h3>
<p>Retrieval-Augmented Generation (RAG) is a generative AI framework that enhances the performance of AI models by retrieving relevant documents from external knowledge bases before generating a response. This retrieval step ensures that the generated output is more accurate and fact-based, as it is grounded in real-time data from a reliable source.</p>
<p>IBM Power Systems play exceptionally well with RAG due to the following reasons:</p>
<ol>
<li><strong>High-Performance Data Handling</strong><ul>
<li>Power Systems are designed for high-volume data transactions and processing. This makes them perfect for storing and managing systems of record, which RAG relies on to retrieve real-time, relevant information.</li>
<li>When RAG retrieves data from a system of record stored on IBM Power, it benefits from the fast data access and throughput that Power Systems offer, ensuring quick and efficient retrieval of documents for AI processing.</li>
</ul>
</li>
<li><strong>Data Security and Compliance</strong><ul>
<li>Power Systems are known for their robust security features, including end-to-end encryption and advanced data protection, making them ideal for storing sensitive data such as customer information, financial records, or healthcare data.</li>
<li>In a RAG scenario, where retrieved data is used to generate answers, the ability of IBM Power Systems to ensure data privacy and regulatory compliance (for example, HIPAA, GDPR) is crucial, especially for enterprises dealing with sensitive or regulated data.</li>
</ul>
</li>
<li><strong>Scalability and Reliability</strong><ul>
<li>RAG applications require scalable infrastructure to handle varying levels of computational demand, especially when dealing with large-scale document retrieval and real-time AI processing. IBM Power Systems are built to scale seamlessly, allowing RAG to handle larger datasets and more complex queries without performance degradation.</li>
<li>Reliability is critical for systems of record, and Power Systems have a proven track record of uptime and resilience, ensuring that the data RAG retrieves is always available when needed, without risk of downtime affecting the retrieval process.</li>
</ul>
</li>
<li><strong>Integration with AI Workloads</strong><ul>
<li>IBM Power Systems are optimized for AI workloads, with features like accelerated AI processing (for example, Power10's Matrix Math Accelerator (MMA)) that boost the performance of both retrieval and generation tasks in RAG.</li>
<li>By running RAG-based applications on Power10, enterprises can take advantage of faster AI inference and improved data handling, resulting in more responsive and accurate AI systems.</li>
</ul>
</li>
<li><strong>Efficient Handling of Structured and Unstructured Data</strong><ul>
<li>Power Systems can efficiently handle both structured data (like databases) and unstructured data (such as documents and records), making them ideal for RAG, where both types of data may be retrieved from systems of record.</li>
<li>RAG can retrieve structured data for quick reference (for example, customer records or transactions) and unstructured data (for example, reports, emails) for more complex, context-driven AI responses. Power Systems' capability to manage both types ensures efficient, accurate information retrieval.</li>
</ul>
</li>
<li><strong>Real-Time Analytics</strong><ul>
<li>Real-time data analytics capabilities in IBM Power Systems enable quick access to up-to-date information, which is essential for RAG when generating responses based on current data.</li>
<li>This feature allows the RAG model to provide contextualized answers based on the latest transactions or data updates from the system of record, improving the relevance of AI-driven outputs.</li>
</ul>
</li>
</ol>
<p><strong>Summary</strong></p>
<p>IBM Power Systems provide the speed, security, and reliability needed to store and manage systems of record that RAG models depend on for data retrieval. Power Systems’ advanced capabilities in data handling, AI optimization, scalability, and security make them an ideal infrastructure for supporting RAG-based AI applications, ensuring that retrieved data is accurate, current, and secure, thus enhancing the quality of generative AI outputs.</p>
<h3 id="lab2-lab2-overview-rag-ibm-power10-solution-architecture">RAG + IBM Power10 - solution architecture<a class="headerlink" href="#lab2-lab2-overview-rag-ibm-power10-solution-architecture" title="Permanent link">¶</a></h3>
<p>Here is the high level solution architecture of a typical RAG use case on IBM Power10. This example is deployed on IBM Power10 end-to-end. The foundation model is simply downloaded from watsonx.ai or open-source repositories such as the Hugging Face model hub. The model does not require fine-tuning thanks to a domain adaptation technique called Retrieval Augmented Generation (RAG). </p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/3476cad1-a743-474f-8535-b70806d8c09f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/3476cad1-a743-474f-8535-b70806d8c09f"></a></p>
<ol>
<li>User asks a domain specific question in natural language.</li>
<li>Q&amp;A app looksup in the knowledge base repository.</li>
<li>Documents relevant to the question is retrieved from the repository.</li>
<li>"Question + Documents" is passed as the context in a prompt to LLM.</li>
<li>LLM generates the domain-specific answer.</li>
</ol></div></section><section class="print-page" id="lab2-lab2-hands-on-guide"><div><h1 id="deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-guide">Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - lab guide<a class="headerlink" href="#lab2-lab2-hands-on-guide-deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-guide" title="Permanent link">¶</a></h1>
<h2 id="lab2-lab2-hands-on-guide-lab-ready-check">Lab-ready check<a class="headerlink" href="#lab2-lab2-hands-on-guide-lab-ready-check" title="Permanent link">¶</a></h2>
<p>Make sure you have the following items ready:</p>
<ul>
<li>In your browser, you have logged in as cecuser in the OpenShift console.</li>
<li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI oc.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not proceed further unless the items mentioned above are ready. Please refer to "Lab setup instructions" section (see left hand side menu) to setup your browser and terminal windows.</p>
</div>
<h2 id="lab2-lab2-hands-on-guide-lab-guide">Lab guide<a class="headerlink" href="#lab2-lab2-hands-on-guide-lab-guide" title="Permanent link">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Image zoom functionality</p>
<p>Feel free to click on the images in the lab guide below to a view larger image.</p>
</div>
<p>In this lab, we will focus on the below:</p>
<ul>
<li>
<p>Deploy a vector DB - milvus.</p>
</li>
<li>
<p>Deploy a jupyter notebook where we will implement RAG pattern and learn about:</p>
<ul>
<li>Index the milvus DB with a sample PDF.</li>
<li>Query the DB to get relevant documents for the question asked.</li>
<li>Create a prompt based on the relevant documents gotten from the previous step and send it to the LLM (granite model we deployed in Lab1) to get a domain specific answer.</li>
</ul>
</li>
</ul>
<h3 id="lab2-lab2-hands-on-guide-create-project">Create project<a class="headerlink" href="#lab2-lab2-hands-on-guide-create-project" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Let's create a new OpenShift project that will hold all resources of this lan. Navigate to your browser window/tab which has the <strong>Administrator</strong> profile. Select <strong>Home</strong> -&gt; <strong>Projects</strong> and click <strong>Create Project</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/ec396478-05eb-4fa9-8862-c49c9321f21e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ec396478-05eb-4fa9-8862-c49c9321f21e"></a></p>
</li>
<li>
<p>In the resulting form, enter <strong>lab2-demo</strong> as the project name and click <strong>Create</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/6ece11da-db12-4ddb-ae79-9334004ccdd1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/6ece11da-db12-4ddb-ae79-9334004ccdd1"></a></p>
</li>
</ol>
<h3 id="lab2-lab2-hands-on-guide-deploy-milvus">Deploy milvus<a class="headerlink" href="#lab2-lab2-hands-on-guide-deploy-milvus" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Navigate to the terminal window where we had setup <code>oc</code> CLI on the bastion node.</p>
<div class="admonition warning">
<p class="admonition-title">Ensure <code>oc</code> CLI is authenticated with the cluster</p>
<p>Run a simple command: <code>oc project</code> and if it throws error you need re-authenticate with the cluster. Follow the steps mentioned in <a href="https://ibm.github.io/AI-on-Power-Level3/lab-setup/#logging-in-to-openshift-cluster-using-oc-cli" target="_blank">lab instructions</a> to ensure <code>oc</code> is authenticated with the cluster.</p>
</div>
</li>
<li>
<p>Make sure you are in the home directory and then run the command below to clone the github repository. Then switch to the newly cloned repository directory.</p>
<ul>
<li><code>cd</code></li>
<li><code>git clone https://github.com/dpkshetty/bcn-lab-2084</code></li>
<li><code>cd bcn-lab-2084/</code></li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/87e39369-ebbc-4fbd-a130-214f04ee4212" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/87e39369-ebbc-4fbd-a130-214f04ee4212"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/ba5c7a65-25f2-4073-878d-1fcd5d72030a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ba5c7a65-25f2-4073-878d-1fcd5d72030a"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/65fecf4b-0328-4df4-b28a-fd255db495f5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/65fecf4b-0328-4df4-b28a-fd255db495f5"></a></p>
</li>
<li>
<p>Make sure `oc CLI is pointing to the <strong>lab2-demo</strong> project.</p>
<p><code>oc project lab2-demo</code>   </p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/721a4e9d-81db-48e8-b85c-c6491e96a8a1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/721a4e9d-81db-48e8-b85c-c6491e96a8a1"></a></p>
</li>
<li>
<p>Run the below set of commands to deploy milvus DB.</p>
<p><code>cd Part2-RAG/milvus-deployment</code></p>
<p><code>oc create configmap milvus-config --from-file=./config/milvus.yaml</code></p>
<p><code>oc apply -f .</code></p>
<p><code>cd ..</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/0b632d95-3af0-4b48-a883-31085455370f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/0b632d95-3af0-4b48-a883-31085455370f"></a></p>
</li>
<li>
<p>Monitor deployment using the below command until all pods are in <strong>Running</strong> state. <br>
   Hit Ctrl-C on the keyboard to exit and come back to the shell prompt.</p>
<p><code>oc get pods -w</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/5289e0ef-3322-4206-9da3-ef833db0608e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5289e0ef-3322-4206-9da3-ef833db0608e"></a></p>
</li>
</ol>
<h3 id="lab2-lab2-hands-on-guide-deploy-jupyter-notebook">Deploy jupyter notebook<a class="headerlink" href="#lab2-lab2-hands-on-guide-deploy-jupyter-notebook" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Run the below set of commands to deploy jupyter notebook (NB).<br>Ignore any warnings if seen.</p>
<p><code>cd nb-deployment</code></p>
<p><code>oc apply -f .</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/6868a6eb-bcc8-4a34-8ed0-172fd536feb9" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/6868a6eb-bcc8-4a34-8ed0-172fd536feb9"></a></p>
</li>
<li>
<p>Verify the notebook pod is running using the command below. Hit Ctrl-C on keyboard to exit and return back to shell prompt.</p>
<p><code>oc get pods --selector=app=cpu-notebook -w</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/3b7b1764-2135-4192-854e-0144068673b0" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/3b7b1764-2135-4192-854e-0144068673b0"></a></p>
</li>
<li>
<p>Once the notebook pod is deployed you should be able to access it using the link retrieved from the below command:</p>
<p><code>oc get route cpu-notebook -o jsonpath='{.spec.host}'</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/be511e8a-15e9-44af-8e40-6079cc266af6" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/be511e8a-15e9-44af-8e40-6079cc266af6"></a></p>
<p>In my case, the URL was: <br>
   <code>cpu-notebook-lab2-demo.apps.p1279.cecc.ihost.com</code> <br>
 but yours can be different! <br></p>
<div class="admonition note">
<p class="admonition-title">Alternate way to get the jupyter NB URL</p>
<p>You can also goto OpenShift <strong>Administrator</strong> profile console window in your browser, navigate to <strong>Networking</strong> -&gt; <strong>Routes</strong>, select <strong>cpu-notebook</strong> route and click on the URL mentioned under <strong>Location</strong> field.</p>
</div>
</li>
<li>
<p>Copy and paste the URL in the browser. You should see the jupyter screen as below:
    <a class="glightbox" href="https://github.com/user-attachments/assets/ee5cf9c5-8f3f-48d4-8741-08d7ae5617ab" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ee5cf9c5-8f3f-48d4-8741-08d7ae5617ab"></a></p>
</li>
<li>
<p>Now let's copy the jupyter NB (.ipynb file) present in the git repository to the NB pod. <br>
   In your <code>oc</code> CLI terminal window, navigate to the root of your git repository which has the <strong>RAG.ipynb</strong> file.</p>
<p><code>cd /home/cecuser/bcn-lab-2084</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/3e7774f6-18c3-49bd-b2ff-dc36740fc121" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/3e7774f6-18c3-49bd-b2ff-dc36740fc121"></a></p>
</li>
<li>
<p>List pods and copy the name of the cpu-notebook pod.</p>
<p><code>oc get pods</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/d6ac1e0d-408a-45e5-9262-c5217d08dd35" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d6ac1e0d-408a-45e5-9262-c5217d08dd35"></a></p>
</li>
<li>
<p>Use <strong>oc cp ...</strong> command to copy the NB file from bastion server to <strong>/tmp/notebooks/</strong> directory of the NB pod.</p>
<p><code>oc cp ./RAG.ipynb cpu-notebook:/tmp/notebooks/</code></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/fb351616-347f-489f-890a-258fc4bea196" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/fb351616-347f-489f-890a-258fc4bea196"></a></p>
</li>
<li>
<p>Go back to the jupyter NB application in your browser and hit refresh (F5 shortcut in keyboard). You should be able to see the <strong>RAG.ipynb</strong> file listed.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/e7e52a0c-a840-4a3d-b2ad-954040ced4ad" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e7e52a0c-a840-4a3d-b2ad-954040ced4ad"></a></p>
</li>
<li>
<p>Double-click on the <strong>RAG.ipynb</strong> file and it should open up in the right pane of the browser.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/e5e26a7c-33d7-4c9f-acaf-fce3290d6b3d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e5e26a7c-33d7-4c9f-acaf-fce3290d6b3d"></a></p>
</li>
<li>
<p>You're all set! Follow the instructions given in the NB to finish this lab.</p>
</li>
</ol></div></section><h1 class='nav-section-title-end'>Ended: Lab2 - Deploy RAG on Power10</h1>
                        <h1 class='nav-section-title' id='section-lab3-deploy-code-llm-on-power10'>
                            Lab3 - Deploy code LLM on Power10 <a class='headerlink' href='#section-lab3-deploy-code-llm-on-power10' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="lab3-lab3-overview"><div><h1 id="generate-sql-query-using-code-llm-on-ibm-power10-lab-education">Generate SQL query using code LLM on IBM Power10 - Lab education<a class="headerlink" href="#lab3-lab3-overview-generate-sql-query-using-code-llm-on-ibm-power10-lab-education" title="Permanent link">¶</a></h1>
<p>Goal of this lab is to acquire some hands-on experience with code LLMs and understand their use cases.
In this lab, we will use a code LLM to convert a natural language query into a SQL statement which can be used to query DBs.
We will also to generate some python and C++ code and understand how better prompting can generate code closer to your expectations.</p>
<p>But before we do that, lets understand what code LLM is, its benefits, key use cases and IBM products around code LLMs.</p>
<h2 id="lab3-lab3-overview-what-is-code-llm">What is code LLM?<a class="headerlink" href="#lab3-lab3-overview-what-is-code-llm" title="Permanent link">¶</a></h2>
<p>A Code LLM (Large Language Model) is a type of AI model specifically trained to understand, generate, and manipulate programming code.
These models, built on architectures like GPT or similar transformers, are trained on vast datasets of code from various programming languages, enabling them to assist with coding tasks such as generating code snippets, debugging, refactoring, and even writing documentation.</p>
<h2 id="lab3-lab3-overview-key-features-of-code-llm">Key Features of Code LLM<a class="headerlink" href="#lab3-lab3-overview-key-features-of-code-llm" title="Permanent link">¶</a></h2>
<ul>
<li><strong>Code Generation</strong>: Code LLMs can generate new code based on prompts, such as writing functions, classes, or even entire programs.</li>
<li><strong>Code Completion</strong>: Similar to how text-based LLMs suggest completions for sentences, code LLMs can suggest code completions in real-time, aiding developers in writing code faster.</li>
<li><strong>Bug Detection and Fixes</strong>: These models can detect bugs or potential issues in code and suggest corrections.</li>
<li><strong>Multi-Language Support</strong>: Code LLMs are typically trained on multiple programming languages, allowing them to work across various tech stacks (for example, Python, Java, JavaScript, C++, etc.).</li>
<li><strong>Code Explanation</strong>: Some models can explain how a piece of code works, making them useful for learning, documentation, and debugging.</li>
</ul>
<h2 id="lab3-lab3-overview-use-cases">Use Cases<a class="headerlink" href="#lab3-lab3-overview-use-cases" title="Permanent link">¶</a></h2>
<ul>
<li><strong>Autocompletion in IDEs</strong>: Code LLMs like GitHub Copilot, powered by OpenAI's Codex, assist developers by offering code suggestions and completions.</li>
<li><strong>Code Refactoring</strong>: LLMs can suggest improvements or optimizations to existing code, helping improve performance or readability.</li>
<li><strong>Debugging and Error Resolution</strong>: Code LLMs can help identify issues in code and suggest potential fixes, streamlining the debugging process.</li>
<li><strong>Automated Documentation</strong>: They can generate comments or documentation for code, reducing the manual effort required for explaining code logic.</li>
</ul>
<h2 id="lab3-lab3-overview-examples-of-code-llms">Examples of Code LLMs<a class="headerlink" href="#lab3-lab3-overview-examples-of-code-llms" title="Permanent link">¶</a></h2>
<p>Below are few examples of code LLMs:</p>
<ul>
<li><strong>OpenAI Codex</strong>: The LLM that powers GitHub Copilot, trained specifically on a large corpus of programming languages and capable of generating code.</li>
<li><strong>AlphaCode</strong>: DeepMind's LLM designed for competitive programming tasks.</li>
<li><strong>CodeT5</strong>: A transformer model from Hugging Face fine-tuned for code generation and understanding tasks.</li>
<li><strong>Code Llama</strong>: It is a large language model developed by Meta, optimized for coding tasks such as code generation, completion, and debugging, supporting multiple programming languages for enhanced developer productivity.</li>
<li><strong>IBM Granite code models</strong>: A family of open foundation models for code intelligence. It is an enterprise-focused large language model developed by IBM Research, designed to enhance productivity in software development by enabling code generation, debugging, and refactoring with a strong emphasis on security and performance.</li>
</ul>
<h2 id="lab3-lab3-overview-benefits-of-code-llms">Benefits of code LLMs<a class="headerlink" href="#lab3-lab3-overview-benefits-of-code-llms" title="Permanent link">¶</a></h2>
<ul>
<li><strong>Boosts Developer Productivity</strong>: By automating repetitive coding tasks, Code LLMs help developers write code faster and reduce errors.</li>
<li><strong>Assists with Learning</strong>: Code LLMs are valuable for learners by explaining how code works and suggesting how to fix bugs.</li>
<li><strong>Cross-Language Compatibility</strong>: Supporting multiple languages, Code LLMs help developers work across different coding environments without needing expertise in all languages.</li>
</ul>
<p>In essence, Code LLMs bring AI-driven enhancements to the software development process, making it faster, more accurate, and more accessible.</p>
<h2 id="lab3-lab3-overview-ibm-watsonx-code-assistant">IBM watsonx Code Assistant<a class="headerlink" href="#lab3-lab3-overview-ibm-watsonx-code-assistant" title="Permanent link">¶</a></h2>
<p>IBM watsonx™ Code Assistant (WCA) is a family of offerings from IBM that leverages generative AI to accelerate development while maintaining the principles of trust, security and compliance at its core. Developers and IT Operators can speed up application modernization efforts and generate automation to rapidly scale IT environments. </p>
<p>WCA is powered by the IBM Granite foundation models that include state-of-the-art large language models designed for code, geared to help IT teams create high-quality code using AI-generated recommendations based on natural language requests or existing source code. Built on the watsonx AI platform, this tool leverages IBM’s expertise in AI and machine learning to enhance productivity across various industries.</p>
<div class="admonition danger">
<p class="admonition-title">IBM watsonx Code Assistant (WCA) support on IBM Power</p>
<p>NOTE: At the time of writing this lab, IBM WCA is not available to run on IBM Power yet. However, clients can use code specific foundation models (either hosted by WCA on IBM Cloud or running stand-alone on-premises on IBM Power) to infuse and harness the power of code LLMs into their on-premises applications running on IBM Power.</p>
</div>
<h3 id="lab3-lab3-overview-ibm-wca-architecture-offerings">IBM WCA architecture &amp; offerings<a class="headerlink" href="#lab3-lab3-overview-ibm-wca-architecture-offerings" title="Permanent link">¶</a></h3>
<p>Here is a high level architecture of IBM WCA:
<a class="glightbox" href="https://github.com/user-attachments/assets/71decd7c-fc8e-46a0-8765-ecfc38b897d4" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/71decd7c-fc8e-46a0-8765-ecfc38b897d4"></a>
At the bottom-most layer, WCA is powered by IBM Granite code models, specifically trained and fine-tuned by IBM for different programming languages.</p>
<p>At the time of writing this lab, there are 2 products available under the IBM WCA offering family:</p>
<ul>
<li>
<p><strong>IBM watsonx Code Assistant for Red Hat Ansible Lightspeed</strong>: The Ansible-tuned model forms the basis of this product. This is fine-tuned for Ansible use cases. In addition to x86 endpoints, this supports generating Ansible tasks/playbooks for IBM Power endpoints (running AIX, IBM i and Linux) as well.</p>
</li>
<li>
<p><strong>IBM watsonx Code Assistant for Z</strong>: The Cobol to Java-tuned model forms the basis of this product. This is fine-tuned for COBOL to Java conversion. It can help with enterprise use cases around IBM Z application modernization.</p>
</li>
</ul>
<p>More and more programming languages support is being added to WCA over time.</p>
<h3 id="lab3-lab3-overview-key-features-of-ibm-wca">Key Features of IBM WCA<a class="headerlink" href="#lab3-lab3-overview-key-features-of-ibm-wca" title="Permanent link">¶</a></h3>
<ol>
<li><strong>Code Generation</strong>: Automatically generates code snippets or complete functions based on natural language prompts or specific programming needs.</li>
<li><strong>Debugging Assistance</strong>: Helps identify bugs and suggests fixes, improving the efficiency of the development process.</li>
<li><strong>Multi-Language Support</strong>: Supports a variety of programming languages, making it versatile for different development environments.</li>
<li><strong>Security and Compliance</strong>: Designed with a strong focus on enterprise-level security, ensuring that generated code adheres to industry standards and compliance regulations.</li>
<li><strong>Customization</strong>: Tailors AI recommendations to the specific needs of a project, enabling more accurate code suggestions and personalized developer assistance.</li>
</ol>
<h3 id="lab3-lab3-overview-benefits-of-ibm-wca">Benefits of IBM WCA<a class="headerlink" href="#lab3-lab3-overview-benefits-of-ibm-wca" title="Permanent link">¶</a></h3>
<ol>
<li><strong>Increased Developer Productivity</strong>: Automates routine coding tasks, allowing developers to focus on higher-level problem-solving.</li>
<li><strong>Enhanced Code Quality</strong>: Improves the accuracy of code with AI-powered insights, reducing the chances of errors or vulnerabilities.</li>
<li><strong>Enterprise-Grade Security</strong>: Ensures that code generation is secure and compliant with regulations, making it suitable for industries like finance and healthcare.</li>
</ol></div></section><section class="print-page" id="lab3-lab3-hands-on-guide"><div><h1 id="use-natural-language-to-generate-code-using-code-llm-on-ibm-power10-lab-guide">Use natural language to generate code using code LLM on IBM Power10 - Lab guide<a class="headerlink" href="#lab3-lab3-hands-on-guide-use-natural-language-to-generate-code-using-code-llm-on-ibm-power10-lab-guide" title="Permanent link">¶</a></h1>
<h2 id="lab3-lab3-hands-on-guide-lab-ready-check">Lab-ready check<a class="headerlink" href="#lab3-lab3-hands-on-guide-lab-ready-check" title="Permanent link">¶</a></h2>
<p>Make sure you have the following items ready:</p>
<ul>
<li>In your browser, you have logged in as cecuser in the OpenShift console.</li>
<li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI oc.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not proceed further unless the items mentioned above are ready. Please refer to "Lab setup instructions" section (see left hand side menu) to setup your browser and terminal windows.</p>
</div>
<h2 id="lab3-lab3-hands-on-guide-lab-guide">Lab guide<a class="headerlink" href="#lab3-lab3-hands-on-guide-lab-guide" title="Permanent link">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Image zoom functionality</p>
<p>Feel free to click on the images in the lab guide below to a view larger image.</p>
</div>
<h3 id="lab3-lab3-hands-on-guide-deploy-granite-code-llm">Deploy granite code LLM<a class="headerlink" href="#lab3-lab3-hands-on-guide-deploy-granite-code-llm" title="Permanent link">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Pre-requisite</p>
<p>This lab assumes you have finished Lab1. This lab uses the OpenShift resources deployed in Lab1 to optimize the usage of TechZone resources and to avoid re-deploying the same resources and re-learning the same concepts already taught in Lab1!</p>
</div>
<ol>
<li>
<p>In this lab, we will deploy IBM's granite code LLM which is available on HF. Navigate to OpenShift developer profile window and ensure you are in <strong>lab1-demo</strong> project. Click <strong>Topology</strong> and ensure that your application is healthy and running (has a blue circle) before proceeding further.
   <a class="glightbox" href="https://github.com/user-attachments/assets/999accf3-e5b2-4a38-85d3-458ec024247c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/999accf3-e5b2-4a38-85d3-458ec024247c"></a></p>
</li>
<li>
<p>Select <strong>ConfigMaps</strong> and click <strong>model-params</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/c00aaf7f-caf5-4e95-8706-895ade37aee5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c00aaf7f-caf5-4e95-8706-895ade37aee5"></a></p>
</li>
<li>
<p>Click on <strong>Actions</strong> -&gt; <strong>Edit ConfigMap</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/e900c672-ee49-4f9d-94f2-f28bbe1aef20" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e900c672-ee49-4f9d-94f2-f28bbe1aef20"></a></p>
</li>
<li>
<p>In the resulting form, the key/value fields for MODEL_NAME and MODEL_URL as below and click <strong>Save</strong>.</p>
<ul>
<li>Key: MODEL_NAME</li>
<li>Value: granite-8b-code-instruct.Q4_K_M.gguf</li>
</ul>
<hr>
<ul>
<li>Key: MODEL_URL</li>
<li>Value: https://huggingface.co/ibm-granite/granite-8b-code-instruct-4k-GGUF/resolve/main/granite-8b-code-instruct.Q4_K_M.gguf     </li>
</ul>
<hr>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/395d95b2-bd4f-4163-980a-536acf9c4877" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/395d95b2-bd4f-4163-980a-536acf9c4877"></a></p>
<hr>
</li>
<li>
<p>Use the deployment resource to restart the pods.</p>
<div class="admonition note">
<p class="admonition-title">ConfigMap update does not restart pods automatically</p>
<p>The existing pod won't see the ConfigMap changes right away as changing values of a ConfigMap doesn't cause a deployment (and hence pod) to restart automatically. It needs to be done manually.</p>
</div>
</li>
<li>
<p>Navigate to the deployment view. Click <strong>Topology</strong>, then click "<strong>D lab1-demo</strong>" part of the application icon, which will open up the deployment details pane (on the right hand side of the browser window). Click "<strong>D lab1-demo</strong>" in that pane which will then open up the deployment details view for lab1-demo deployment.
   <a class="glightbox" href="https://github.com/user-attachments/assets/c1c50473-0673-48e9-a2b8-54611fd67ead" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c1c50473-0673-48e9-a2b8-54611fd67ead"></a></p>
</li>
<li>
<p>Click <strong>Actions</strong> and select <strong>Restart rollout</strong>. This will restart the deployment which results in re-deployment of the pod. This ensure the pod will now see the new ConfigMap changes.
   <a class="glightbox" href="https://github.com/user-attachments/assets/51476c1a-52a2-4fbe-ae28-9dca9348ac4f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/51476c1a-52a2-4fbe-ae28-9dca9348ac4f"></a></p>
</li>
<li>
<p>Click on <strong>Pods</strong> tab, where you will see a new pod instantiated. The new pod will download the model and then start it. Since the configmap points to granite code model, it will be downloaded from HuggingFace and then started. When that happens the new pod's status will change to Running and the existing pod will be terminated.
    <a class="glightbox" href="https://github.com/user-attachments/assets/3eb442b4-08a5-4bec-8e24-c4f2e1b57926" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/3eb442b4-08a5-4bec-8e24-c4f2e1b57926"></a></p>
<div class="admonition info">
<p class="admonition-title">Model download will take time - Have patience!!</p>
<p>This process will take a few minutes (in my case it took around 3-4 mins as this is a fairly large model compared to tinyllama) and your mileage may vary! Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV).</p>
</div>
</li>
<li>
<p>If all goes well, you should see just 1 pod running.
    <a class="glightbox" href="https://github.com/user-attachments/assets/32fb028f-fef6-4abe-996c-b1bdfbe80489" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/32fb028f-fef6-4abe-996c-b1bdfbe80489"></a></p>
</li>
<li>
<p>Let's verify that the model running is granite code LLM!. Click on the pod to enter the pod details view/page.
    <a class="glightbox" href="https://github.com/user-attachments/assets/74893247-6c4b-444f-9163-97ec3eb3e934" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/74893247-6c4b-444f-9163-97ec3eb3e934"></a></p>
</li>
<li>
<p>In the pod details page, click on the Logs tab to see the pod logs.
    <a class="glightbox" href="https://github.com/user-attachments/assets/ac49a5bb-bbf0-4ac1-9050-a88327185c18" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ac49a5bb-bbf0-4ac1-9050-a88327185c18"></a></p>
</li>
<li>
<p>In the log window, scroll upwards to see the name of the model against the attribute <strong>llm_load_print_meta: general.name</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/a84291e2-c952-4710-a2cd-88caed8b4dd2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a84291e2-c952-4710-a2cd-88caed8b4dd2"></a></p>
<p>This verifies that we have indeed deployed granite code LLM.</p>
</li>
<li>
<p>Let's access our model. As we did in Lab1, head to <strong>Topology</strong> view, click on the <strong>Open URL</strong> icon of your application.
    <a class="glightbox" href="https://github.com/user-attachments/assets/d48c3a0e-9d52-4a24-8fbf-8776b38ead5f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d48c3a0e-9d52-4a24-8fbf-8776b38ead5f"></a></p>
</li>
<li>A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:
    <a class="glightbox" href="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53"></a></li>
<li>
<p>Scroll all the way down to the input field "Say something..." where you can interact with the LLM. You can ask any question you like!
    <a class="glightbox" href="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce"></a></p>
<div class="admonition note">
<p class="admonition-title">Experimenting with model parameters</p>
<p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Feel free to google and learn about them and experiment with it. You may want to change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab.</p>
</div>
</li>
</ol>
<h3 id="lab3-lab3-hands-on-guide-generate-python-code">Generate python code<a class="headerlink" href="#lab3-lab3-hands-on-guide-generate-python-code" title="Permanent link">¶</a></h3>
<p>Now let's use the granite code model to generate python code using natural language queries.</p>
<ol>
<li>
<p>Generating python code for printing the fibonacci series.<br> Here is what I entered: <code>python for finonacci sequence</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/031c1d4a-d700-4f2a-89ec-87e49b1cc8f3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/031c1d4a-d700-4f2a-89ec-87e49b1cc8f3"></a></p>
<ul>
<li>
<p>I ran this code on my local python environment and it ran without errors!</p>
</li>
<li>
<p>Note that I spelled fibonacci incorrectly, yet it understood my question correctly.</p>
</li>
<li>
<p>Also note that the answer it gave uses recursion (function fib is being called recursively).</p>
</li>
</ul>
</li>
<li>
<p>Now let's try asking it to generate the same code without using recursion.<br> Here is what I entered: <code>give me python code for generating fibonacci sequence without using recursion</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/74f1ce36-7aee-40ac-8444-8e212402745b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/74f1ce36-7aee-40ac-8444-8e212402745b"></a></p>
<ul>
<li>
<p>As expected, it did give the code snippet that doesn't use recursion, so it did well there.</p>
</li>
<li>
<p>Generated code is not 100% correct. I ran this code on my local python environment and it ran into some issues and had to fix some code to make it generate the right fibonacci sequence.</p>
</li>
<li>
<p>The above shows that code LLMs can generate almost perfect code and in some cases it might need developer intervention to make it perfect!</p>
</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Accuracy of code LLMs</p>
<ul>
<li>LLMs excel at generating simple or boilerplate code, often producing highly accurate results for common tasks such as sorting algorithms, basic input/output operations, or template-based functions.</li>
<li>For routine tasks and widely-used languages like Python or JavaScript, accuracy rates can be high, sometimes upwards of 80-90% for straightforward problems.</li>
<li>When dealing with more complex algorithms, nuanced edge cases, or multi-step logic, LLMs may struggle. The model can produce syntactically correct code that might not solve the problem as intended or might have logic errors.</li>
<li>For complex use cases, accuracy can drop significantly, often requiring human intervention to correct the output.</li>
<li>AI-generated content may vary and may not always provide consistent answers. Your response may be different than what I got.</li>
</ul>
</div>
</li>
</ol>
<p>Feel free to explore and try out more scenarios!</p>
<h3 id="lab3-lab3-hands-on-guide-generate-c-code">Generate C code<a class="headerlink" href="#lab3-lab3-hands-on-guide-generate-c-code" title="Permanent link">¶</a></h3>
<p>Now let's use the granite code model to generate C code using natural language queries.</p>
<ol>
<li>
<p>Generate C code for sorting a list of numbers.<br> Here is what I entered: <code>C code to sort a list of numbers</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/57ae7cf6-2d8f-4b20-8618-5c19fe2b833b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/57ae7cf6-2d8f-4b20-8618-5c19fe2b833b"></a></p>
<ul>
<li>I ran this code on my local C environment and it ran without errors! Ofcourse I had to fix the first line (which was incomplete) as <code>#include &lt;stdio.h&gt;</code>.</li>
</ul>
</li>
<li>
<p>Generate C code for swapping 2 numbers without using a temporary variable.<br> Here is what I entered: <code>C code for swapping 2 numbers</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/86fe9b4b-0684-45c0-a3da-c159121384ef" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/86fe9b4b-0684-45c0-a3da-c159121384ef"></a></p>
<ul>
<li>I ran this code on my local C environment and it ran without errors! Ofcourse I had to fix the first line (which was incomplete) as <code>#include &lt;stdio.h&gt;</code>.</li>
</ul>
<p><strong>NOTE</strong>: AI-generated content may vary and may not always provide consistent answers. Your response may be different than what I got.<br>
   Feel free to explore and try out more scenarios!</p>
</li>
</ol>
<h3 id="lab3-lab3-hands-on-guide-generate-sql-query">Generate SQL query<a class="headerlink" href="#lab3-lab3-hands-on-guide-generate-sql-query" title="Permanent link">¶</a></h3>
<p>Generating SQL query using natural langguage is a little different than C or python code since SQL is a language to query Databases (DBs). One needs to give enough context to the code LLM for it to understand the DB table schemas for which you want it to generate the SQL query.</p>
<p>The context is given as a prompt to the code LLM which preceeds the natural language query and the code LLM answers the query using the context given in the prompt. The best way to understand is looking at some examples as depicted below.</p>
<p>Let's take an super simple example of a bank which has information stored in 2 tables in a DB:</p>
<ul>
<li>USERS - This which has user specific info (user_id, name, age, dob etc).</li>
<li>ACCOUNTS - This holds the account balance of each user with user_id being the common field between the 2 tables.</li>
</ul>
<ol>
<li>Given the above DB example, the prompt (also known as context) we need to provide to the code LLM is as below:
   <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#lab3-lab3-hands-on-guide-__codelineno-0-1"></a>You are a developer writing SQL queries given natural language questions. The database contains a set of 2 tables. The schema of each table with description of the attributes is given. Write the SQL query given a natural language statement with names being not case sensitive.
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#lab3-lab3-hands-on-guide-__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#lab3-lab3-hands-on-guide-__codelineno-0-3"></a> Here are the 2 tables :
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#lab3-lab3-hands-on-guide-__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#lab3-lab3-hands-on-guide-__codelineno-0-5"></a> (1) Database Table Name: USERS
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#lab3-lab3-hands-on-guide-__codelineno-0-6"></a> Table Schema:
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#lab3-lab3-hands-on-guide-__codelineno-0-7"></a> Column Name # Meaning
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#lab3-lab3-hands-on-guide-__codelineno-0-8"></a> user_id # unique identifier of the user
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#lab3-lab3-hands-on-guide-__codelineno-0-9"></a> user_name # name of the user
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#lab3-lab3-hands-on-guide-__codelineno-0-10"></a> usertypeid # user is '\''employee'\'', '\''customer'\''
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#lab3-lab3-hands-on-guide-__codelineno-0-11"></a> gender_id # user'\''s gender is 1 for female, 2 for male and 3 for other
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#lab3-lab3-hands-on-guide-__codelineno-0-12"></a> dob # date of birth of the user
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#lab3-lab3-hands-on-guide-__codelineno-0-13"></a> address # adress of the user
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#lab3-lab3-hands-on-guide-__codelineno-0-14"></a> state # state of the user
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#lab3-lab3-hands-on-guide-__codelineno-0-15"></a> country # country of residence of the user
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#lab3-lab3-hands-on-guide-__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#lab3-lab3-hands-on-guide-__codelineno-0-17"></a> (2) Database Table Name: ACCOUNTS
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#lab3-lab3-hands-on-guide-__codelineno-0-18"></a> Table Schema:
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#lab3-lab3-hands-on-guide-__codelineno-0-19"></a> Column Name # Meaning
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#lab3-lab3-hands-on-guide-__codelineno-0-20"></a> acc_id # account number or account id of the user
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#lab3-lab3-hands-on-guide-__codelineno-0-21"></a> user_id # user id of the user
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#lab3-lab3-hands-on-guide-__codelineno-0-22"></a> balance # available balance in the account
</code></pre></div></li>
<li>
<p>Natural language query can be something like:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#lab3-lab3-hands-on-guide-__codelineno-1-1"></a>With the above schema, please generate sql query to list all users whose balance is &gt; 2000.
</code></pre></div>
</li>
<li>
<p>Let's send the "Prompt + Query" to the code LLM and see how it responds. Feel free to copy and paste it in your LLM application window.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#lab3-lab3-hands-on-guide-__codelineno-2-1"></a>You are a developer writing SQL queries given natural language questions. The database contains a set of 2 tables. The schema of each table with description of the attributes is given. Write the SQL query given a natural language statement with names being not case sensitive.
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#lab3-lab3-hands-on-guide-__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#lab3-lab3-hands-on-guide-__codelineno-2-3"></a> Here are the 2 tables :
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#lab3-lab3-hands-on-guide-__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#lab3-lab3-hands-on-guide-__codelineno-2-5"></a> (1) Database Table Name: USERS
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#lab3-lab3-hands-on-guide-__codelineno-2-6"></a> Table Schema:
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#lab3-lab3-hands-on-guide-__codelineno-2-7"></a> Column Name # Meaning
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#lab3-lab3-hands-on-guide-__codelineno-2-8"></a> user_id # unique identifier of the user
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#lab3-lab3-hands-on-guide-__codelineno-2-9"></a> user_name # name of the user
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#lab3-lab3-hands-on-guide-__codelineno-2-10"></a> usertypeid # user is '\''employee'\'', '\''customer'\''
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#lab3-lab3-hands-on-guide-__codelineno-2-11"></a> gender_id # user'\''s gender is 1 for female, 2 for male and 3 for other
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#lab3-lab3-hands-on-guide-__codelineno-2-12"></a> dob # date of birth of the user
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#lab3-lab3-hands-on-guide-__codelineno-2-13"></a> address # adress of the user
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#lab3-lab3-hands-on-guide-__codelineno-2-14"></a> state # state of the user
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#lab3-lab3-hands-on-guide-__codelineno-2-15"></a> country # country of residence of the user
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#lab3-lab3-hands-on-guide-__codelineno-2-16"></a>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#lab3-lab3-hands-on-guide-__codelineno-2-17"></a> (2) Database Table Name: ACCOUNTS
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#lab3-lab3-hands-on-guide-__codelineno-2-18"></a> Table Schema:
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#lab3-lab3-hands-on-guide-__codelineno-2-19"></a> Column Name # Meaning
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#lab3-lab3-hands-on-guide-__codelineno-2-20"></a> acc_id # account number or account id of the user
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#lab3-lab3-hands-on-guide-__codelineno-2-21"></a> user_id # user id of the user
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#lab3-lab3-hands-on-guide-__codelineno-2-22"></a> balance # available balance in the account
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#lab3-lab3-hands-on-guide-__codelineno-2-23"></a>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#lab3-lab3-hands-on-guide-__codelineno-2-24"></a> With the above schema, please generate sql query to list all users whose balance is &gt; 2000
</code></pre></div>
<p><br></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/445f9928-074f-4a58-a5b4-a1f757910c11" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/445f9928-074f-4a58-a5b4-a1f757910c11"></a></p>
<ul>
<li>The SQL query generated seems correct.</li>
<li>Its joining both the tables using <code>user_id</code> as the key and selecting all records where the user's account balance is &gt; 2000.</li>
<li>For the sake of people who may want to analyse further, pasting the SQL query that was generated:  <code>SELECT * FROM USERS u, ACCOUNTS a WHERE u.user_id = a.user_id AND a.balance &gt; 2000;</code></li>
</ul>
</li>
<li>
<p>Interestingly, code LLM works both ways! Given a SQL query, you can ask code LLM to explain what it does. To do that I have formed the query as below. Feel free to copy the query and post it in your LLM application window.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#lab3-lab3-hands-on-guide-__codelineno-3-1"></a>What does the below SQL query do ?
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#lab3-lab3-hands-on-guide-__codelineno-3-2"></a>SQL Query:
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#lab3-lab3-hands-on-guide-__codelineno-3-3"></a>SELECT * FROM USERS u, ACCOUNTS a WHERE u.userid = a.userid AND a.balance &gt; 2000;
</code></pre></div>
<p><br></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/8576ca30-42b8-4647-bd92-e092d0c67aa8" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/8576ca30-42b8-4647-bd92-e092d0c67aa8"></a></p>
<p>That's a decent explanation of the SQL query!</p>
</li>
<li>
<p>Let's try one more example. Here I give it 2 conditions to match in the query.<br>
   NOTE: You don't have to repeat the whole schema in the prompt. LLMs can remember context.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#lab3-lab3-hands-on-guide-__codelineno-4-1"></a>Using the schema given above, generate SQL query to list all users whose account balance &gt; 2000 and user is of type employee.
</code></pre></div>
<p><br></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/2be00e6a-8d79-4f1b-8844-b6768b63d86a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2be00e6a-8d79-4f1b-8844-b6768b63d86a"></a></p>
<p>Response received as below:</p>
<p></p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#lab3-lab3-hands-on-guide-__codelineno-5-1"></a>Here's the SQL query to list all users whose account balance is greater than 2000 and they are of type employee from the USERS and ACCOUNTS tables:
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#lab3-lab3-hands-on-guide-__codelineno-5-2"></a>SELECT * FROM USERS u, ACCOUNTS a WHERE u.user_id = a.user_id AND a.balance &gt; 2000 AND usertypeid='employee';
</code></pre></div>
 <br>
<ul>
<li>The response is not 100% correct.</li>
<li>The last part of the SQL query <code>usertypeid='employee'</code> is ambiguous as the DB won't know which <code>usertypeid</code> column to reference.</li>
<li>The correct SQL query would have <code>u.usertypeid='employee'</code> so that the DB knows that its part of the USERS (aliased as <code>u</code> in the query) table.</li>
</ul>
<p><strong>NOTE</strong>: AI-generated content may vary and may not always provide consistent answers. Your response may be different than what I got.<br></p>
</li>
<li>
<p>Re-iterating some of the points we learned in this lab:</p>
<ul>
<li>
<p>Code LLMs are not 100% correct, yet they can be immensely helpful for a developer as they can help generate near perfect code which can then be analyzed &amp; tweaked to perfection by the developer.</p>
</li>
<li>
<p>There are many code LLMs available in HF with varied degree of accuracy and clients can choose the right one by experimenting with them in the context of their specific use case.</p>
</li>
<li>
<p>IBM also offers enterprise grade code LLMs via its watsonx Code Assistant (WCA) family of product offerings.</p>
</li>
</ul>
</li>
<li>
<p>IBM Power servers are best used as system of record (SoR) servers which means they hold a lot of enterprise specific data in different DBs (for example: Oracle on AIX, DB2 on AIX / IBM i, PostgreSQL on Linux etc). From an IBM Power solution point of view, an end to end solution to query DB records using natural language can be easily implemented which can help clients immensely. They don't need to depend on experts to generate DB reports. Even executives (with authorized access to the DB) can generate reports and/or view data using natural language queries.</p>
<ul>
<li>Check this ~2min short <a href="https://mediacenter.ibm.com/media/Infusing+AI+into+mission+critical+workloads+with+PowerVS+and+watsonx.ai/1_fzqutamr" target="_blank">video demo</a> on "Infusing AI into mission critical workloads with PowerVS and watsonx.ai" which uses natural language to query fraudulent transactions and list high value customers. Although this demo is based on PowerVS use case the same is applicable to on-premises as well.</li>
</ul>
</li>
</ol>
<p><strong>Summary</strong></p>
<p>Code LLMs democratize coding by making it more accessible to non-coders, accelerating the development process for smaller teams, and assisting both beginners and experienced developers in learning and improving productivity. By lowering technical barriers, they are transforming who can engage in software development and how innovation happens across industries.</p></div></section><h1 class='nav-section-title-end'>Ended: Lab3 - Deploy code LLM on Power10</h1><section class="print-page" id="resources"><div><h1 id="additional-learning-assets-optional-but-recommended">Additional learning assets (optional but recommended)<a class="headerlink" href="#resources-additional-learning-assets-optional-but-recommended" title="Permanent link">¶</a></h1>
<h2 id="resources-read-and-learn">Read and learn<a class="headerlink" href="#resources-read-and-learn" title="Permanent link">¶</a></h2>
<ul>
<li><a href="https://ibm.seismic.com/Link/Content/DC4HhmXXFPpFdGcTJJJb996JFR7V" target="_blank">AI on IBM Power - Seismic sales kit</a></li>
<li><a href="https://ibm.seismic.com/Link/Content/DCFbHqRHpq9d88cHj2dMWFffVP4P" target="_blank">AI on IBM Power - One Pager</a></li>
<li><a href="https://ibm.seismic.com/Link/Content/DC3CbdBWFpgTX8CDCFmg9JWmbMpV" target="_blank">AI on Power - Conversation Starters &amp; Opportunity Qualification Questions</a></li>
<li><a href="https://www.ibm.com/docs/en/watsonx/saas?topic=models-choosing-model" target="_blank">Choosing a generative AI model</a></li>
<li>Delivering an On-Prem Generative AI Chatbot with IBM Power - <a href="https://www.ibm.com/downloads/cas/YE3OVQNB" target="_blank">Whitepaper</a></li>
<li><a href="https://community.ibm.com/community/user/powerdeveloper/blogs/sebastian-lehrig/2024/03/26/sizing-for-ai" target="_blank">Tutorial - Sizing and configuring an LPAR for AI workloads</a></li>
</ul>
<h2 id="resources-watch-and-learn">Watch and learn<a class="headerlink" href="#resources-watch-and-learn" title="Permanent link">¶</a></h2>
<ul>
<li>TechZone demos<ul>
<li><a href="https://techzone.ibm.com/collection/ai-inferencing-on-ibm-power10-mma" target="_blank">AI Inferencing on IBM Power10 with MMA</a></li>
<li><a href="https://techzone.ibm.com/collection/generative-ai-demos-on-ibm-power" target="_blank">Generative AI demos on IBM Power</a></li>
</ul>
</li>
<li>RedHat Developer webinar hosted on YouTube - <a href="https://www.youtube.com/watch?v=qx6MHt24TrY" target="_blank">Implementing enterprise-ready generative AI</a></li>
<li><a href="https://ec.yourlearning.ibm.com/w3/playback/10467290" target="_blank">Panel discussion on AI performance on Power - Sept 2024</a> - IBMers only</li>
</ul></div></section><section class="print-page" id="credits"><div><h1 id="credits">Credits<a class="headerlink" href="#credits-credits" title="Permanent link">¶</a></h1>
<p>Sincere and heartfelt thanks to the below people who helped in various stages of creating this course:</p>
<ul>
<li>
<p><strong>Marvin Gießing</strong>: Lab1 and Lab2 are inspired by Marvin's existing work <a href="https://github.com/mgiessing/bcn-lab-2084/" target="_blank">here</a>.</p>
</li>
<li>
<p><strong>Jean Midot</strong>: For the support he provided on the TechZone front.</p>
</li>
<li>
<p><strong>Sebastian Lehrig</strong>, <strong>David Spurway</strong> &amp; <strong>DANIEL DE SOUZA CASALI</strong>: For all the technical guidance and support provided during the course.</p>
</li>
</ul></div></section><section class="print-page" id="support"><div><h1 id="support">Support<a class="headerlink" href="#support-support" title="Permanent link">¶</a></h1>
<p>Think something is down? Check the applicable status pages for any known issues like a site or service not available:</p>
<ul>
<li><a href="https://techzone.status.io/" target="_blank">IBM Technology Zone</a></li>
</ul>
<p>For issues with provisioning the ITZ environment for this lab (for example, a failed reservation request due to insufficient quota capacity) open a ticket with ITZ support:</p>
<ul>
<li>
<p>Web:  <a href="https://ibmsf.force.com/ibminternalproducts/s/createrecord/NewCase?language=en_US" target="_blank">IBM Technology Zone</a></p>
</li>
<li>
<p>Email: <a href="mailto:techzone.help@ibm.com" target="_blank">techzone.help@ibm.com</a></p>
</li>
</ul>
<p>For issues related to specific steps found in the demonstration guide after the ITZ environment is provisioned, contact the author(s):</p>
<ul>
<li>Email: <a href="mailto:deepakcshetty@in.ibm.com" target="_blank">Author</a></li>
</ul></div></section></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 IBM
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "/", "features": ["navigation.instant", "navigation.tracking", "content.code.annotate", "content.code.copy"], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
    
  </body>
</html>