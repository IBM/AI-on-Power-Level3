
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="AI on Power - Level 3 course">
      
      
        <meta name="author" content="Deepak C Shetty, Learning Content Developer, IBM Power">
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Print Course - AI on Power - Level 3</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.35f28582.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Sans:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"IBM Plex Sans";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings4.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings5.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings6.css">
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#helpful-tips" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AI on Power - Level 3" class="md-header__button md-logo" aria-label="AI on Power - Level 3" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI on Power - Level 3
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Course
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AI on Power - Level 3" class="md-nav__button md-logo" aria-label="AI on Power - Level 3" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AI on Power - Level 3
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../helpful-tips/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting started
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre-req/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prerequisites
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lab-setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab setup instructions
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lab 1 - Deploy a LLM on Power10
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Lab 1 - Deploy a LLM on Power10
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab1/lab1-overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab education
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab1/lab1-hands-on-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on guide
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lab 2 - Deploy RAG on Power10
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Lab 2 - Deploy RAG on Power10
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab2/lab2-overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab education
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab2/lab2-hands-on-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on guide
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lab 3 - Deploy code LLM on Power10
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Lab 3 - Deploy code LLM on Power10
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab3/lab3-overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lab education
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Lab3/lab3-hands-on-guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hands-on guide
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../resources/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Additional resources
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../credits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Credits
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../support/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Support
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Print Course
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Print Course
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#helpful-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Getting started
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pre-req" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Lab setup instructions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab-1-deploy-a-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 1 - Deploy a LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 1 - Deploy a LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab-2-deploy-rag-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 2 - Deploy RAG on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 2 - Deploy RAG on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab-3-deploy-code-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 3 - Deploy code LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 3 - Deploy code LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    <span class="md-ellipsis">
      Additional resources
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#credits" class="md-nav__link">
    <span class="md-ellipsis">
      Credits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#support" class="md-nav__link">
    <span class="md-ellipsis">
      Support
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#helpful-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Getting started
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pre-req" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lab-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Lab setup instructions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab-1-deploy-a-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 1 - Deploy a LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 1 - Deploy a LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab1-lab1-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab-2-deploy-rag-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 2 - Deploy RAG on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 2 - Deploy RAG on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab2-lab2-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-lab-3-deploy-code-llm-on-power10" class="md-nav__link">
    <span class="md-ellipsis">
      Lab 3 - Deploy code LLM on Power10
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lab 3 - Deploy code LLM on Power10">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lab education
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lab3-lab3-hands-on-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Hands-on guide
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    <span class="md-ellipsis">
      Additional resources
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#credits" class="md-nav__link">
    <span class="md-ellipsis">
      Credits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#support" class="md-nav__link">
    <span class="md-ellipsis">
      Support
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<div id="print-site-page" class="print-site-enumerate-figures">
        <section id="print-site-cover-page">
            
<div>

    
        <h1>AI on Power - Level 3</h1>
    

</div>


<table>

    
    <tr>
        <td>Description</td>
        <td>AI on Power - Level 3 course</td>
    </tr>
    

    
    <tr>
        <td>Author(s)</td>
        <td>Deepak C Shetty, Learning Content Developer, IBM Power</td>
    </tr>
    

    

    
    <tr>
        <td>Copyright</td>
        <td>Copyright &copy; 2024 IBM</td>
    </tr>
    

</table>





        </section>
        
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
</p>
<p>This page has combined all site pages into one. You can export to PDF using <b>File > Print > Save as PDF</b>.</p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="6">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index"><div><h1 id="introduction">Introduction<a class="headerlink" href="#index-introduction" title="Permanent link">¶</a></h1>
<p>Welcome to the AI on Power - Level 3 course - seller enablement demonstration.
This Level 3 course provides hands-on enablement on how to use generative AI (gen AI) models on Power10 and covers several use cases to help understand the art of the possible.</p>
<p>This course has 4 main parts as you can see on the left window:</p>
<ul>
<li>
<p>Lab setup - How to provision and setup the lab environment for the hands-on demos.</p>
</li>
<li>
<p>Lab 1 – Deploy a large language model (LLM) on Power10 and switch between LLMs.</p>
</li>
<li>
<p>Lab 2 – Deploy Retrieval-Augmented Generation (RAG) on Power10 and use it for domain-specific question and answers</p>
</li>
<li>
<p>Lab 3 – Deploy code LLM Power10 - Generate Python, C code, and Sequential Query Language (SQL) query by using natural language.</p>
</li>
</ul>
<h2 id="index-generative-ai-and-llms-overview">Generative AI and LLMs overview<a class="headerlink" href="#index-generative-ai-and-llms-overview" title="Permanent link">¶</a></h2>
<p>Large language models (LLMs) and gen AI has revolutionized industries by enabling machines to understand, generate, and interact with human language in ways that are never seen before. It has disrupted the market by:</p>
<ul>
<li>
<p><strong>Boosting productivity</strong>: LLMs streamline workflows by handling complex tasks, like writing, summarizing, and translating, allowing professionals to focus on more strategic, creative efforts.</p>
</li>
<li>
<p><strong>Automating content creation</strong>: From drafting articles, reports, and code to generating creative content like music and artwork, gen AI has drastically reduced time and costs in creative industries.</p>
</li>
<li>
<p><strong>Enhancing customer experience</strong>: LLM-powered chatbots and virtual assistants are offering more personalized, human-like interactions, reshaping customer service and support.</p>
</li>
<li>
<p><strong>Transforming decision-making</strong>: With advanced data processing and natural language understanding, businesses are using AI for smarter, faster insights and predictive analytics. </p>
</li>
</ul>
<p>In short, LLMs and gen AI are reshaping industries by enhancing efficiency, creativity, and customer engagement, making them indispensable tools in the modern business landscape.</p>
<h2 id="index-why-ibm-power-for-generative-ai">Why IBM Power for generative AI<a class="headerlink" href="#index-why-ibm-power-for-generative-ai" title="Permanent link">¶</a></h2>
<p>IBM Power platform is a high-performance, enterprise-grade computing solution that is designed to handle demanding workloads such as AI, analytics, cloud, and mission-critical applications. With unmatched processing power, scalability, and reliability, IBM Power enables organizations to optimize performance, lower costs, and ensure security.</p>
<h3 id="index-key-benefits">Key benefits<a class="headerlink" href="#index-key-benefits" title="Permanent link">¶</a></h3>
<ul>
<li>
<p><strong>Unmatched performance</strong>: Optimized for data-intensive applications, AI, and large-scale analytics, delivering faster insights and better decision-making.</p>
</li>
<li>
<p><strong>Scalability</strong>: Seamlessly scale workloads across on-premises, cloud, and hybrid environments to meet evolving business needs.</p>
</li>
<li>
<p><strong>Enterprise-grade security</strong>: Built-in security features provide robust protection for sensitive data and critical operations.</p>
</li>
<li>
<p><strong>Efficiency and flexibility</strong>: Designed for diverse environments, supporting Linux, AIX, IBM i, and containerized applications, making it versatile for a range of industries.</p>
</li>
</ul>
<p>IBM Power is the platform of choice for businesses seeking to future-proof their IT infrastructure, handle complex workloads, and accelerate innovation.</p>
<h2 id="index-ibm-power10-differentiation">IBM Power10 differentiation<a class="headerlink" href="#index-ibm-power10-differentiation" title="Permanent link">¶</a></h2>
<p>IBM Power10 offers several advantages for gen AI use cases, providing the computational power, security, and efficiency that is required to support the demanding workloads of AI-driven applications.</p>
<h3 id="index-key-advantages">Key advantages<a class="headerlink" href="#index-key-advantages" title="Permanent link">¶</a></h3>
<ol>
<li>
<p><strong>AI acceleration:</strong> IBM Power10 comes with several features that accelerate gen AI workloads by using high memory bandwidth and on-chip acceleration:</p>
<ul>
<li>
<p><strong>High-bandwidth data-path:</strong> As data fuels AI, a fast access to big data for training and inferencing is required. Most AI workloads are memory bandwidth bound. IBM Power10's large memory capacity that, in general exceeds limited Graphics Processing Unit (GPU) memory and IBM Power10’s high memory bandwidth are optimal for such scenarios. In addition, the Double Data Rate 5 (DDR5) memory and faster input/output (I/O) bandwidth provide a significant performance boost, enabling large-scale gen AI models to run more efficiently.</p>
</li>
<li>
<p><strong>4 Matrix Math Accelerator (MMA) engines per core:</strong> MMA does matrix math and helps accelerate matrix multiplications, which are required for training, fine-tuning, and inferencing of AI models such as neural networks and foundation models. IBM is seeing strong evidence that it supersedes GPUs and improves consolidation when deploying AI at point of use. </p>
</li>
<li>
<p><strong>8 Single Instruction Multiple Data (SIMD) engines per core:</strong> SIMD does vector math. Vectors are used in most AI algorithms. Vector processing can be highly parallelized by using SIMD engines. A single inference request is often “just” a vector that is “send through” the LLM using SIMD acceleration. By batching multiple inference requests together, the workload can become MMA-bound where a whole matrix is "send through" the LLM, leading to improved throughputs.</p>
</li>
</ul>
</li>
<li>
<p><strong>Optimized AI software:</strong> AI software with IBM Power10 is optimized down to the core.</p>
<ul>
<li>AI software running on top of IBM Power10 hardware fully leverages the AI acceleration features, without requiring data scientists to alter their code – optimization comes right away.</li>
<li>The optimized AI software portfolio spans from enterprise options to supported open source options; even hybrid approaches that mix enterprise and supported open source are possible.</li>
<li>This flexibility allows solution architects to adapt the AI software portfolio to the unique requirements of their company. For example, if a company’s data scientists already use a set of open source tools, they can continue to do so while benefitting from all optimizations in IBM Power10 and paving the road to integrations with the enterprise portfolio.</li>
</ul>
</li>
<li>
<p><strong>Support for AI ecosystems:</strong> IBM Power10 works seamlessly with AI frameworks like TensorFlow, PyTorch, and ONNX runtime, making it easier for developers to build, train, and deploy gen AI models without needing major infrastructure changes.</p>
<ul>
<li>These AI frameworks leverage some of the popular math libraries like OpenBLAS, libAten, Eigen, and MLAS which provide reusable functions for matrix multiplication.</li>
<li>IBM has already integrated Power10's hardware acceleration capabilities into these math libraries, thus allowing AI workloads by using these frameworks to automatically get the IBM Power10 speed-up without any code changes.</li>
<li>Support for container orchestration platforms like Red Hat OpenShift and Kubernetes allows efficient orchestration of AI workloads.</li>
</ul>
</li>
<li>
<p><strong>Supersede GPUs:</strong> Integrating GPU clusters into computing centers is a daunting and complex task. CUDA drivers need to be installed and managed. Hardware faults regularly cause systems to crash. GPUs are expensive and consume lots of energy. Using multiple GPUs and distributed training on GPUs is complicated and buggy. By consolidating on IBM Power10, these are all problems of the past while maintaining low latencies and a high throughput for AI workloads. </p>
</li>
<li>
<p><strong>Energy efficiency:</strong> Power10 processors are highly energy-efficient, offering more performance per watt compared to previous generations. This reduces operational costs and is especially beneficial for the resource-intensive training and deployment of gen AI models.</p>
</li>
<li>
<p><strong>Security:</strong> IBM Power10 comes with end-to-end encryption and advanced memory protection features, ensuring that sensitive AI workloads are secure from potential breaches. This is important in industries like healthcare and finance, where gen AI applications need strong data protection. IBM's support for quantum-safe encryption helps future-proof AI workloads against quantum computing threats.</p>
</li>
<li>
<p><strong>Flexible cloud and hybrid deployments:</strong> Power10 integrates smoothly with hybrid cloud environments, allowing enterprises to run gen AI workloads both on-premises and in the cloud. This flexibility is key for scaling AI applications while maintaining control over data and infrastructure.</p>
</li>
</ol>
<h2 id="index-summary"><strong>Summary</strong><a class="headerlink" href="#index-summary" title="Permanent link">¶</a></h2>
<p>IBM Power10 is designed to meet the specific needs of gen AI use cases by offering high-performance processing, memory scalability, security, and energy efficiency. It empowers enterprises to handle complex AI workloads efficiently while maintaining flexibility and security, making it ideal for industries that rely on AI-driven innovation.</p></div></section><section class="print-page" id="pre-req"><div><h1 id="prerequisites-for-the-course">Prerequisites for the course<a class="headerlink" href="#pre-req-prerequisites-for-the-course" title="Permanent link">¶</a></h1>
<p>The following are the prerequisites for this course:</p>
<ul>
<li>
<p>AI on Power - Level 2 course - <a href="https://yourlearning.ibm.com/activity/PLAN-95E47B97CBB5" target="_blank">IBM</a> | <a href="https://learn.ibm.com/course/view.php?id=16329" target="_blank">BP</a></p>
</li>
<li>
<p>Basic understanding of Generative AI (gen AI) and large language models (LLM).</p>
</li>
<li>
<p>Basic understanding of Red Hat OpenShift and working with Yet Another Markup Language (YAML) files. </p>
<ul>
<li>While sufficient instructions are provided through-out the course, it is highly recommended to have a basic understanding of Red Hat OpenShift and working with YAML files.</li>
</ul>
</li>
<li>
<p>Laptop with Secure Shell (ssh) preinstalled to connect to lab environment.</p>
<ul>
<li>Windows: PowerShell has ssh preinstalled. PuTTY tool can also be used.</li>
<li>Mac/Linux: ssh is already available.</li>
</ul>
</li>
</ul></div></section><section class="print-page" id="lab-setup"><div><h1 id="lab-setup">Lab setup<a class="headerlink" href="#lab-setup-lab-setup" title="Permanent link">¶</a></h1>
<h2 id="lab-setup-provisioning-the-environment">Provisioning the environment<a class="headerlink" href="#lab-setup-provisioning-the-environment" title="Permanent link">¶</a></h2>
<p>This hands-on lab uses Red Hat OpenShift on Power10 on-premises environment which is optimized for AI workloads and hosted on IBM TechZone.</p>
<p>Follow these steps:</p>
<ol>
<li>
<p>Open <a href="https://techzone.ibm.com/collection/generative-ai-demos-on-ibm-power/environments" target="_blank">this</a> TechZone collection and provision the environment named "OpenShift ready for AI on IBM Power10 (Container PaaS)" by clicking <strong>Reserve</strong> <strong>(A)</strong> and submitting the resulting form (select <strong>Education</strong> as purpose)</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/9c5c4702-509b-4757-b91e-2098ae818454" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9c5c4702-509b-4757-b91e-2098ae818454"></a></p>
</li>
<li>
<p>Watch your email for updates from TechZone and wait for your environment to be provisioned.</p>
<div class="admonition note">
<p class="admonition-title">TechZone provisioning can take time</p>
<p>It can take up to 3 hours for the provisioning to happen. Allow adequate time for its completion and maintain patience during this period. If you get into provisioning issues, refer to the <a href="https://ibm.github.io/AI-on-Power-Level3/support/" target="_blank">Support</a> section for assistance.</p>
</div>
</li>
<li>
<p>After provisioning is complete, go to <a href="https://techzone.ibm.com/my/reservations" target="_blank">my reservations</a> page to verify it's in "Ready" <strong>(A)</strong> state.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/46640f65-545e-4dca-aa7a-6c0b6ca771f8" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/46640f65-545e-4dca-aa7a-6c0b6ca771f8"></a></p>
<div class="admonition tip">
<p class="admonition-title">Extend your reservation</p>
<p>You can extend your reservation up to a maximum of 6 days. Click the three dots that are seen on the upper right of your reservation and select the <strong>Extend</strong> option.</p>
</div>
</li>
</ol>
<h2 id="lab-setup-accessing-the-environment">Accessing the environment<a class="headerlink" href="#lab-setup-accessing-the-environment" title="Permanent link">¶</a></h2>
<ol>
<li>This lab uses an on-premises environment. Verify that you are connected to the IBM Virtual Private Network (VPN) to access the environment. Refer to <a href="https://github.com/IBM/itz-support-public/blob/main/IBM-On-premise/IBM-On-premise-Runbooks/configure-vpn.md" target="_blank">this</a> link for more details.</li>
<li>In TechZone, open the <a href="https://techzone.ibm.com/my/reservations" target="_blank">My Reservations</a> page.</li>
<li>Click your reservation, which opens up the details page.</li>
<li>Scroll to the "Reservation Details" section of the page which has information on how to connect to the Red Hat OpenShift console.
   <a class="glightbox" href="https://github.com/user-attachments/assets/9e7df820-6a8b-4cc6-9ca3-b2a8cdc7decb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9e7df820-6a8b-4cc6-9ca3-b2a8cdc7decb"></a></li>
</ol>
<h3 id="lab-setup-accessing-red-hat-openshift-console">Accessing Red Hat OpenShift console<a class="headerlink" href="#lab-setup-accessing-red-hat-openshift-console" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>In the "Reservation Details" section of the TechZone environment details page, click the <strong>Red Hat OpenShift Console URL</strong> <strong>(A)</strong>.
     <a class="glightbox" href="https://github.com/user-attachments/assets/68c271a0-0077-4cf7-9080-b031b2070cb6" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/68c271a0-0077-4cf7-9080-b031b2070cb6"></a></p>
</li>
<li>
<p>This action opens a new browser tab or window and navigates to the Red Hat OpenShift console login page.</p>
<ul>
<li>If you encounter any security exception, navigate to the bottom of the browser page, accept the exception under Advanced and continue. This action is acceptable since this is a lab or demo environment and it uses self-signed certificates.</li>
</ul>
</li>
<li>
<p>Using the user account (cecuser) and password from your reservation details page, login by using the <strong>htpasswd</strong> <strong>(A)</strong> option.
     <a class="glightbox" href="https://github.com/user-attachments/assets/61c1015c-41ab-4400-9b83-ad8f6e2cba10" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/61c1015c-41ab-4400-9b83-ad8f6e2cba10"></a></p>
<div class="admonition tip">
<p class="admonition-title">How to copy user password?</p>
<p>Click the <strong>copy icon</strong> <strong>(A)</strong> provided under 'User Password' in the reservation details page to copy the password and paste it in the Red Hat OpenShift console window.</p>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/33fe60d7-c285-4d55-a830-73bac9e8b032" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/33fe60d7-c285-4d55-a830-73bac9e8b032"></a>     </p>
</li>
<li>
<p>You are logged in to the Red Hat OpenShift cluster successfully by using the console. The dashboard of the Red Hat OpenShift console is now be visible (or the page you were on, before you logged off). You land in <strong>Administrator</strong> profile (or <strong>Developer</strong> profile if that was the last profile you were in when you logged off).</p>
<div class="admonition tip">
<p class="admonition-title">Tip - Maintain 2 browser windows, one each for Administrator and Developer persona</p>
<p>It is a good idea to have 2 browser windows (or tabs per your preference) for Red Hat OpenShift console access - one with Administrator profile and another with Developer profile because in the hands-on labs you are switching between these profiles and it's easier and efficient to do so with 2 browser windows.</p>
<ul>
<li>To do so, copy the URL from the browser address bar. Open a new browser window (or tab) and paste the URL there. It opens up one more Red Hat OpenShift console in the new window (or tab). In the new window/tab, switch to the Developer profile (also known as Persona) by going to the upper left and clicking <strong>Administrator</strong> and selecting <strong>Developer</strong> (or vice-versa) in the drop down menu. In short, verify that you have 2 browser windows, one each with Administrator and Developer profile (also known as Persona). These two windows are referred to as the Red Hat OpenShift Administrator console and Developer console.</li>
</ul>
</div>
<p><video style="width:100%" muted="true" autoplay="true" loop="true" controls="" alt="type:video">
   <source src="https://github.com/user-attachments/assets/a622a195-00a6-4950-b2e5-686b04fa3401" type="video/mp4">
</source></video></p>
</li>
</ol>
<h3 id="lab-setup-reauthenticating-to-the-red-hat-openshift-console">Reauthenticating to the Red Hat OpenShift console<a class="headerlink" href="#lab-setup-reauthenticating-to-the-red-hat-openshift-console" title="Permanent link">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">REAUTHENTICATING in case you lose console access</p>
<p>In case you lose access to the Red Hat OpenShift cluster and need to reauthenticate to the console, which is possible in case your reservation expires and/or your console authentication timed-out, follow the steps in the preceding section to reauthenticate to your Red Hat OpenShift console.</p>
</div>
<h3 id="lab-setup-accessing-red-hat-openshift-command-line-interface-cli">Accessing Red Hat OpenShift Command Line Interface (CLI)<a class="headerlink" href="#lab-setup-accessing-red-hat-openshift-command-line-interface-cli" title="Permanent link">¶</a></h3>
<p>Red Hat OpenShift CLIs are accessed by using the <code>oc</code> command</p>
<ol>
<li>
<p>Navigate to the "Reservation Details" section of your TechZone environment details page and make a note of the Bastion server's <strong>hostname</strong> <strong>(A)</strong>, <strong>IP address</strong> <strong>(B)</strong> and the <strong>user account</strong> <strong>(C)</strong> that is needed to login by using SSH client.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/ecef8ba6-9790-4c75-94e0-3be409bff4ea" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ecef8ba6-9790-4c75-94e0-3be409bff4ea"></a></p>
</li>
<li>
<p>Open a terminal window and use the SSH client to connect to the Bastion node of the Red Hat OpenShift cluster.</p>
<ul>
<li><code>ssh -l cecuser &lt;your bastion hostname/IP as provided in the Reservation Details section&gt;</code>.</li>
<li>If <code>ssh</code> presents a warning, type <code>yes</code> and continue.</li>
<li>When prompted for password, copy the password from Reservation Details page by clicking the copy icon and pasting it in the ssh terminal window</li>
</ul>
</li>
<li>
<p>You have now logged in to the bastion node of your Red Hat OpenShift cluster.</p>
<ul>
<li>Keep this terminal window open as you are using it frequently to run CLI commands.
        <a class="glightbox" href="https://github.com/user-attachments/assets/576d86f0-8873-492c-8b13-9433c9f25604" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/576d86f0-8873-492c-8b13-9433c9f25604"></a></li>
</ul>
</li>
<li>
<p><code>oc</code> CLI is preinstalled on the bastion node. Verify by running <code>oc version</code> <strong>(A)</strong> command.</p>
<ul>
<li>Ignore the error part of the <code>oc version</code> for now. It is as expected since you have not yet logged in to the cluster from the CLI.<br>
<a class="glightbox" href="https://github.com/user-attachments/assets/b0e21c56-3757-4771-bb47-42d3aaf5591f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b0e21c56-3757-4771-bb47-42d3aaf5591f"></a></li>
</ul>
</li>
</ol>
<h3 id="lab-setup-logging-in-to-red-hat-openshift-cluster-by-using-oc-cli">Logging in to Red Hat OpenShift Cluster by using <code>oc</code> CLI<a class="headerlink" href="#lab-setup-logging-in-to-red-hat-openshift-cluster-by-using-oc-cli" title="Permanent link">¶</a></h3>
<p>Login to the Red Hat OpenShift cluster by using the <code>oc</code> CLI. This login is needed to run some CLI commands as part of the lab steps.</p>
<ol>
<li>
<p>In the Red Hat OpenShift console, upper right section, click <strong>cecuser</strong> <strong>(A)</strong> and select <strong>Copy login command</strong> <strong>(B)</strong> option.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/52c8e57b-d507-4b48-9eb8-d3843fc9d3d4" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/52c8e57b-d507-4b48-9eb8-d3843fc9d3d4"></a></p>
</li>
<li>
<p>A new browser window (or tab, depending on your browser setting) opens up.</p>
<ul>
<li>If you encounter any security exception, navigate to the bottom of the browser page, accept the exception under Advanced and continue. This action is acceptable since this is a lab or demo environment and it uses self-signed certificates.</li>
</ul>
</li>
<li>
<p>You are presented with another login screen. Click <strong>htpasswd</strong> <strong>(A)</strong> option
   <a class="glightbox" href="https://github.com/user-attachments/assets/1fb91841-e243-4d56-a4f1-1068fb3058df" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/1fb91841-e243-4d56-a4f1-1068fb3058df"></a></p>
</li>
<li>
<p>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code>.</p>
<ul>
<li>TIP: Click the copy icon that is provided under 'User Password' in the Reservation Details page to copy the password and paste it in the Red Hat OpenShift console window.</li>
</ul>
</li>
<li>
<p>Click <strong>Display token</strong> <strong>(A)</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/45865182-61f6-4bfd-86ec-4b5f43e99706" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/45865182-61f6-4bfd-86ec-4b5f43e99706"></a></p>
</li>
<li>
<p>Copy the <code>oc login --token=...</code> CLI and paste it into the bastion node’s terminal window.
     <a class="glightbox" href="https://github.com/user-attachments/assets/75ad62a0-d0a0-45f6-8797-fedad6e5877a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/75ad62a0-d0a0-45f6-8797-fedad6e5877a"></a>
     <a class="glightbox" href="https://github.com/user-attachments/assets/a2753a4c-86d6-49ca-96c8-54f3ed7dbac5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a2753a4c-86d6-49ca-96c8-54f3ed7dbac5"></a></p>
</li>
<li>You are logged in to the Red Hat OpenShift cluster successfully by using the <code>oc</code> CLI.</li>
<li>
<p>In case you lose access to the <code>oc</code> CLI, you get an error as below, in which case you need to reauthenticate.<br>
   Refer to the following section on how to reauthenticate in CLI.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/a1e8d00c-64d0-41ab-997c-540378df0544" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a1e8d00c-64d0-41ab-997c-540378df0544"></a></p>
</li>
</ol>
<h3 id="lab-setup-reauthenticating-for-cli-access">Reauthenticating for CLI access<a class="headerlink" href="#lab-setup-reauthenticating-for-cli-access" title="Permanent link">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">REAUTHENTICATING in case you lose CLI access</p>
<p>In case you lose access to the Red Hat OpenShift cluster and need to reauthenticate by using the CLI, which is possible in case your reservation expires and/or your CLI window gets terminated for some reason, follow the steps in the preceding section to get back your <code>oc</code> CLI authenticated to the Red Hat OpenShift cluster.</p>
</div>
<h3 id="lab-setup-summary">Summary<a class="headerlink" href="#lab-setup-summary" title="Permanent link">¶</a></h3>
<p>Efforts are made to keep the lab instructions simple and easy to follow to cater to an audience of all skill levels.
Both the Red Hat OpenShift CLI and Red Hat OpenShift web console are used in the lab guide as not all Red Hat OpenShift capabilities are available in the Red Hat OpenShift web console.</p></div></section>
                        <h1 class='nav-section-title' id='section-lab-1-deploy-a-llm-on-power10'>
                            Lab 1 - Deploy a LLM on Power10 <a class='headerlink' href='#section-lab-1-deploy-a-llm-on-power10' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="lab1-lab1-overview"><div><h1 id="deploy-a-large-language-model-llm-on-ibm-power10-lab-education">Deploy a large language model (LLM) on IBM Power10 - Lab Education<a class="headerlink" href="#lab1-lab1-overview-deploy-a-large-language-model-llm-on-ibm-power10-lab-education" title="Permanent link">¶</a></h1>
<p>The goal of this lab is to showcase the basics (and the ease) of deploying an LLM (also known as foundation models) on Power and how easy it is to switch to a different LLM.
Before proceeding, it is important to understand the ecosystem around LLMs (also known as foundation models or generative AI) in the context of IBM Power.</p>
<p>This lab involves deploying two LLMs from <a href="https://huggingface.co/" target="_blank">Hugging Face (HF)</a> on IBM Power10 within the provisioned on-premises lab environment.</p>
<h2 id="lab1-lab1-overview-what-is-hugging-face">What is Hugging Face?<a class="headerlink" href="#lab1-lab1-overview-what-is-hugging-face" title="Permanent link">¶</a></h2>
<p>Hugging Face is an AI company that built an incredibly popular AI community around open source libraries, models, and datasets.
It's an open source community with a large collection of AI models and datasets that are available for sharing and collaboration.
The Hugging Face hub now hosts more than 950 K models and over 210 K datasets, and those numbers are growing quickly.</p>
<div class="admonition note">
<p class="admonition-title">What is Hugging Face?</p>
<p>Hugging Face is an AI company that serves as a major hub for Natural Language Processing (NLP) and machine learning (ML) tools. It is widely known for its open source library, Transformers, and its collaborative platform, which provides a vast collection of pre-trained models and datasets. Hugging Face simplifies access, fine-tuning, and deployment of state-of-the-art machine learning models for developers, researchers, and businesses, particularly for NLP tasks such as text classification, translation, summarization, and more.</p>
</div>
<p>HF is used to access LLMs in this lab due to its extensive collection of publicly hosted pre-trained models that are available for free.</p>
<h2 id="lab1-lab1-overview-what-is-ibm-watsonx">What is IBM watsonx?<a class="headerlink" href="#lab1-lab1-overview-what-is-ibm-watsonx" title="Permanent link">¶</a></h2>
<p>Enterprise clients of IBM use <a href="https://www.ibm.com/watsonx" target="_blank">IBM watsonx platform</a>, an enterprise-ready AI, and data platform.
IBM watsonx is designed for the enterprise and is targeted for business domains, empowering value creators to transform business applications into AI-first applications.</p>
<div class="admonition note">
<p class="admonition-title">What is IBM watsonx?</p>
<p>IBM watsonx is IBM’s next-generation AI and data platform, which is designed to help enterprises build, train, fine-tune, and deploy large-scale AI models efficiently. IBM watsonx enables businesses to harness the power of artificial intelligence and machine learning (AI/ML) for various tasks like generative AI, predictive analytics, and decision-making. It integrates advanced capabilities for handling large language models (LLMs), machine learning workflows, and AI governance.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">IBM watsonx support on IBM Power</p>
<p><strong>NOTE</strong>: At the time of writing, IBM watsonx is not yet available to run on IBM Power Systems. However, the LLMs that support its functions are fully compatible with IBM Power. Clients can leverage foundation models, including IBM-developed and open source options, to integrate and use (gen)AI capabilities within their on-premises applications running on IBM Power. This course's labs demonstrate the use of such LLMs.</p>
</div>
<h2 id="lab1-lab1-overview-ibm-watsonx-vs-hugging-face">IBM watsonx Vs Hugging Face<a class="headerlink" href="#lab1-lab1-overview-ibm-watsonx-vs-hugging-face" title="Permanent link">¶</a></h2>
<p>IBM watsonx and Hugging Face both offer AI tools, but they serve different purposes:</p>
<ul>
<li>
<p><strong>IBM watsonx</strong>: An enterprise-grade AI platform designed for large-scale AI model training, fine-tuning, and deployment. It focuses on governance, compliance, and custom AI models for business use, offering robust tools for data management and AI governance. Ideal for enterprises needing secure, scalable AI with industry-specific solutions.</p>
</li>
<li>
<p><strong>Hugging Face</strong>: A community-driven platform offering pre-trained models (especially in NLP) and datasets for rapid experimentation and development. It is known for its open source library and easy access to state-of-the-art models, making it great for research, startups, and developers.</p>
</li>
</ul>
<p>In essence, IBM watsonx focuses on enterprise-grade AI with strong governance and scalability, while Hugging Face is designed for flexible, community-driven AI development with a focus on rapid prototyping and open access to pre-trained models.</p>
<h2 id="lab1-lab1-overview-ibm-watsonx-and-hugging-face">IBM watsonx and Hugging Face<a class="headerlink" href="#lab1-lab1-overview-ibm-watsonx-and-hugging-face" title="Permanent link">¶</a></h2>
<p>With IBM watsonx, clients can run not only IBM-trained foundation models, but also open source models and models from Hugging Face.</p>
<p>IBM watsonx and Hugging Face can work together by combining IBM's enterprise AI capabilities with Hugging Face's vast collection of pre-trained models and tools for rapid AI development:</p>
<ul>
<li><strong>Model Access</strong>: IBM watsonx users can leverage Hugging Face's pre-trained models from its model hub to fine-tune or deploy in enterprise environments by using watsonx’s scalable infrastructure.</li>
<li><strong>Fine-Tuning and Customization</strong>: Businesses can use Hugging Face models in watsonx to fine-tune them with proprietary data while benefiting from watsonx’s AI governance and compliance tools.</li>
<li><strong>Deployment</strong>: Hugging Face models can be integrated into IBM watsonx to deploy at scale on hybrid cloud or on-premises environments, ensuring enterprise-level security and performance.</li>
</ul>
<p>Together, Hugging Face provides the models, and watsonx offers the enterprise-ready infrastructure for secure, large-scale, compliant deployment.
Read <a href="https://developer.ibm.com/blogs/awb-hugging-face-and-ibm-working-together-in-open-source/" target="_blank">this blog</a> to understand more about how IBM and Hugging Face are working to bring open source communities together for enterprise AI.</p>
<h2 id="lab1-lab1-overview-ai-and-watsonx-with-ibm-power">AI and watsonx with IBM Power<a class="headerlink" href="#lab1-lab1-overview-ai-and-watsonx-with-ibm-power" title="Permanent link">¶</a></h2>
<p>This 1-slide overview highlights the capabilities of AI and watsonx on IBM Power available today.
<a class="glightbox" href="https://github.com/user-attachments/assets/f3e6a66d-e418-4e3c-8315-08e125ad8149" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/f3e6a66d-e418-4e3c-8315-08e125ad8149"></a></p>
<p>Clients can get started with AI and watsonx with IBM Power today. 
IBM simplifies the process by aligning common use cases with four key areas that are observed in pilots and client activations.</p>
<ul>
<li>
<p><strong>Pattern 1</strong>: Securely tune, deploy, and manage foundation models. When it comes to task-specific use cases, it is a good idea to work with large open source models in your own workspace that are under your control. Hugging Face is a large repository with over 950 K pre-trained ML models and a platform where the AI ecosystem collaborates on models, datasets, and applications. Download any model from Hugging Face and securely deploy at scale on IBM Power. Then, use high-quality software to help you tune, deploy, and manage as many models as you need. Some examples of what enterprises can use this capability for: Customer service, knowledge workers to augment staff and fraud reporting.</p>
</li>
<li>
<p><strong>Pattern 2</strong>: There are many new and existing business apps that are using foundation models that are integrated into the workflows. Deploy your foundation models anywhere, on Power, x86, cloud, and use the watsonx.ai software development kit (SDK) available in Python and embed directly into applications. On Power, enterprises can do this quickly so that services can be released to clients faster on a resilient 24/7 environment. Some examples of clients can embed AI into apps are generating the first draft of reports, citizen services for government end-clients and knowledge management.</p>
</li>
<li>
<p><strong>Pattern 3</strong>: The ecosystem is important to enterprises that have long-standing investments in software that drives their core business workflows. Consume watsonx services from clients customized ecosystem apps. Enterprises can generate code for Ansible playbooks for IBM i or AIX to enhance the Ansible IT management experience. Also, SAP applications can be embedded with watsonx services within the SAP ABAP (Advanced Business Application Programming) environments. These custom apps enable faster service delivery, leverage existing investments, and offer an appealing solution for many. Some example use cases include asset management, code generation, accounting automation. </p>
</li>
<li>
<p><strong>Pattern 4</strong>: A comprehensive suite of AI capabilities is being introduced to the Power platform, enabling clients to train, tune, and deploy models without the need for purchasing GPUs. The lead time for GPUs is somewhere around a year and clients can miss an opportunity if they can’t get started today. Some of the popular use cases that enterprises use are fraud detection, risk underwriting, and demand forecasting.</p>
</li>
</ul>
<h2 id="lab1-lab1-overview-choosing-a-foundation-model">Choosing a foundation model<a class="headerlink" href="#lab1-lab1-overview-choosing-a-foundation-model" title="Permanent link">¶</a></h2>
<p>There are many factors to consider when you choose a foundation model to use for inferencing from a generative AI project.
Determine which factors are most important for you and your organization.</p>
<ul>
<li>Tasks the model can do</li>
<li>Languages supported</li>
<li>Tuning options for customizing the model</li>
<li>License and IP indemnity terms</li>
<li>Model attributes, such as size, architecture, and context window length</li>
</ul>
<p>After you have a short list of models that best fit your needs, you can test the models to see which ones consistently return the results you want.</p>
<p>For more details on choosing a foundation model that supports your use case / language / other factors, refer to <a href="https://www.ibm.com/docs/en/watsonx/saas?topic=models-choosing-model" target="_blank">this document</a>. Although this document is part of the watsonx.ai product documentation, the information that is provided includes both IBM-trained models and open source models as watsonx.ai support both types of models.</p></div></section><section class="print-page" id="lab1-lab1-hands-on-guide"><div><h1 id="deploy-an-llm-on-power10-hands-on-lab-guide">Deploy an LLM on Power10 - Hands-on lab guide<a class="headerlink" href="#lab1-lab1-hands-on-guide-deploy-an-llm-on-power10-hands-on-lab-guide" title="Permanent link">¶</a></h1>
<h2 id="lab1-lab1-hands-on-guide-lab-ready-check">Lab-ready check<a class="headerlink" href="#lab1-lab1-hands-on-guide-lab-ready-check" title="Permanent link">¶</a></h2>
<p>Make sure that you have the following items ready:</p>
<ul>
<li>In your browser, you are logged in as <code>cecuser</code> within the Red Hat OpenShift console.</li>
<li>In your terminal, you are logged in to the bastion server and authenticated to the Red Hat OpenShift cluster by using the Red Hat OpenShift CLI <code>oc</code>.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Proceed only once the items mentioned earlier are ready. Refer to "Lab setup instructions" section (see left side menu) to setup your browser and terminal windows.</p>
</div>
<h2 id="lab1-lab1-hands-on-guide-lab-guide">Lab guide<a class="headerlink" href="#lab1-lab1-hands-on-guide-lab-guide" title="Permanent link">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Image zoom functionality</p>
<p>Click the images in the following lab guide to view them in a larger format.</p>
</div>
<h3 id="lab1-lab1-hands-on-guide-create-a-project">Create a project<a class="headerlink" href="#lab1-lab1-hands-on-guide-create-a-project" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Navigate to the Red Hat OpenShift Administrator profile. Click <strong>Home</strong> <strong>(A)</strong> -&gt; <strong>Projects</strong> <strong>(B)</strong> and click <strong>Create Project</strong> <strong>(C)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/87c5fec4-e76d-4ca3-9845-77d25b8c7d46" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/87c5fec4-e76d-4ca3-9845-77d25b8c7d46"></a></p>
</li>
<li>
<p>Enter a project name: <strong>lab1-demo</strong> <strong>(A)</strong> and click <strong>Create</strong> <strong>(B)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/6d1b6e5d-3cf6-4c3b-811a-6bd40a7a715e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/6d1b6e5d-3cf6-4c3b-811a-6bd40a7a715e"></a></p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-setup-storage">Setup storage<a class="headerlink" href="#lab1-lab1-hands-on-guide-setup-storage" title="Permanent link">¶</a></h3>
<p>Setup storage for this lab which is needed for storing the downloaded AI models. This environment comes with NFS (file storage) pre-configured. 
In Red Hat OpenShift, you first request for the storage (also known as PersistentVolumeClaim or PVC) and the actual storage (also known as PersistentVolume or PV) gets allotted based on your request and the storage availability in the storage pool (NFS in this case).</p>
<ol>
<li>Navigate to the Red Hat OpenShift Administrator profile. Click <strong>Storage</strong> <strong>(A)</strong> -&gt; <strong>PersistentVolumeClaims</strong> <strong>(B)</strong> and click <strong>Create PersistentVolumeClaim</strong> <strong>(C)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/23b17459-0bd3-4abd-8cd2-4bfd572efa0f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/23b17459-0bd3-4abd-8cd2-4bfd572efa0f"></a></li>
<li>In the resulting form, enter the PVC name: <strong>model-storage</strong> <strong>(A)</strong> and Size: <strong>20</strong> <strong>(B)</strong> GB. Leave other fields as defaults and click <strong>Create</strong> <strong>(C)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/98571147-31a2-4e37-a063-c2d1c3929a64" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/98571147-31a2-4e37-a063-c2d1c3929a64"></a></li>
<li>It shows PVC status as bound, which means storage was allotted.
   <a class="glightbox" href="https://github.com/user-attachments/assets/ea19ae1f-899d-4ff5-a5e0-f97df1e97ea2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ea19ae1f-899d-4ff5-a5e0-f97df1e97ea2"></a></li>
<li>
<p>Navigate to <strong>Storage</strong> <strong>(A)</strong> -&gt; <strong>PersistentVolumes</strong> <strong>(B)</strong> and view the actual storage (PV) bound to your storage request (PVC = <strong>model-storage</strong> <strong>(C)</strong>).
   <a class="glightbox" href="https://github.com/user-attachments/assets/4209536d-a21a-42d0-b847-cc6ad15cd64d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4209536d-a21a-42d0-b847-cc6ad15cd64d"></a></p>
<p>The storage setup is now complete.</p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-setup-configmap">Setup ConfigMap<a class="headerlink" href="#lab1-lab1-hands-on-guide-setup-configmap" title="Permanent link">¶</a></h3>
<p>In Red Hat OpenShift, a ConfigMap is an object that is used to manage configuration data for applications. It helps to decouple configuration details from application logic, which makes your applications more portable and easier to manage across different environments. Instead of hardcoding configuration values in your application, you store them in a ConfigMap and inject them into your application at run time.</p>
<p>ConfigMap is used to store the model URL and model name, both of which are required during model deployment. Using ConfigMap allows for easy switching to a different model.</p>
<ol>
<li>
<p>Navigate to <strong>Workloads</strong> <strong>(A)</strong> -&gt; <strong>ConfigMaps</strong> <strong>(B)</strong> and select <strong>lab1-demo</strong> <strong>(C)</strong> as the active Project.
   <a class="glightbox" href="https://github.com/user-attachments/assets/4b0f77a2-f5f5-4821-a073-bbc67ded39cf" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4b0f77a2-f5f5-4821-a073-bbc67ded39cf"></a></p>
</li>
<li>
<p>Click <strong>Create ConfigMap</strong> <strong>(A)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/793a95ad-953a-49d9-8cec-8c7e321947cd" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/793a95ad-953a-49d9-8cec-8c7e321947cd"></a></p>
</li>
<li>
<p>In the resulting form, make sure the <strong>Form view</strong> <strong>(A)</strong> option is selected.
   <a class="glightbox" href="https://github.com/user-attachments/assets/f0c6b9e0-f60d-4b92-b3c5-45252351e539" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/f0c6b9e0-f60d-4b92-b3c5-45252351e539"></a></p>
</li>
<li>
<p>Enter a name: <strong>model-params</strong> <strong>(A)</strong>, and fill the Key and Value fields as below:</p>
<ul>
<li>
<p><strong>Key</strong>: <code>MODEL_NAME</code> <strong>(B)</strong></p>
</li>
<li>
<p><strong>Value</strong>: <code>tinyllama-1.1b-chat-v1.0.Q8_0.gguf</code> <strong>(C)</strong></p>
</li>
</ul>
<p>Click <strong>Add key/value</strong> <strong>(D)</strong> which opens up one more Key/Value box.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/be119bf2-f37d-45f8-b9df-c8d4d7045e63" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/be119bf2-f37d-45f8-b9df-c8d4d7045e63"></a></p>
</li>
<li>
<p>In the newly created Key/Value box, enter the values as below:</p>
<ul>
<li>
<p><strong>Key</strong>: <code>MODEL_URL</code> <strong>(A)</strong></p>
</li>
<li>
<p><strong>Value</strong>: <code>https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf</code> <strong>(B)</strong></p>
</li>
</ul>
<p>Click <strong>Create</strong> <strong>(C)</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/5abc809b-0f3b-4356-9429-2d5159a561e7" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5abc809b-0f3b-4356-9429-2d5159a561e7"></a></p>
<p>The ConfigMap setup is now complete.</p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-deploy-first-model">Deploy first model<a class="headerlink" href="#lab1-lab1-hands-on-guide-deploy-first-model" title="Permanent link">¶</a></h3>
<ol start="2">
<li>
<p>Navigate to the Red Hat OpenShift Developer profile console window.</p>
<ul>
<li><strong>Note</strong>: If you followed the TIP given in the "Lab setup instructions" page, you already have a browser window/tab with the Developer profile. In case you didn't, go to the upper left section of your console, click <strong>Administrator</strong> and select <strong>Developer</strong>.</li>
</ul>
</li>
<li>
<p><strong>IMP</strong>: Make sure that you are in the <strong>lab1-demo</strong> project in the Developer profile window. If not, goto <strong>Project</strong> and select <strong>lab1-demo</strong>.</p>
<p><div class="video-container"><video style="position:relative;width:100%;height:22.172vw" controls alt="type:video"><source src="../_attachments/switch-to-lab1-demo-project.mp4" type="video/mp4"></source></video></div></p>
</li>
<li>
<p>Click <strong>+Add</strong> <strong>(A)</strong> and select <strong>Import YAML</strong> <strong>(B)</strong> option.
   <a class="glightbox" href="https://github.com/user-attachments/assets/72e28c16-590c-4acd-9c29-5dc91bc031d7" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/72e28c16-590c-4acd-9c29-5dc91bc031d7"></a></p>
</li>
<li>
<p>In the resulting window, copy and paste the following deployment YAML into it and click <strong>Create</strong> <strong>(A)</strong>.
   </p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-1"> 1</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-2"> 2</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-3"> 3</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-4"> 4</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-5"> 5</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-6"> 6</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-7"> 7</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-8"> 8</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-9"> 9</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-10">10</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-11">11</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-12">12</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-13">13</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-14">14</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-15">15</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-16">16</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-17">17</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-18">18</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-19">19</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-20">20</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-21">21</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-22">22</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-23">23</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-24">24</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-25">25</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-26">26</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-27">27</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-28">28</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-29">29</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-30">30</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-31">31</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-32">32</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-33">33</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-34">34</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-35">35</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-36">36</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-37">37</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-38">38</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-39">39</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-40">40</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-41">41</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-42">42</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-43">43</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-44">44</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-45">45</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-0-46">46</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="w"> </span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<a id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="w"> </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<a id="__codelineno-0-3" name="__codelineno-0-3"></a><span class="w"> </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="w">   </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-demo</span>
<a id="__codelineno-0-5" name="__codelineno-0-5"></a><span class="w"> </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6"></a><span class="w">   </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-0-7" name="__codelineno-0-7"></a><span class="w">   </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-0-8" name="__codelineno-0-8"></a><span class="w">     </span><span class="nt">matchLabels</span><span class="p">:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9"></a><span class="w">       </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-demo</span>
<a id="__codelineno-0-10" name="__codelineno-0-10"></a><span class="w">   </span><span class="nt">template</span><span class="p">:</span>
<a id="__codelineno-0-11" name="__codelineno-0-11"></a><span class="w">     </span><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-0-12" name="__codelineno-0-12"></a><span class="w">       </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-0-13" name="__codelineno-0-13"></a><span class="w">         </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-demo</span>
<a id="__codelineno-0-14" name="__codelineno-0-14"></a><span class="w">     </span><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-0-15" name="__codelineno-0-15"></a><span class="w">       </span><span class="nt">initContainers</span><span class="p">:</span>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a><span class="w">         </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fetch-model-data</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a><span class="w">           </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubi8</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a><span class="w">           </span><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-models</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="w">               </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/models</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="w">           </span><span class="nt">command</span><span class="p">:</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sh</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">'-c'</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">|</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="w">               </span><span class="no">if [ ! -f /models/$MODEL_NAME ] ; then</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="w">                 </span><span class="no">curl -L $MODEL_URL --output /models/$MODEL_NAME</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="w">               </span><span class="no">else</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="w">                 </span><span class="no">echo "model /models/$MODEL_NAME already present"</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="w">               </span><span class="no">fi</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="w">               </span><span class="no">rm -f /models/mymodel</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="w">               </span><span class="no">ln -sf /models/$MODEL_NAME /models/mymodel</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="w">           </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="w">       </span><span class="nt">containers</span><span class="p">:</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="w">         </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-cpp</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="w">           </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">quay.io/daniel_casali/llama.cpp-mma:sep2024</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="w">           </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">"-m"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"/models/mymodel"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"-c"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"4096"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"--host"</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">"0.0.0.0"</span><span class="p p-Indicator">]</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="w">           </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="w">               </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="w">           </span><span class="nt">volumeMounts</span><span class="p">:</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a><span class="w">             </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-models</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a><span class="w">               </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/models</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a><span class="w">       </span><span class="nt">volumes</span><span class="p">:</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a><span class="w">         </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-models</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a><span class="w">           </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a><span class="w">             </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-storage</span>
</code></pre></div></td></tr></table></div>
    <a class="glightbox" href="https://github.com/user-attachments/assets/35195937-4df3-4263-9d4a-755b9bd67fce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/35195937-4df3-4263-9d4a-755b9bd67fce"></a>
</li>
<li>
<p>A quick explanation of the deployment YAML follows.</p>
<details class="info">
<summary>Deployment YAML explanation</summary>
<ul>
<li><strong>Line 2</strong> says its a deployment YAML  </li>
<li><strong>Line 4</strong> shows the name of this deployment resource (<code>lab-demo</code>)         </li>
<li><strong>Lines 7-9</strong>: Defines the label selector to match Pods with the "app: lab1-demo" label         </li>
<li><strong>Lines 11-13</strong>: Specifies the labels that the Pods have.  </li>
<li><code>spec</code> section starting on <strong>Line 14</strong> contains the pod specification. This deployment has 1 pod which contains 2 containers.   <ul>
<li><strong>initContainer</strong> named <code>fetch-model-data</code> (<strong>Lines 15-32</strong>) which fetches the model from HF and saves it in the underlying storage. The shell script embedded under the <code>command</code> section makes sure that the model is downloaded only if it is not already present. <code>volumeMounts</code> section (<strong>Lines 18-20</strong>) has the name of the volume <code>llama-models</code> this container uses for disk storage and the path where the storage is mounted. This initContainer container uses the standard RHEL8 UBI (Universal Base Image) image and exits after the model is downloaded.</li>
<li><strong>Container</strong> named <code>llama-cpp</code> (<strong>Lines 33-42</strong>) which is the main workload container that serves the LLM. This container uses the docker image <code>quay.io/daniel_casali/llama.cpp-mma:sep2024</code> which is a custom-built image for ppc64le architecture with MMA optimized libraries. This image provides a runtime environment based on the open source <a href="https://github.com/ggerganov/llama.cpp" target="_blank">llama.cpp</a> project which enables LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware.<ul>
<li>You may ask... What's the need for a model runtime?                   <ul>
<li>A model runtime serves as the infrastructure needed to deploy, manage, and run these models in production.                      </li>
<li>It helps manage system resources such as CPU, GPU, memory, and network resources that are critical for deploying models.                     </li>
<li>Advanced runtimes may include optimizations for specific hardware, including accelerators like TPUs or MMAs (Matrix Math Accelerators), for faster computation.                      </li>
<li>It allows models to be scaled for real-world applications, particularly in cloud or distributed computing environments.                      </li>
<li>It enables handling multiple requests simultaneously while keeping latency low, which is vital in production systems.                      </li>
</ul>
</li>
<li>By facilitating the efficient execution and management of machine learning models, runtimes are essential for operationalizing AI and machine learning solutions in production.   </li>
</ul>
</li>
<li><strong>NOTE</strong>: Container <code>llama-cpp</code> uses the same storage volume <code>llama-models</code> as container <code>initContainer</code> for underlying storage and hence can access the models that are downloaded by container <code>initContainer</code>. <strong>Lines 43-46</strong> specifies the PVC (Persistent Volume Claim) <code>model-storage</code> used as the source of storage for the volume. Recall that this PVC was created at the beginning of this lab!</li>
</ul>
</li>
</ul>
</details>
</li>
<li>
<p>You are now in the Deployment details window. Click <strong>Pods</strong> <strong>(A)</strong> tab and you see the Pod erring out. This behavior is expected as the YAML references MODEL_URL and MODEL_NAME environment variables which aren’t supplied yet! These environment variables aren’t in the ConfigMap, and will be injected in the next step.
   <a class="glightbox" href="https://github.com/user-attachments/assets/d6032ff5-03ba-4c2f-b733-ffb8a83fdf30" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d6032ff5-03ba-4c2f-b733-ffb8a83fdf30"></a></p>
</li>
<li>
<p>Navigate to <strong>Environment</strong> <strong>(A)</strong> tab, select <strong>fetch-model-data</strong> <strong>(B)</strong> container and select <strong>model-params</strong> <strong>(C)</strong> ConfigMap and click <strong>Save</strong> <strong>(D)</strong>.
    <a class="glightbox" href="https://github.com/user-attachments/assets/ef491f57-4908-427d-bf8b-c7e4a20c5a4e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ef491f57-4908-427d-bf8b-c7e4a20c5a4e"></a></p>
<div class="admonition info">
<p class="admonition-title">About LLama and tinyLLama models</p>
<p>The ConfigMap currently points to the tinyLLaMa model. Meta (formerly Facebook) developed the LLaMA (Large Language Model Meta AI) model, a family of state-of-the-art large language models, which are designed to perform various natural language processing tasks. TinyLLaMA is a compact variant of the LLaMA (Large Language Model Meta AI) model, which is designed for efficiency and accessibility, especially when deployed in smaller environments. Available in Hugging Face (HF), it focuses on reducing the size of large-scale language models while retaining strong performance across various natural language processing tasks.</p>
</div>
</li>
<li>
<p>Switch back to the <strong>Pods</strong> <strong>(A)</strong> tab and observe that a new pod is started by Red Hat OpenShift, as the pod's environment was modified when the ConfigMap was added.
    <a class="glightbox" href="https://github.com/user-attachments/assets/74c8a1f9-fa9d-42b8-a26b-f8ea873d1116" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/74c8a1f9-fa9d-42b8-a26b-f8ea873d1116"></a></p>
</li>
<li>
<p>The new pod downloads the model and then starts it. Since the configmap points to a tinyllama model, it is downloaded from HuggingFace and then started. When that happens the pod's status changes to Running.</p>
<div class="admonition info">
<p class="admonition-title">Model download takes time</p>
<p>This process takes a few minutes (in this case, it took around 1 to 1.5 minutes) and your results might vary. Remember, this is a demo environment and the models are a few GBs in size. Models that are once downloaded are not downloaded again if you are using the same storage (PV).</p>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/06801c61-7ec4-46c6-b5d7-1f9f4af660dc" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/06801c61-7ec4-46c6-b5d7-1f9f4af660dc"></a></p>
<p>** Well done!** The LLM is successfully deployed on Power10.</p>
</li>
<li>
<p>Verify that the model running is indeed tinyllama! Click the pod name <strong>(A)</strong> to enter the pod details view/page.
    <a class="glightbox" href="https://github.com/user-attachments/assets/5a6e4078-9047-4d2f-ab84-942f938b46b7" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5a6e4078-9047-4d2f-ab84-942f938b46b7"></a></p>
</li>
<li>In the pod details page, click the <strong>Logs</strong> <strong>(A)</strong> tab to see the pod logs.
    <a class="glightbox" href="https://github.com/user-attachments/assets/c7bb75d5-6768-45fa-b51f-37fef4132218" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c7bb75d5-6768-45fa-b51f-37fef4132218"></a></li>
<li>
<p>In the log window, scroll upwards to see the name of the model against the attribute <strong>llm_load_print_meta: general.name</strong>.
    <a class="glightbox" href="https://github.com/user-attachments/assets/89536022-644a-497c-9225-9a08d68de52a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/89536022-644a-497c-9225-9a08d68de52a"></a></p>
<p>This confirms that tinyllama is successfully deployed.</p>
</li>
<li>
<p>The next step is to access the model and interact with it. In Red Hat OpenShift, you need to create a service and a route which generates the cluster internal and publicly accessible HTTP endpoints, respectively. To do that, navigate to the Red Hat OpenShift Administrator profile and click <strong>Networking</strong> <strong>(A)</strong> -&gt; <strong>Services</strong> <strong>(B)</strong> and click <strong>Create Service</strong> <strong>(C)</strong>.</p>
<div class="admonition note">
<p>If you are switching browser window/tab, make sure that you are in the <strong>lab1-demo</strong> project in the new window/tab.</p>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/a4f691e2-f597-42c8-951f-6872d02d7713" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a4f691e2-f597-42c8-951f-6872d02d7713"></a></p>
</li>
<li>
<p>In the resulting Create Service YAML window, select all, and delete everything. Then copy the following service YAML, paste it in the YAML window and click <strong>Create</strong> <strong>(A)</strong>.
    </p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-1"> 1</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-2"> 2</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-3"> 3</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-4"> 4</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-5"> 5</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-6"> 6</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-7"> 7</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-8"> 8</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-9"> 9</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-10">10</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-11">11</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-12">12</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-13">13</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-14">14</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-1-15">15</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<a id="__codelineno-1-2" name="__codelineno-1-2"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-1-3" name="__codelineno-1-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-1-4" name="__codelineno-1-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">"lab1-service"</span>
<a id="__codelineno-1-5" name="__codelineno-1-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-1-6" name="__codelineno-1-6"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="s">"lab1-service"</span>
<a id="__codelineno-1-7" name="__codelineno-1-7"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-1-8" name="__codelineno-1-8"></a><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="s">"ClusterIP"</span>
<a id="__codelineno-1-9" name="__codelineno-1-9"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
<a id="__codelineno-1-10" name="__codelineno-1-10"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-port</span>
<a id="__codelineno-1-11" name="__codelineno-1-11"></a><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<a id="__codelineno-1-12" name="__codelineno-1-12"></a><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<a id="__codelineno-1-13" name="__codelineno-1-13"></a><span class="w">      </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8080</span>
<a id="__codelineno-1-14" name="__codelineno-1-14"></a><span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<a id="__codelineno-1-15" name="__codelineno-1-15"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="s">"lab1-demo"</span>
</code></pre></div></td></tr></table></div>
    <a class="glightbox" href="https://github.com/user-attachments/assets/c53a1eaf-57c7-4fcd-8148-b0ab9fba77e4" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c53a1eaf-57c7-4fcd-8148-b0ab9fba77e4"></a>
</li>
<li>
<p>A brief explanation of the Service YAML is provided below.</p>
<details class="info">
<summary>Service YAML explanation</summary>
<ul>
<li>
<p><strong>Line 2</strong> shows that it is a service YAML</p>
</li>
<li>
<p><strong>Line 4</strong> shows the name of the service (<code>lab1-service</code>)</p>
</li>
<li>
<p><strong>Line 8</strong> shows that the type of this service is <code>ClusterIP</code>. ClusterIP is the default service type that is used to expose a set of Pods internally within the cluster. It creates a virtual IP (ClusterIP) for the service, which other services or Pods within the same Kubernetes cluster can use to access it.</p>
</li>
<li>
<p><strong>Lines 9-13</strong> has the port details. <code>port: 8080</code> refers to the port for the incoming traffic of the service. <code>targetPort: 8080</code> refers to the port on the pod. This means that incoming traffic for the service on port 8080 is forwarded to the pod on port 8080.</p>
</li>
<li>
<p><strong>Lines 14-15</strong> specifies the pod selector label. It defines which Pods the service routes traffic to. In this case, it matches Pods that have the label <code>app: lab1-demo</code>, which is the label that was assigned in the deployment YAML.</p>
</li>
</ul>
</details>
</li>
<li>
<p>You are in service details view/page. You can see the ClusterIP which is accessible from inside the cluster only.
    <a class="glightbox" href="https://github.com/user-attachments/assets/5b8b3385-44a7-4dec-bbfa-f384c5f784fb" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5b8b3385-44a7-4dec-bbfa-f384c5f784fb"></a></p>
</li>
<li>
<p>Navigate to <strong>Networking</strong> <strong>(A)</strong> -&gt; <strong>Routes</strong> <strong>(B)</strong> and click <strong>Create Route</strong> <strong>(C)</strong>.
      <a class="glightbox" href="https://github.com/user-attachments/assets/4bf9bb4f-b017-43fe-ae12-99db103d001f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4bf9bb4f-b017-43fe-ae12-99db103d001f"></a></p>
</li>
<li>
<p>In the resulting Create Route window, select <strong>YAML view</strong> <strong>(A)</strong> and clear everything from the YAML window. Copy the below YAML and paste it in the YAML window and click <strong>Create</strong> <strong>(B)</strong>.
    </p><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-1"> 1</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-2"> 2</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-3"> 3</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-4"> 4</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-5"> 5</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-6"> 6</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-7"> 7</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-8"> 8</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-9"> 9</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-10">10</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-11">11</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-12">12</a></span>
<span class="normal"><a href="#lab1-lab1-hands-on-guide-__codelineno-2-13">13</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Route</span>
<a id="__codelineno-2-2" name="__codelineno-2-2"></a><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">route.openshift.io/v1</span>
<a id="__codelineno-2-3" name="__codelineno-2-3"></a><span class="nt">metadata</span><span class="p">:</span>
<a id="__codelineno-2-4" name="__codelineno-2-4"></a><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-route</span>
<a id="__codelineno-2-5" name="__codelineno-2-5"></a><span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<a id="__codelineno-2-6" name="__codelineno-2-6"></a><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-route</span>
<a id="__codelineno-2-7" name="__codelineno-2-7"></a><span class="nt">spec</span><span class="p">:</span>
<a id="__codelineno-2-8" name="__codelineno-2-8"></a><span class="w">  </span><span class="nt">to</span><span class="p">:</span>
<a id="__codelineno-2-9" name="__codelineno-2-9"></a><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span>
<a id="__codelineno-2-10" name="__codelineno-2-10"></a><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-service</span>
<a id="__codelineno-2-11" name="__codelineno-2-11"></a><span class="w">  </span><span class="nt">tls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<a id="__codelineno-2-12" name="__codelineno-2-12"></a><span class="w">  </span><span class="nt">port</span><span class="p">:</span>
<a id="__codelineno-2-13" name="__codelineno-2-13"></a><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lab1-port</span>
</code></pre></div></td></tr></table></div>
    <a class="glightbox" href="https://github.com/user-attachments/assets/9f313dfd-c1a3-4ec7-8ebe-b1e2595b8ae6" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9f313dfd-c1a3-4ec7-8ebe-b1e2595b8ae6"></a>
</li>
<li>
<p>A brief explanation of the Route YAML is being provided here. In Red Hat OpenShift, a Route exposes a service to external clients by mapping an external URL to an internal Red Hat OpenShift service.</p>
<details class="info">
<summary>Route YAML explanation</summary>
<ul>
<li>
<p><strong>Line 1</strong>: This line defines the creation of a Red Hat OpenShift Route resource.</p>
</li>
<li>
<p><strong>Lines 3-6</strong>: The <code>name</code> field defines the name of the route (<code>lab1-route</code>), and the <code>labels</code> field associates labels with the route, which can be used for tracking or organizing the route in Red Hat OpenShift.</p>
</li>
<li>
<p><strong>Lines 7-10</strong>: The <code>to</code> field under <code>spec</code> defines the target resource for the route. In this case, it targets a Service that is named <code>lab1-service</code>, which is already defined and exposes Pods internally by using ClusterIP resource.</p>
</li>
<li>
<p><strong>Line 11</strong>: TLS is set to <code>null</code>, which means that SSL/TLS encryption that is not configured for this route. For secure routes (HTTPS), you'd configure this to specify certificates and encryption protocols.</p>
</li>
<li>
<p><strong>Lines 12-13</strong>: The <code>targetPort</code> field under <code>port</code> specifies which port on the service (or Pods) the route will forward requests to. In this case, it refers to <code>lab1-port</code>, which was defined as port 8080 in our service configuration.</p>
</li>
</ul>
</details>
</li>
<li>
<p>You are in the route details view. The URL mentioned under <strong>Location</strong> is the externally accessible URL of your application (which hosts the tinyllama LLM).
    <a class="glightbox" href="https://github.com/user-attachments/assets/abfcc310-985e-4208-9e56-d2f39e4f2c13" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/abfcc310-985e-4208-9e56-d2f39e4f2c13"></a></p>
</li>
<li>
<p>Click the external URL in the route details view to access your model. A new browser window/tab where you are able to interact with your newly deployed LLM. You see a screen like this:      </p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53"></a></p>
</li>
<li>
<p>Scroll all the way down to the input field "Say something..." where you can interact with the LLM. You can ask any question that you like, but keep in mind you're using a small model and many more powerful models out there are available for general conversation.
    <a class="glightbox" href="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce"></a></p>
<div class="admonition note">
<p class="admonition-title">Experimenting with model parameters</p>
<p>You can see numerous model parameters or tunables (for example: Predictions, Temperature, and so on). Visit the model-specific documentation and learn about them and experiment with it. You can try changing some parameters, ask the same question and check how the response changes. These parameters are not covered in this lab as it is outside the scope of the lab.</p>
</div>
</li>
<li>
<p>Presented below are the questions and their corresponding responses.</p>
<div class="admonition warning">
<p class="admonition-title">Accuracy of LLM responses</p>
<ul>
<li>Large language models (LLMs) are trained on vast amounts of text data, but that training is limited to a <strong>specific cutoff date</strong>. It means that the model can only answer questions based on the information available up to that point in time. It cannot access real-time data or understand events, trends, or new information that occurred after the cutoff date. Therefore, the knowledge on which they were trained limits their ability to provide accurate answers.</li>
<li>AI-generated content varies and does not always provide consistent answers. Your response might be different.</li>
</ul>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/c0c4b3ca-dbc6-4f9f-8d29-115c11486843" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c0c4b3ca-dbc6-4f9f-8d29-115c11486843"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/c9fde420-3c8c-40bc-a48c-0ece97fc2248" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c9fde420-3c8c-40bc-a48c-0ece97fc2248"></a></p>
<p><strong>Well done!</strong>, you successfully deployed an LLM model and provisioned a public-facing API endpoint to access it.
In the next step, you will learn how to deploy a different LLM.</p>
</li>
</ol>
<h3 id="lab1-lab1-hands-on-guide-deploy-second-model">Deploy second model<a class="headerlink" href="#lab1-lab1-hands-on-guide-deploy-second-model" title="Permanent link">¶</a></h3>
<p>In this section, IBM's Granite model is deployed.</p>
<div class="admonition info">
<p class="admonition-title">About IBM's Granite LLM</p>
<p>The IBM Granite model family is designed as enterprise-grade AI models tailored for business applications. Granite models are available both as open source models on platforms like Hugging Face and through IBM's watsonx.ai for more enterprise-specific needs.</p>
</div>
<ol>
<li>Navigate to your Red Hat OpenShift developer profile window. Click <strong>ConfigMaps</strong> <strong>(A)</strong>, then click <strong>model-params</strong> <strong>(B)</strong>. This opens up the ConfigMap details page.
   <a class="glightbox" href="https://github.com/user-attachments/assets/3f611c0a-e4b0-47fc-90e5-3fbedce627f3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/3f611c0a-e4b0-47fc-90e5-3fbedce627f3"></a></li>
<li>In the model-params ConfigMap details page, click <strong>Action</strong> <strong>(A)</strong> and select <strong>Edit ConfigMap</strong> <strong>(B)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/2e7d312c-fb9c-44a4-ae39-9c958b72403e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2e7d312c-fb9c-44a4-ae39-9c958b72403e"></a></li>
<li>
<p>In the resulting form, edit the key/value fields for MODEL_NAME and MODEL_URL as below and click <strong>Save</strong> <strong>(E)</strong>.</p>
<ul>
<li><strong>Key</strong>: <code>MODEL_NAME</code> <strong>(A)</strong></li>
<li><strong>Value</strong>: <code>granite-7b-lab.Q4_K_M.gguf</code> <strong>(B)</strong></li>
</ul>
<hr>
<ul>
<li><strong>Key</strong>: <code>MODEL_URL</code> <strong>(C)</strong></li>
<li><strong>Value</strong>: <code>https://huggingface.co/RichardErkhov/instructlab_-_granite-7b-lab-gguf/resolve/main/granite-7b-lab.Q4_K_M.gguf</code> <strong>(D)</strong></li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/9173fc4e-7dde-4afd-917b-aec289c42b5b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9173fc4e-7dde-4afd-917b-aec289c42b5b"></a></p>
</li>
<li>
<p>The existing pod cannot see the changes right away as changing values of a ConfigMap does not cause a deployment (and hence the pod) to restart. It needs to be done manually.</p>
</li>
<li>
<p>Click <strong>Topology</strong> <strong>(A)</strong>, then click "<strong>D lab1-demo</strong>" <strong>(B)</strong> part of the application icon, which opens the deployment details pane (on the right side of the browser window). Click <strong>D lab1-demo</strong> <strong>(C)</strong> in that pane which opens the deployment details view for lab1-demo deployment.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/26ab79ec-d0f2-4d58-a1d3-e744326b4b91" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/26ab79ec-d0f2-4d58-a1d3-e744326b4b91"></a></p>
</li>
<li>
<p>Click <strong>Actions</strong> <strong>(A)</strong> and select <strong>Restart rollout</strong> <strong>(B)</strong>. This restarts the deployment which results in the redeployment of the pod.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/b12bf560-6689-4002-8c34-2bb936ef5931" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b12bf560-6689-4002-8c34-2bb936ef5931"></a></p>
</li>
<li>
<p>Click <strong>Pods</strong> <strong>(A)</strong> tab, where you see a new pod instantiated. The new pod downloads the model and then starts it. Since the configmap points to a Granite model, it is downloaded from HuggingFace and then started. When that happens the new pod's status changes to Running and the existing pod is terminated.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/5d1d9b22-dfcd-4c6a-91cf-fbfc4a8a3384" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5d1d9b22-dfcd-4c6a-91cf-fbfc4a8a3384"></a></p>
<div class="admonition info">
<p class="admonition-title">Model download takes time</p>
<p>This process takes a few minutes (in this case that it took around 3-4 mins as it is a fairly large model compared to tinyllama) and your results might vary. Remember, this is a demo environment and the models are a few GBs in size. Models that are downloaded are not downloaded again if you are using the same storage (PV).</p>
</div>
</li>
<li>
<p>If the previous step is completed successfully, only one pod is running.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/854703df-2ca4-42ce-825c-30c49a94a050" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/854703df-2ca4-42ce-825c-30c49a94a050"></a></p>
</li>
<li>
<p>Verify that the model that is running is an IBM Granite model. Click the pod name <strong>(A)</strong> to enter the pod details view/page.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/4c7c0139-3920-413c-bb20-c8ddeeab6fa3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4c7c0139-3920-413c-bb20-c8ddeeab6fa3"></a></p>
</li>
<li>
<p>In the pod details page, click the <strong>Logs</strong> <strong>(A)</strong> tab to see the pod logs.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/4bec6d4d-8dd3-4b5f-823b-99342ca633a3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4bec6d4d-8dd3-4b5f-823b-99342ca633a3"></a></p>
</li>
<li>
<p>In the log window, scroll upwards to see the name of the model against the attribute <strong>llm_load_print_meta: general.name</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/5ca4f0b1-8655-41df-85d2-8d740627989c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5ca4f0b1-8655-41df-85d2-8d740627989c"></a></p>
<p>This verifies that the IBM Granite (granite-7b-lab) model is successfully deployed.   </p>
</li>
<li>
<p>Access the model by locating the external public endpoint, as demonstrated in the previous section of this lab. The beauty of Red Hat OpenShift is that the endpoint remains the same despite the pod being restarted. So, either you can refresh the earlier page (if you have it opened in the browser) or follow the steps below to access the public URL of your application by using the Topology view.</p>
<div class="admonition note">
<p class="admonition-title">Multiple ways to access the public URL of your application</p>
<p>If you are in the Administrator profile, you can navigate to <strong>Networking</strong> -&gt; <strong>Routes</strong> to access your route resource and click the URL to open your application. Alternatively, if you are in the Developer profile, you can go to <strong>Topology</strong> view and access the URL of your application as well, which is the method that is used here:</p>
</div>
<ul>
<li>In the Developer profile window, click <strong>Topology</strong> <strong>(A)</strong> and you see the icon representing your deployed application.
  <a class="glightbox" href="https://github.com/user-attachments/assets/4dd6037e-1bfd-49b6-b190-f0096aae28b4" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4dd6037e-1bfd-49b6-b190-f0096aae28b4"></a></li>
<li>Click the arrow (at the upper right of the icon) <strong>(A)</strong> that says "Open URL".
  <a class="glightbox" href="https://github.com/user-attachments/assets/5b7ecd28-8491-4b37-b64d-629563c7ae82" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5b7ecd28-8491-4b37-b64d-629563c7ae82"></a></li>
<li>A new browser window/tab where you are able to interact with your newly deployed LLM. You see a screen like this:
  <a class="glightbox" href="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53"></a></li>
<li>Scroll all the way down to the input field "Say something..." where you can interact with the LLM. You can ask any question that you like!
  <a class="glightbox" href="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce"></a></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Experimenting with model parameters</p>
<p>You can see numerous model parameters or tunables (for example: Predictions, Temperature, and so on). Visit the model-specific documentation and learn about them and experiment with it. You might want to change some parameters, ask the same question, and check how the response changes. These parameters are not covered in this lab as it is outside the scope of the lab.</p>
</div>
</li>
<li>
<p>Presented below are the questions and their corresponding responses.</p>
<div class="admonition warning">
<p class="admonition-title">Accuracy of LLM responses</p>
<ul>
<li>Large language models (LLMs) are trained on vast amounts of text data, but that training is limited to a <strong>specific cutoff date</strong>. It means that the model can answer questions based on the information available up to that point in time. It cannot access real-time data or understand events, trends, or new information that occurred after the cutoff date. Therefore, the knowledge on which they were trained limits their ability to provide accurate answers.</li>
<li>AI-generated content varies and does not always provide consistent answers. Your response might be different.</li>
</ul>
</div>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/4201cafb-9f11-4f82-945b-1f843713426e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4201cafb-9f11-4f82-945b-1f843713426e"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/e0a23318-e2bc-4778-8c12-84643a8f530c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e0a23318-e2bc-4778-8c12-84643a8f530c"></a></p>
<p><strong>Well done!</strong> In this lab, you learned how straightforward it is to transition to a different LLM, access it, and use its capabilities.</p>
<p>This concludes the lab.</p>
</li>
</ol></div></section><h1 class='nav-section-title-end'>Ended: Lab 1 - Deploy a LLM on Power10</h1>
                        <h1 class='nav-section-title' id='section-lab-2-deploy-rag-on-power10'>
                            Lab 2 - Deploy RAG on Power10 <a class='headerlink' href='#section-lab-2-deploy-rag-on-power10' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="lab2-lab2-overview"><div><h1 id="deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-education">Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - Lab education<a class="headerlink" href="#lab2-lab2-overview-deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-education" title="Permanent link">¶</a></h1>
<p>Goal of this lab is to get hands-on experience in deploying a RAG pattern on IBM Power10.
Before we do that, lets understand what RAG is, its advantages, key use cases, and how it fits in the IBM Power landscape.</p>
<h2 id="lab2-lab2-overview-what-is-rag">What is RAG?<a class="headerlink" href="#lab2-lab2-overview-what-is-rag" title="Permanent link">¶</a></h2>
<p>RAG (Retrieval-Augmented Generation) is an advanced technique that combines retrieval-based and generation-based approaches to improve the performance of large language models, especially in question-answering and knowledge-intensive tasks. In layman terms, clients can use RAG pattern to generate factually accurate output from LLMs, that is grounded in information in a knowledge base.</p>
<p>Key Components of RAG:</p>
<ol>
<li><strong>Retrieval</strong>: The model first retrieves relevant documents or information from a large knowledge base (like Wikipedia or custom databases) based on the user's query. This helps provide the model with factual and up-to-date information that it may not have learned during training.</li>
<li><strong>Augmented Generation</strong>: Once the relevant documents are retrieved, the model then generates a response based on the query and the retrieved documents. This ensures the generated answer is both relevant and factual, combining the reasoning power of the language model with the accuracy of external knowledge.</li>
</ol>
<h2 id="lab2-lab2-overview-how-rag-works">How RAG works?<a class="headerlink" href="#lab2-lab2-overview-how-rag-works" title="Permanent link">¶</a></h2>
<p>RAG is a technique that uses vector databases to retrieve relevant information and improve the accuracy of Large Language Models (LLMs):</p>
<ol>
<li><strong>Vector database storage</strong>: Text data is converted into vector embeddings using pre-trained models like BERT or GPT. These embeddings are then stored in a vector database.<ul>
<li>In this lab, we use <strong>all-MiniLM-L6-v2</strong> model from HF to convert text data into embeddings which are then stored in a <strong>Milvus</strong> vector DB.</li>
</ul>
</li>
<li><strong>Query conversion</strong>: When a query is posed to the AI system, it is also converted into a vector.</li>
<li><strong>Vector search</strong>: The vector database performs a vector search to find relevant embeddings from the stored dataset.</li>
<li><strong>Information retrieval</strong>: The retrieved information is then integrated into the LLM's query input.</li>
<li><strong>Response generation</strong>: The augmented query is sent to the LLM to generate an accurate answer.</li>
</ol>
<div class="admonition info">
<p class="admonition-title">Why use vector database in RAG?</p>
<p>Vector databases are used in RAG because they store data in a way that makes it easy to search and retrieve. Vector search techniques go beyond keyword matching and focus on semantic relationships, which improves the quality of the retrieved information.</p>
</div>
<h2 id="lab2-lab2-overview-need-and-advantages-of-rag">Need and advantages of RAG<a class="headerlink" href="#lab2-lab2-overview-need-and-advantages-of-rag" title="Permanent link">¶</a></h2>
<p>The need for Retrieval-Augmented Generation (RAG) arises from the limitations of current large language models (LLMs) and the growing demands for factual accuracy and knowledge scalability in AI applications. </p>
<p>Here are the key reasons why RAG is necessary and its associated advantages:</p>
<ol>
<li><strong>Handling Knowledge Gaps</strong><ul>
<li><strong>LLMs are static</strong>: Traditional language models, once trained, cannot access new or external information. They can only generate text based on the data they were trained on, which means they might miss important or up-to-date knowledge.</li>
<li><strong>RAG dynamically retrieves information</strong>: By incorporating a retrieval step, RAG can pull in relevant, up-to-date documents from external sources to complement the model's output, making it more accurate and current.</li>
</ul>
</li>
<li><strong>Reducing Hallucinations</strong><ul>
<li><strong>LLMs sometimes "hallucinate"</strong>: LLMs can generate convincing but incorrect or fabricated answers because they are predicting text based on patterns rather than verifying facts.</li>
<li><strong>RAG grounds responses in real data</strong>: Since RAG retrieves factual documents before generating a response, it ensures that the output is based on real, verifiable information, reducing the risk of false or misleading content.</li>
</ul>
</li>
<li><strong>Scalability in Knowledge</strong><ul>
<li><strong>LLMs are limited by training data</strong>: Even the largest models have limitations on how much they can remember from their training data, which might become outdated or incomplete.</li>
<li><strong>RAG scales with external data</strong>: By leveraging vast external knowledge bases or documents, RAG allows for almost unlimited knowledge expansion without retraining the model. This is particularly useful for enterprises or specific domains where continuous data updates are essential.</li>
</ul>
</li>
<li><strong>Improved Performance in Specific Domains</strong><ul>
<li><strong>Specialized knowledge is often needed</strong>: Many applications require access to niche or domain-specific information, such as legal texts, scientific papers, or proprietary company data.</li>
<li><strong>RAG retrieves domain-specific documents</strong>: The retrieval step allows RAG to pull in domain-specific or proprietary documents, making the output more relevant for specialized tasks.</li>
</ul>
</li>
<li><strong>Efficiency and Adaptability</strong><ul>
<li><strong>Model retraining is costly</strong>: Constantly retraining LLMs with new data is computationally expensive and time-consuming.</li>
<li><strong>RAG avoids retraining</strong>: By using real-time retrieval, RAG can access the latest information or new content without the need to retrain the entire model, making it more adaptable and cost-efficient.</li>
</ul>
</li>
<li><strong>Better Results for Open-Domain Question Answering</strong><ul>
<li><strong>Complex queries require precise answers</strong>: In tasks like open-domain question answering, general models might struggle to provide precise answers for complex or rare questions.</li>
<li><strong>RAG enhances accuracy</strong>: By combining retrieval with generation, RAG can provide more accurate, context-rich answers, drawing from a wide range of documents.</li>
</ul>
</li>
</ol>
<p>In summary, RAG addresses limitations in current LLMs by improving factual accuracy, scalability, and adaptability, making it particularly useful for knowledge-intensive tasks and dynamic environments.</p>
<h2 id="lab2-lab2-overview-common-use-cases-of-rag">Common use cases of RAG<a class="headerlink" href="#lab2-lab2-overview-common-use-cases-of-rag" title="Permanent link">¶</a></h2>
<p>Below are some of the common use cases for RAG:</p>
<ol>
<li><strong>Open-domain question answering</strong>: Where models need to answer questions about a wide range of topics, potentially beyond the training data.</li>
<li><strong>Customer support</strong>: Providing accurate answers by retrieving relevant documents from knowledge bases.</li>
<li><strong>Enterprise AI</strong>: Helping businesses with information retrieval, knowledge management, and research by retrieving and summarizing relevant documents.</li>
</ol>
<p>To summarize, RAG allows models to perform better in knowledge-intensive tasks by combining the strengths of both retrieval systems and generative language models.</p>
<h2 id="lab2-lab2-overview-ibm-power-and-rag">IBM Power and RAG<a class="headerlink" href="#lab2-lab2-overview-ibm-power-and-rag" title="Permanent link">¶</a></h2>
<h3 id="lab2-lab2-overview-ibm-power-systems-as-systems-of-record">IBM Power Systems as Systems of Record<a class="headerlink" href="#lab2-lab2-overview-ibm-power-systems-as-systems-of-record" title="Permanent link">¶</a></h3>
<p>IBM Power Systems are renowned for their performance, reliability, and scalability, making them ideal for handling systems of record. A system of record (SOR) refers to a trusted source of truth that stores essential business data and transactions, such as financial systems, customer data, and inventory management. IBM Power Systems are often used for mission-critical applications in industries like banking, healthcare, and government due to their ability to manage large volumes of secure, transactional data.</p>
<h3 id="lab2-lab2-overview-how-ibm-power-systems-plays-well-with-rag">How IBM Power Systems Plays Well with RAG<a class="headerlink" href="#lab2-lab2-overview-how-ibm-power-systems-plays-well-with-rag" title="Permanent link">¶</a></h3>
<p>Retrieval-Augmented Generation (RAG) is a generative AI framework that enhances the performance of AI models by retrieving relevant documents from external knowledge bases before generating a response. This retrieval step ensures that the generated output is more accurate and fact-based, as it is grounded in real-time data from a reliable source.</p>
<p>IBM Power Systems play exceptionally well with RAG due to the following reasons:</p>
<ol>
<li><strong>High-Performance Data Handling</strong><ul>
<li>Power Systems are designed for high-volume data transactions and processing. This makes them perfect for storing and managing SOR, which RAG relies on to retrieve real-time, relevant information.</li>
<li>When RAG retrieves data from a SOR stored on IBM Power, it benefits from the fast data access and throughput that Power Systems offer, ensuring quick and efficient retrieval of documents for AI processing.</li>
</ul>
</li>
<li><strong>Data Security and Compliance</strong><ul>
<li>Power Systems are known for their robust security features, including end-to-end encryption and advanced data protection, making them ideal for storing sensitive data such as customer information, financial records, or healthcare data.</li>
<li>In a RAG scenario, where retrieved data is used to generate answers, the ability of IBM Power Systems to ensure data privacy and regulatory compliance (for example, Health Insurance Portability and Accountability Act (HIPAA), General Data Protection Regulation (GDPR)) is crucial, especially for enterprises dealing with sensitive or regulated data.</li>
</ul>
</li>
<li><strong>Scalability and Reliability</strong><ul>
<li>RAG applications require scalable infrastructure to handle varying levels of computational demand, especially when dealing with large-scale document retrieval and real-time AI processing. IBM Power Systems are built to scale seamlessly, allowing RAG to handle larger datasets and more complex queries without performance degradation.</li>
<li>Reliability is critical for SOR, and Power Systems have a proven track record of uptime and resilience, ensuring that the data RAG retrieves is always available when needed, without risk of downtime affecting the retrieval process.</li>
</ul>
</li>
<li><strong>Integration with AI Workloads</strong><ul>
<li>IBM Power Systems are optimized for AI workloads, with features like accelerated AI processing (for example, Power10's MMA (Matrix Math Accelerator))) that boost the performance of both retrieval and generation tasks in RAG.</li>
<li>By running RAG-based applications on Power10, enterprises can take advantage of faster AI inference and improved data handling, resulting in more responsive and accurate AI systems.</li>
</ul>
</li>
<li><strong>Efficient Handling of Structured and Unstructured Data</strong><ul>
<li>Power Systems can efficiently handle both structured data (like databases) and unstructured data (such as documents and records), making them ideal for RAG, where both types of data may be retrieved from SOR.</li>
<li>RAG can retrieve structured data for quick reference (for example, customer records or transactions) and unstructured data (for example, reports, emails) for more complex, context-driven AI responses. Power Systems' capability to manage both types ensures efficient, accurate information retrieval.</li>
</ul>
</li>
<li><strong>Real-Time Analytics</strong><ul>
<li>Real-time data analytics capabilities in IBM Power Systems enable quick access to up-to-date information, which is essential for RAG when generating responses based on current data.</li>
<li>This feature allows the RAG model to provide contextualized answers based on the latest transactions or data updates from the SOR, improving the relevance of AI-driven outputs.</li>
</ul>
</li>
</ol>
<p><strong>Summary</strong></p>
<p>IBM Power Systems provide the speed, security, and reliability needed to store and manage SOR that RAG models depend on for data retrieval. Power Systems’ advanced capabilities in data handling, AI optimization, scalability, and security make them an ideal infrastructure for supporting RAG-based AI applications, ensuring that retrieved data is accurate, current, and secure, thus enhancing the quality of generative AI outputs.</p>
<h3 id="lab2-lab2-overview-rag-and-ibm-power10-solution-architecture">RAG and IBM Power10 solution architecture<a class="headerlink" href="#lab2-lab2-overview-rag-and-ibm-power10-solution-architecture" title="Permanent link">¶</a></h3>
<p>Here is the high level solution architecture of a typical RAG use case on IBM Power10. This example is deployed on IBM Power10 end-to-end. The foundation model is simply downloaded from watsonx.ai or open-source repositories such as the Hugging Face model hub. The model does not require fine-tuning thanks to a domain adaptation technique called RAG. </p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/3476cad1-a743-474f-8535-b70806d8c09f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/3476cad1-a743-474f-8535-b70806d8c09f"></a></p>
<ol>
<li>User asks a domain specific question in natural language.</li>
<li>Q&amp;A app looksup in the knowledge base repository.</li>
<li>Documents relevant to the question is retrieved from the repository.</li>
<li>"Question + Documents" is passed as the context in a prompt to LLM.</li>
<li>LLM generates the domain-specific answer.</li>
</ol></div></section><section class="print-page" id="lab2-lab2-hands-on-guide"><div><h1 id="deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-guide">Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - lab guide<a class="headerlink" href="#lab2-lab2-hands-on-guide-deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-guide" title="Permanent link">¶</a></h1>
<h2 id="lab2-lab2-hands-on-guide-lab-ready-check">Lab-ready check<a class="headerlink" href="#lab2-lab2-hands-on-guide-lab-ready-check" title="Permanent link">¶</a></h2>
<p>Make sure you have the following items ready:</p>
<ul>
<li>In your browser, you have logged in as cecuser in the OpenShift console.</li>
<li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI oc.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not proceed further unless the items mentioned above are ready. Please refer to "Lab setup instructions" section (see left hand side menu) to setup your browser and terminal windows.</p>
</div>
<h2 id="lab2-lab2-hands-on-guide-lab-guide">Lab guide<a class="headerlink" href="#lab2-lab2-hands-on-guide-lab-guide" title="Permanent link">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Image zoom functionality</p>
<p>Feel free to click on the images in the lab guide below to a view larger image.</p>
</div>
<p>In this lab, we will focus on the below:</p>
<ul>
<li>
<p>Deploy a vector database (DB) - milvus.</p>
</li>
<li>
<p>Deploy a jupyter notebook where we will implement RAG pattern and learn about:</p>
<ul>
<li>Index the milvus DB with a sample PDF.</li>
<li>Query the DB to get relevant documents for the question asked.</li>
<li>Create a prompt based on the relevant documents gotten from the previous step and send it to the LLM (IBM Granite model we deployed in Lab1) to get a domain specific answer.</li>
</ul>
</li>
</ul>
<h3 id="lab2-lab2-hands-on-guide-create-project">Create project<a class="headerlink" href="#lab2-lab2-hands-on-guide-create-project" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Let's create a new OpenShift project that will hold all resources of this lan. Navigate to your browser window/tab which has the <strong>Administrator</strong> profile. Select <strong>Home</strong> <strong>(A)</strong> -&gt; <strong>Projects</strong> <strong>(B)</strong> and click <strong>Create Project</strong> <strong>(C)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/839bbfd3-e4a0-4c19-9223-c01e6ce60221" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/839bbfd3-e4a0-4c19-9223-c01e6ce60221"></a></p>
</li>
<li>
<p>In the resulting form, enter <strong>lab2-demo</strong> <strong>(A)</strong> as the project name and click <strong>Create</strong> <strong>(B)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/4bec3979-3d79-445e-88ff-3318c60123ea" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4bec3979-3d79-445e-88ff-3318c60123ea"></a></p>
</li>
</ol>
<h3 id="lab2-lab2-hands-on-guide-deploy-milvus">Deploy milvus<a class="headerlink" href="#lab2-lab2-hands-on-guide-deploy-milvus" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Navigate to the terminal window where we had setup <code>oc</code> CLI on the bastion node.</p>
<div class="admonition warning">
<p class="admonition-title">Ensure <code>oc</code> CLI is authenticated with the cluster</p>
<p>Run a simple command: <code>oc project</code> and if it throws error you need re-authenticate with the cluster. Follow the steps mentioned in <a href="https://ibm.github.io/AI-on-Power-Level3/lab-setup/#logging-in-to-openshift-cluster-using-oc-cli" target="_blank">lab instructions</a> to ensure <code>oc</code> is authenticated with the cluster.</p>
</div>
</li>
<li>
<p>Make sure you are in the home directory and then run the command below to clone the github repository. Then switch to the newly cloned repository directory.</p>
<ul>
<li><code>cd</code> <strong>(A)</strong></li>
<li><code>git clone https://github.com/dpkshetty/bcn-lab-2084</code> <strong>(B)</strong></li>
<li><code>cd bcn-lab-2084/</code> <strong>(C)</strong></li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/138af434-6b94-48ef-998f-b5bbe8fc86dd" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/138af434-6b94-48ef-998f-b5bbe8fc86dd"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/82c6187a-7163-4fe6-b6c6-c62a7e036bb2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82c6187a-7163-4fe6-b6c6-c62a7e036bb2"></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/bcb3f05c-5352-4f0f-92ff-8521d6857bfe" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/bcb3f05c-5352-4f0f-92ff-8521d6857bfe"></a></p>
</li>
<li>
<p>Make sure <code>oc</code> CLI is pointing to the lab2-demo project.</p>
<p><code>oc project lab2-demo</code>   </p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/50330a50-d662-4967-83a3-86e5618fc1ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/50330a50-d662-4967-83a3-86e5618fc1ce"></a></p>
</li>
<li>
<p>Run the below set of commands to deploy milvus DB.</p>
<p><code>cd Part2-RAG/milvus-deployment</code> <strong>(A)</strong></p>
<p><code>oc create configmap milvus-config --from-file=./config/milvus.yaml</code> <strong>(B)</strong></p>
<p><code>oc apply -f .</code> <strong>(C)</strong></p>
<p><code>cd ..</code> <strong>(D)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/38e7a10e-427d-4f22-97a7-8b421870723d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/38e7a10e-427d-4f22-97a7-8b421870723d"></a></p>
</li>
<li>
<p>Monitor deployment using the below command until all pods are in Running state. <br>
   Hit Ctrl-C on the keyboard to exit and come back to the shell prompt.</p>
<p><code>oc get pods -w</code> <strong>(A)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/67a1498b-25f2-4e19-be32-530fec0ea62a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/67a1498b-25f2-4e19-be32-530fec0ea62a"></a></p>
</li>
</ol>
<h3 id="lab2-lab2-hands-on-guide-deploy-jupyter-notebook">Deploy jupyter notebook<a class="headerlink" href="#lab2-lab2-hands-on-guide-deploy-jupyter-notebook" title="Permanent link">¶</a></h3>
<ol>
<li>
<p>Run the below set of commands to deploy jupyter notebook (NB).<br>Ignore any warnings if seen.</p>
<p><code>cd nb-deployment</code> <strong>(A)</strong></p>
<p><code>oc apply -f .</code> <strong>(B)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/e668c714-2559-4df5-a595-e5c9c02bef20" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e668c714-2559-4df5-a595-e5c9c02bef20"></a></p>
</li>
<li>
<p>Verify the notebook pod is running using the command below. Hit Ctrl-C on keyboard to exit and return back to shell prompt.</p>
<p><code>oc get pods --selector=app=cpu-notebook -w</code> <strong>(A)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/d188af80-225a-45ff-b28d-f37c5d257dbf" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d188af80-225a-45ff-b28d-f37c5d257dbf"></a></p>
</li>
<li>
<p>Once the notebook pod is deployed you should be able to access it using the link retrieved from the below command:</p>
<p><code>oc get route cpu-notebook -o jsonpath='{.spec.host}'</code> <strong>(A)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/5d89416b-ff39-49ba-aaec-1155df45b9c7" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/5d89416b-ff39-49ba-aaec-1155df45b9c7"></a></p>
<p>In my case, the URL was: <br>
   <code>cpu-notebook-lab2-demo.apps.p1279.cecc.ihost.com</code> <br>
 but yours can be different! <br></p>
<div class="admonition note">
<p class="admonition-title">Alternate way to get the jupyter NB URL</p>
<p>You can also goto OpenShift <strong>Administrator</strong> profile console window in your browser, navigate to <strong>Networking</strong> -&gt; <strong>Routes</strong>, select <strong>cpu-notebook</strong> route and click on the URL mentioned under <strong>Location</strong> field.</p>
</div>
</li>
<li>
<p>Copy and paste the URL in the browser. You should see the jupyter screen as below:
    <a class="glightbox" href="https://github.com/user-attachments/assets/ee5cf9c5-8f3f-48d4-8741-08d7ae5617ab" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ee5cf9c5-8f3f-48d4-8741-08d7ae5617ab"></a></p>
</li>
<li>
<p>Now let's copy the jupyter NB (.ipynb file) present in the git repository to the NB pod. <br>
   In your <code>oc</code> CLI terminal window, navigate to the root of your git repository which has the <strong>RAG.ipynb</strong> file.</p>
<p><code>cd /home/cecuser/bcn-lab-2084</code> <strong>(A)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/706f1bbe-cbcc-492b-b65f-ca3c9820181b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/706f1bbe-cbcc-492b-b65f-ca3c9820181b"></a></p>
</li>
<li>
<p>List pods and copy the name of the cpu-notebook pod.</p>
<p><code>oc get pods</code> <strong>(A)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/b6cd888d-31b1-4212-9a2f-7911c481941d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b6cd888d-31b1-4212-9a2f-7911c481941d"></a></p>
</li>
<li>
<p>Use <strong>oc cp ...</strong> command to copy the NB file from bastion server to <strong>/tmp/notebooks/</strong> directory of the NB pod.</p>
<p><code>oc cp ./RAG.ipynb cpu-notebook:/tmp/notebooks/</code> <strong>(A)</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/bdfa2804-3a18-4b31-b47d-e3dd8345eea2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/bdfa2804-3a18-4b31-b47d-e3dd8345eea2"></a></p>
</li>
<li>
<p>Go back to the jupyter NB application in your browser and hit <strong>refresh</strong> (F5 shortcut in keyboard) <strong>(A)</strong>. You should be able to see the <strong>RAG.ipynb</strong> <strong>(B)</strong> file listed.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/c0c44ba3-88d8-40b0-850d-f57a78f16e64" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/c0c44ba3-88d8-40b0-850d-f57a78f16e64"></a></p>
</li>
<li>
<p><strong>Double-click</strong> <strong>(A)</strong> on the <strong>RAG.ipynb</strong> file and it should open up in the right pane of the browser.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/aebc53e3-6a93-4378-b3dd-6d9b6c7ec180" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/aebc53e3-6a93-4378-b3dd-6d9b6c7ec180"></a></p>
</li>
<li>
<p>You're all set! Follow the instructions given in the NB to finish this lab.</p>
</li>
</ol></div></section><h1 class='nav-section-title-end'>Ended: Lab 2 - Deploy RAG on Power10</h1>
                        <h1 class='nav-section-title' id='section-lab-3-deploy-code-llm-on-power10'>
                            Lab 3 - Deploy code LLM on Power10 <a class='headerlink' href='#section-lab-3-deploy-code-llm-on-power10' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="lab3-lab3-overview"><div><h1 id="generate-sql-query-using-code-large-language-model-llm-on-ibm-power10-lab-education">Generate SQL query using code Large Language Model (LLM) on IBM Power10 - Lab education<a class="headerlink" href="#lab3-lab3-overview-generate-sql-query-using-code-large-language-model-llm-on-ibm-power10-lab-education" title="Permanent link">¶</a></h1>
<p>Goal of this lab is to acquire some hands-on experience with code LLMs and understand their use cases.
In this lab, we will use a code LLM to convert a natural language query into a SQL statement which can be used to query DBs.
We will also to generate some python and C++ code and understand how better prompting can generate code closer to your expectations.</p>
<p>But before we do that, lets understand what code LLM is, its benefits, key use cases, and IBM products around code LLMs.</p>
<h2 id="lab3-lab3-overview-what-is-code-llm">What is code LLM?<a class="headerlink" href="#lab3-lab3-overview-what-is-code-llm" title="Permanent link">¶</a></h2>
<p>A code LLM is a type of AI model specifically trained to understand, generate, and manipulate programming code.
These models, built on architectures like GPT or similar transformers, are trained on vast datasets of code from various programming languages, enabling them to assist with coding tasks such as generating code snippets, debugging, refactoring, and even writing documentation.</p>
<h2 id="lab3-lab3-overview-key-features-of-code-llm">Key Features of code LLM<a class="headerlink" href="#lab3-lab3-overview-key-features-of-code-llm" title="Permanent link">¶</a></h2>
<ul>
<li><strong>Code Generation</strong>: Code LLMs can generate new code based on prompts, such as writing functions, classes, or even entire programs.</li>
<li><strong>Code Completion</strong>: Similar to how text-based LLMs suggest completions for sentences, code LLMs can suggest code completions in real-time, aiding developers in writing code faster.</li>
<li><strong>Bug Detection and Fixes</strong>: These models can detect bugs or potential issues in code and suggest corrections.</li>
<li><strong>Multi-Language Support</strong>: Code LLMs are typically trained on multiple programming languages, allowing them to work across various tech stacks (for example, Python, Java, JavaScript, C++, etc.).</li>
<li><strong>Code Explanation</strong>: Some models can explain how a piece of code works, making them useful for learning, documentation, and debugging.</li>
</ul>
<h2 id="lab3-lab3-overview-use-cases">Use Cases<a class="headerlink" href="#lab3-lab3-overview-use-cases" title="Permanent link">¶</a></h2>
<ul>
<li><strong>Autocompletion in integrated developement environment (IDE)</strong>: Code LLMs like GitHub Copilot, powered by OpenAI's Codex, assist developers by offering code suggestions and completions.</li>
<li><strong>Code Refactoring</strong>: LLMs can suggest improvements or optimizations to existing code, helping improve performance or readability.</li>
<li><strong>Debugging and Error Resolution</strong>: Code LLMs can help identify issues in code and suggest potential fixes, streamlining the debugging process.</li>
<li><strong>Automated Documentation</strong>: They can generate comments or documentation for code, reducing the manual effort required for explaining code logic.</li>
</ul>
<h2 id="lab3-lab3-overview-examples-of-code-llms">Examples of code LLMs<a class="headerlink" href="#lab3-lab3-overview-examples-of-code-llms" title="Permanent link">¶</a></h2>
<p>Below are few examples of code LLMs:</p>
<ul>
<li><strong>OpenAI Codex</strong>: The LLM that powers GitHub Copilot, trained specifically on a large corpus of programming languages and capable of generating code.</li>
<li><strong>AlphaCode</strong>: DeepMind's LLM designed for competitive programming tasks.</li>
<li><strong>CodeT5</strong>: A transformer model from Hugging Face fine-tuned for code generation and understanding tasks.</li>
<li><strong>Code Llama</strong>: It is a large language model developed by Meta, optimized for coding tasks such as code generation, completion, and debugging, supporting multiple programming languages for enhanced developer productivity.</li>
<li><strong>IBM Granite code models</strong>: A family of open foundation models for code intelligence. It is an enterprise-focused large language model developed by IBM Research, designed to enhance productivity in software development by enabling code generation, debugging, and refactoring with a strong emphasis on security and performance.</li>
</ul>
<h2 id="lab3-lab3-overview-benefits-of-code-llms">Benefits of code LLMs<a class="headerlink" href="#lab3-lab3-overview-benefits-of-code-llms" title="Permanent link">¶</a></h2>
<ul>
<li><strong>Boosts Developer Productivity</strong>: By automating repetitive coding tasks, Code LLMs help developers write code faster and reduce errors.</li>
<li><strong>Assists with Learning</strong>: Code LLMs are valuable for learners by explaining how code works and suggesting how to fix bugs.</li>
<li><strong>Cross-Language Compatibility</strong>: Supporting multiple languages, Code LLMs help developers work across different coding environments without needing expertise in all languages.</li>
</ul>
<p>In essence, Code LLMs bring AI-driven enhancements to the software development process, making it faster, more accurate, and more accessible.</p>
<h2 id="lab3-lab3-overview-ibm-watsonx-code-assistant">IBM watsonx Code Assistant<a class="headerlink" href="#lab3-lab3-overview-ibm-watsonx-code-assistant" title="Permanent link">¶</a></h2>
<p>IBM watsonx Code Assistant (WCA) is a family of offerings from IBM that leverages generative AI to accelerate development while maintaining the principles of trust, security and compliance at its core. Developers and IT Operators can speed up application modernization efforts and generate automation to rapidly scale IT environments. </p>
<p>WCA is powered by the IBM Granite foundation models that include state-of-the-art large language models designed for code, geared to help IT teams create high-quality code using AI-generated recommendations based on natural language requests or existing source code. Built on the watsonx AI platform, this tool leverages IBM’s expertise in AI and machine learning to enhance productivity across various industries.</p>
<div class="admonition danger">
<p class="admonition-title">IBM watsonx Code Assistant (WCA) support on IBM Power</p>
<p>NOTE: At the time of writing, IBM WCA is not yet available to run on IBM Power systems. However, the LLMs that support its functionality are fully compatible with IBM Power. Clients can leverage foundation models, including IBM-developed and open-source options, to integrate and utilize (gen)AI capabilities within their on-premises applications running on IBM Power. This course's labs demonstrate the use of such LLMs.</p>
</div>
<h3 id="lab3-lab3-overview-ibm-wca-architecture-and-offerings">IBM WCA architecture and offerings<a class="headerlink" href="#lab3-lab3-overview-ibm-wca-architecture-and-offerings" title="Permanent link">¶</a></h3>
<p>Here is a high level architecture of IBM WCA:
<a class="glightbox" href="https://github.com/user-attachments/assets/71decd7c-fc8e-46a0-8765-ecfc38b897d4" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/71decd7c-fc8e-46a0-8765-ecfc38b897d4"></a>
At the bottom-most layer, WCA is powered by IBM Granite code models, specifically trained and fine-tuned by IBM for different programming languages.</p>
<p>At the time of writing this lab, there are 3 products available under the IBM WCA offering family:</p>
<ul>
<li>
<p><strong>IBM watsonx Code Assistant</strong>: IBM watsonx Code Assistant (WCA) is the flagship offering in a suite of generative AI code assistant products, which also include offerings for Ansible Automation Platform (via IBM watsonx Code Assistant for Red Hat Ansible Lightspeed) and IBM Z modernization (via IBM watsonx Code Assistant for Z).  IBM WCA delivers enterprise-ready AI for code solutions to address skills gaps and increase developer productivity for targeted, business use cases. The WCA portfolio accelerates software development lifecycle (SDLC) tasks with AI-powered capabilities including context-aware code generation, explanation, documentation, translation, and unit test generation. It does so while maintaining the principles of trust, security, and compliance with regards to IBM client's data and intellectual property. WCA offerings are powered by IBM Granite foundation models that include state-of-the-art large language models (LLMs) designed for code. WCA empowers developers to accelerate software development lifecycles, enhance productivity, and improve code for over 116 languages — Java, Python, YAML, COBOL, and more.</p>
</li>
<li>
<p><strong>IBM watsonx Code Assistant for Red Hat Ansible Lightspeed</strong>: The Ansible-tuned model forms the basis of this product. This is fine-tuned for Ansible use cases. In addition to x86 endpoints, this supports generating Ansible tasks/playbooks for IBM Power endpoints (running AIX, IBM i and Linux) as well.</p>
</li>
<li>
<p><strong>IBM watsonx Code Assistant for Z</strong>: The Cobol to Java-tuned model forms the basis of this product. This is fine-tuned for COBOL to Java conversion. It can help with enterprise use cases around IBM Z application modernization.</p>
</li>
</ul>
<h3 id="lab3-lab3-overview-key-features-of-ibm-wca">Key Features of IBM WCA<a class="headerlink" href="#lab3-lab3-overview-key-features-of-ibm-wca" title="Permanent link">¶</a></h3>
<ol>
<li><strong>Code Generation</strong>: Automatically generates code snippets or complete functions based on natural language prompts or specific programming needs.</li>
<li><strong>Debugging Assistance</strong>: Helps identify bugs and suggests fixes, improving the efficiency of the development process.</li>
<li><strong>Multi-Language Support</strong>: Supports a variety of programming languages, making it versatile for different development environments.</li>
<li><strong>Security and Compliance</strong>: Designed with a strong focus on enterprise-level security, ensuring that generated code adheres to industry standards and compliance regulations.</li>
<li><strong>Customization</strong>: Tailors AI recommendations to the specific needs of a project, enabling more accurate code suggestions and personalized developer assistance.</li>
</ol>
<h3 id="lab3-lab3-overview-benefits-of-ibm-wca">Benefits of IBM WCA<a class="headerlink" href="#lab3-lab3-overview-benefits-of-ibm-wca" title="Permanent link">¶</a></h3>
<ol>
<li><strong>Increased Developer Productivity</strong>: Automates routine coding tasks, allowing developers to focus on higher-level problem-solving.</li>
<li><strong>Enhanced Code Quality</strong>: Improves the accuracy of code with AI-powered insights, reducing the chances of errors or vulnerabilities.</li>
<li><strong>Enterprise-Grade Security</strong>: Ensures that code generation is secure and compliant with regulations, making it suitable for industries like finance and healthcare.</li>
</ol></div></section><section class="print-page" id="lab3-lab3-hands-on-guide"><div><h1 id="use-natural-language-to-generate-code-using-code-llm-on-ibm-power10-lab-guide">Use natural language to generate code using code LLM on IBM Power10 - Lab guide<a class="headerlink" href="#lab3-lab3-hands-on-guide-use-natural-language-to-generate-code-using-code-llm-on-ibm-power10-lab-guide" title="Permanent link">¶</a></h1>
<h2 id="lab3-lab3-hands-on-guide-lab-ready-check">Lab-ready check<a class="headerlink" href="#lab3-lab3-hands-on-guide-lab-ready-check" title="Permanent link">¶</a></h2>
<p>Make sure you have the following items ready:</p>
<ul>
<li>In your browser, you have logged in as cecuser in the OpenShift console.</li>
<li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI oc.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not proceed further unless the items mentioned above are ready. Please refer to "Lab setup instructions" section (see left hand side menu) to setup your browser and terminal windows.</p>
</div>
<h2 id="lab3-lab3-hands-on-guide-lab-guide">Lab guide<a class="headerlink" href="#lab3-lab3-hands-on-guide-lab-guide" title="Permanent link">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Image zoom functionality</p>
<p>Feel free to click on the images in the lab guide below to a view larger image.</p>
</div>
<h3 id="lab3-lab3-hands-on-guide-deploy-ibm-granite-code-llm">Deploy IBM Granite code LLM<a class="headerlink" href="#lab3-lab3-hands-on-guide-deploy-ibm-granite-code-llm" title="Permanent link">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Pre-requisite</p>
<p>This lab assumes you have finished Lab 1. This lab uses the OpenShift resources deployed in Lab 1 to optimize the usage of TechZone resources and to avoid re-deploying the same resources and re-learning the same concepts already taught in Lab 1!</p>
</div>
<ol>
<li>
<p>In this lab, we will deploy IBM Granite code LLM which is available on Hugging Face (HF). Navigate to OpenShift developer profile window and ensure you are in <strong>lab1-demo</strong> <strong>(A)</strong> project. Click <strong>Topology</strong> <strong>(B)</strong> and ensure that your application is healthy and running (has a blue circle) before proceeding further.
   <a class="glightbox" href="https://github.com/user-attachments/assets/e64ca98b-4abc-4ba4-b483-57e38482073a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/e64ca98b-4abc-4ba4-b483-57e38482073a"></a></p>
</li>
<li>
<p>Select <strong>ConfigMaps</strong> <strong>(A)</strong> and click <strong>model-params</strong> <strong>(B)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/59e35af8-0b8a-4c3a-a085-4d6e3d6841f1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/59e35af8-0b8a-4c3a-a085-4d6e3d6841f1"></a></p>
</li>
<li>
<p>Click on <strong>Actions</strong> <strong>(A)</strong> -&gt; <strong>Edit ConfigMap</strong> <strong>(B)</strong>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/687cce4c-6d96-4693-883b-432b1d3bcae2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/687cce4c-6d96-4693-883b-432b1d3bcae2"></a></p>
</li>
<li>
<p>In the resulting form, the key/value fields for MODEL_NAME and MODEL_URL as below and click <strong>Save</strong> <strong>(E)</strong>.</p>
<ul>
<li><strong>Key</strong>: <code>MODEL_NAME</code> <strong>(A)</strong></li>
<li><strong>Value</strong>: <code>granite-8b-code-instruct.Q4_K_M.gguf</code> <strong>(B)</strong></li>
</ul>
<hr>
<ul>
<li><strong>Key</strong>: <code>MODEL_URL</code> <strong>(C)</strong></li>
<li><strong>Value</strong>: <code>https://huggingface.co/ibm-granite/granite-8b-code-instruct-4k-GGUF/resolve/main/granite-8b-code-instruct.Q4_K_M.gguf</code> <strong>(D)</strong></li>
</ul>
<hr>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/ed94d17e-283e-4f90-9cf5-9b11c6a4adb1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/ed94d17e-283e-4f90-9cf5-9b11c6a4adb1"></a></p>
<hr>
</li>
<li>
<p>Use the deployment resource to restart the pods.</p>
<div class="admonition note">
<p class="admonition-title">ConfigMap update does not restart pods automatically</p>
<p>The existing pod won't see the ConfigMap changes right away as changing values of a ConfigMap doesn't cause a deployment (and hence pod) to restart automatically. It needs to be done manually.</p>
</div>
</li>
<li>
<p>Navigate to the deployment view. Click <strong>Topology</strong> <strong>(A)</strong>, then click "<strong>D lab1-demo</strong>" <strong>(B)</strong> part of the application icon, which will open up the deployment details pane (on the right hand side of the browser window). Click "<strong>D lab1-demo</strong>" <strong>(C)</strong> in that pane which will then open up the deployment details view for lab1-demo deployment.
   <a class="glightbox" href="https://github.com/user-attachments/assets/07715c25-5b84-44d4-8e74-a1a989581d80" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/07715c25-5b84-44d4-8e74-a1a989581d80"></a></p>
</li>
<li>
<p>Click <strong>Actions</strong> <strong>(A)</strong> and select <strong>Restart rollout</strong> <strong>(B)</strong>. This will restart the deployment which results in re-deployment of the pod. This ensure the pod will now see the new ConfigMap changes.
   <a class="glightbox" href="https://github.com/user-attachments/assets/bca10454-1bd3-4e7a-ad7d-e3340aaedc73" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/bca10454-1bd3-4e7a-ad7d-e3340aaedc73"></a></p>
</li>
<li>
<p>Click on <strong>Pods</strong> <strong>(A)</strong> tab, where you will see a new pod instantiated. The new pod will download the model and then start it. Since the configmap points to IBM Granite code model, it will be downloaded from HuggingFace and then started. When that happens the new pod's status will change to Running and the existing pod will be terminated.
    <a class="glightbox" href="https://github.com/user-attachments/assets/4299ccb0-9233-45f8-904e-9fdb4960ad12" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4299ccb0-9233-45f8-904e-9fdb4960ad12"></a></p>
<div class="admonition info">
<p class="admonition-title">Model download will take time</p>
<p>This process will take a few minutes (in my case it took around 3-4 mins as this is a fairly large model compared to tinyllama) and your results may vary. Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV).</p>
</div>
</li>
<li>
<p>Provided the previous step completed without errors, you should see just 1 pod running.
    <a class="glightbox" href="https://github.com/user-attachments/assets/32fb028f-fef6-4abe-996c-b1bdfbe80489" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/32fb028f-fef6-4abe-996c-b1bdfbe80489"></a></p>
</li>
<li>
<p>Let us verify that the model running is IBM Granite code LLM! Click on the pod name <strong>(A)</strong> to enter the pod details view/page.
    <a class="glightbox" href="https://github.com/user-attachments/assets/f7ae35e7-808e-4a92-bbad-804a5284fffa" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/f7ae35e7-808e-4a92-bbad-804a5284fffa"></a></p>
</li>
<li>
<p>In the pod details page, click on the <strong>Logs</strong> <strong>(A)</strong> tab to see the pod logs.
    <a class="glightbox" href="https://github.com/user-attachments/assets/9e03dd0f-a3c4-457a-b735-03a8cfce48a2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/9e03dd0f-a3c4-457a-b735-03a8cfce48a2"></a></p>
</li>
<li>
<p>In the log window, scroll upwards to see the name of the model against the attribute <strong>llm_load_print_meta: general.name</strong>.</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/a84291e2-c952-4710-a2cd-88caed8b4dd2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/a84291e2-c952-4710-a2cd-88caed8b4dd2"></a></p>
<p>This verifies that we have indeed deployed IBM Granite code LLM.</p>
</li>
<li>
<p>Let us access our model. As we did in Lab 1, head to <strong>Topology</strong> <strong>(A)</strong> view, click on the <strong>Open URL</strong> <strong>(B)</strong> icon of your application.
    <a class="glightbox" href="https://github.com/user-attachments/assets/40a959ed-e233-4597-b904-5ea4dd9eb96d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/40a959ed-e233-4597-b904-5ea4dd9eb96d"></a></p>
</li>
<li>
<p>A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:
    <a class="glightbox" href="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2237409c-7160-471f-aafa-f0e1254c5a53"></a></p>
</li>
<li>
<p>Scroll all the way down to the input field "Say something..." where you can interact with the LLM. You can ask any question you like!
    <a class="glightbox" href="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/82196cf5-d4c2-459d-af7e-c24650f1f6ce"></a></p>
<div class="admonition note">
<p class="admonition-title">Experimenting with model parameters</p>
<p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Visit the model specific documentation and learn about them and experiment with it. You may want to change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab.</p>
</div>
</li>
</ol>
<h3 id="lab3-lab3-hands-on-guide-generate-python-code">Generate python code<a class="headerlink" href="#lab3-lab3-hands-on-guide-generate-python-code" title="Permanent link">¶</a></h3>
<p>Now let's use the IBM Granite code model to generate python code using natural language queries.</p>
<ol>
<li>
<p>Generating python code for printing the fibonacci series.<br> The input provided was: <code>python for finonacci sequence</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/031c1d4a-d700-4f2a-89ec-87e49b1cc8f3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/031c1d4a-d700-4f2a-89ec-87e49b1cc8f3"></a></p>
<ul>
<li>
<p>The code snippet above was run on my local python environment and it ran without errors!</p>
</li>
<li>
<p>Note that I spelled fibonacci incorrectly, yet it understood my question correctly.</p>
</li>
<li>
<p>Also note that the answer it gave uses recursion (function fib is being called recursively).</p>
</li>
</ul>
</li>
<li>
<p>Now let's try asking it to generate the same code without using recursion.<br> The input provided was: <code>give me python code for generating fibonacci sequence without using recursion</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/74f1ce36-7aee-40ac-8444-8e212402745b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/74f1ce36-7aee-40ac-8444-8e212402745b"></a></p>
<ul>
<li>
<p>As expected, it did give the code snippet that doesn't use recursion, so it did well there.</p>
</li>
<li>
<p>Generated code is not 100% correct. The code snippet above was run on my local python environment and it ran into some issues and had to fix some code to make it generate the right fibonacci sequence.</p>
</li>
<li>
<p>The above shows that code LLMs can generate almost perfect code and in some cases it might need developer intervention to make it perfect!</p>
</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Accuracy of code LLMs</p>
<ul>
<li>LLMs excel at generating simple or boilerplate code, often producing highly accurate results for common tasks such as sorting algorithms, basic input/output operations, or template-based functions.</li>
<li>For routine tasks and widely-used languages like Python or JavaScript, accuracy rates can be high, sometimes upwards of 80-90% for straightforward problems.</li>
<li>When dealing with more complex algorithms, nuanced edge cases, or multi-step logic, LLMs may struggle. The model can produce syntactically correct code that might not solve the problem as intended or might have logic errors.</li>
<li>For complex use cases, accuracy can drop significantly, often requiring human intervention to correct the output.</li>
<li>AI-generated content may vary and may not always provide consistent answers. Your results may vary.</li>
</ul>
</div>
</li>
</ol>
<p>Feel free to explore and try out more scenarios!</p>
<h3 id="lab3-lab3-hands-on-guide-generate-c-code">Generate C code<a class="headerlink" href="#lab3-lab3-hands-on-guide-generate-c-code" title="Permanent link">¶</a></h3>
<p>Now let's use the IBM Granite code model to generate C code using natural language queries.</p>
<ol>
<li>
<p>Generate C code for sorting a list of numbers.<br> The input provided was: <code>C code to sort a list of numbers</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/57ae7cf6-2d8f-4b20-8618-5c19fe2b833b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/57ae7cf6-2d8f-4b20-8618-5c19fe2b833b"></a></p>
<ul>
<li>The code snippet above was run on my local C environment and it ran without errors! Ofcourse I had to fix the first line (which was incomplete) as <code>#include &lt;stdio.h&gt;</code>.</li>
</ul>
</li>
<li>
<p>Generate C code for swapping 2 numbers without using a temporary variable.<br> The input provided was: <code>C code for swapping 2 numbers</code>.
   <a class="glightbox" href="https://github.com/user-attachments/assets/86fe9b4b-0684-45c0-a3da-c159121384ef" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/86fe9b4b-0684-45c0-a3da-c159121384ef"></a></p>
<ul>
<li>The code snippet above was run on my local C environment and it ran without errors! Ofcourse I had to fix the first line (which was incomplete) as <code>#include &lt;stdio.h&gt;</code>.</li>
</ul>
<p><strong>NOTE</strong>: AI-generated content may vary and may not always provide consistent answers. Your results may vary.
   Feel free to explore and try out more scenarios!</p>
</li>
</ol>
<h3 id="lab3-lab3-hands-on-guide-generate-sql-query">Generate SQL query<a class="headerlink" href="#lab3-lab3-hands-on-guide-generate-sql-query" title="Permanent link">¶</a></h3>
<p>Generating SQL query using natural langguage is a little different than C or python code since SQL is a language to query Databases (DBs). One needs to give enough context to the code LLM for it to understand the DB table schemas for which you want it to generate the SQL query.</p>
<p>The context is given as a prompt to the code LLM which preceeds the natural language query and the code LLM answers the query using the context given in the prompt. The best way to understand is looking at some examples as depicted below.</p>
<p>Let's take an super simple example of a bank which has information stored in 2 tables in a DB:</p>
<ul>
<li>USERS - This which has user specific info (user_id, name, age, dob etc).</li>
<li>ACCOUNTS - This holds the account balance of each user with user_id being the common field between the 2 tables.</li>
</ul>
<ol>
<li>Given the above DB example, the prompt (also known as context) we need to provide to the code LLM is as below:
   <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#lab3-lab3-hands-on-guide-__codelineno-0-1"></a>You are a developer writing SQL queries given natural language questions. The database contains a set of 2 tables. The schema of each table with description of the attributes is given. Write the SQL query given a natural language statement with names being not case sensitive.
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#lab3-lab3-hands-on-guide-__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#lab3-lab3-hands-on-guide-__codelineno-0-3"></a> Here are the 2 tables :
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#lab3-lab3-hands-on-guide-__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#lab3-lab3-hands-on-guide-__codelineno-0-5"></a> (1) Database Table Name: USERS
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#lab3-lab3-hands-on-guide-__codelineno-0-6"></a> Table Schema:
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#lab3-lab3-hands-on-guide-__codelineno-0-7"></a> Column Name # Meaning
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#lab3-lab3-hands-on-guide-__codelineno-0-8"></a> user_id # unique identifier of the user
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#lab3-lab3-hands-on-guide-__codelineno-0-9"></a> user_name # name of the user
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#lab3-lab3-hands-on-guide-__codelineno-0-10"></a> usertypeid # user is '\''employee'\'', '\''customer'\''
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#lab3-lab3-hands-on-guide-__codelineno-0-11"></a> gender_id # user'\''s gender is 1 for female, 2 for male and 3 for other
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#lab3-lab3-hands-on-guide-__codelineno-0-12"></a> dob # date of birth of the user
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#lab3-lab3-hands-on-guide-__codelineno-0-13"></a> address # adress of the user
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#lab3-lab3-hands-on-guide-__codelineno-0-14"></a> state # state of the user
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#lab3-lab3-hands-on-guide-__codelineno-0-15"></a> country # country of residence of the user
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#lab3-lab3-hands-on-guide-__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#lab3-lab3-hands-on-guide-__codelineno-0-17"></a> (2) Database Table Name: ACCOUNTS
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#lab3-lab3-hands-on-guide-__codelineno-0-18"></a> Table Schema:
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#lab3-lab3-hands-on-guide-__codelineno-0-19"></a> Column Name # Meaning
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#lab3-lab3-hands-on-guide-__codelineno-0-20"></a> acc_id # account number or account id of the user
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#lab3-lab3-hands-on-guide-__codelineno-0-21"></a> user_id # user id of the user
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#lab3-lab3-hands-on-guide-__codelineno-0-22"></a> balance # available balance in the account
</code></pre></div></li>
<li>
<p>Natural language query can be something like:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#lab3-lab3-hands-on-guide-__codelineno-1-1"></a>With the above schema, please generate sql query to list all users whose balance is &gt; 2000.
</code></pre></div>
</li>
<li>
<p>Let's send the "Prompt + Query" to the code LLM and see how it responds. Feel free to copy and paste it in your LLM application window.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#lab3-lab3-hands-on-guide-__codelineno-2-1"></a>You are a developer writing SQL queries given natural language questions. The database contains a set of 2 tables. The schema of each table with description of the attributes is given. Write the SQL query given a natural language statement with names being not case sensitive.
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#lab3-lab3-hands-on-guide-__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#lab3-lab3-hands-on-guide-__codelineno-2-3"></a> Here are the 2 tables :
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#lab3-lab3-hands-on-guide-__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#lab3-lab3-hands-on-guide-__codelineno-2-5"></a> (1) Database Table Name: USERS
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#lab3-lab3-hands-on-guide-__codelineno-2-6"></a> Table Schema:
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#lab3-lab3-hands-on-guide-__codelineno-2-7"></a> Column Name # Meaning
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#lab3-lab3-hands-on-guide-__codelineno-2-8"></a> user_id # unique identifier of the user
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#lab3-lab3-hands-on-guide-__codelineno-2-9"></a> user_name # name of the user
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#lab3-lab3-hands-on-guide-__codelineno-2-10"></a> usertypeid # user is '\''employee'\'', '\''customer'\''
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#lab3-lab3-hands-on-guide-__codelineno-2-11"></a> gender_id # user'\''s gender is 1 for female, 2 for male and 3 for other
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#lab3-lab3-hands-on-guide-__codelineno-2-12"></a> dob # date of birth of the user
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#lab3-lab3-hands-on-guide-__codelineno-2-13"></a> address # adress of the user
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#lab3-lab3-hands-on-guide-__codelineno-2-14"></a> state # state of the user
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#lab3-lab3-hands-on-guide-__codelineno-2-15"></a> country # country of residence of the user
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#lab3-lab3-hands-on-guide-__codelineno-2-16"></a>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#lab3-lab3-hands-on-guide-__codelineno-2-17"></a> (2) Database Table Name: ACCOUNTS
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#lab3-lab3-hands-on-guide-__codelineno-2-18"></a> Table Schema:
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#lab3-lab3-hands-on-guide-__codelineno-2-19"></a> Column Name # Meaning
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#lab3-lab3-hands-on-guide-__codelineno-2-20"></a> acc_id # account number or account id of the user
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#lab3-lab3-hands-on-guide-__codelineno-2-21"></a> user_id # user id of the user
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#lab3-lab3-hands-on-guide-__codelineno-2-22"></a> balance # available balance in the account
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#lab3-lab3-hands-on-guide-__codelineno-2-23"></a>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#lab3-lab3-hands-on-guide-__codelineno-2-24"></a> With the above schema, please generate sql query to list all users whose balance is &gt; 2000
</code></pre></div>
<p><br></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/445f9928-074f-4a58-a5b4-a1f757910c11" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/445f9928-074f-4a58-a5b4-a1f757910c11"></a></p>
<ul>
<li>The SQL query generated seems correct.</li>
<li>Its joining both the tables using <code>user_id</code> as the key and selecting all records where the user's account balance is &gt; 2000.</li>
<li>For the sake of people who may want to analyse further, pasting the SQL query that was generated:  <code>SELECT * FROM USERS u, ACCOUNTS a WHERE u.user_id = a.user_id AND a.balance &gt; 2000;</code></li>
</ul>
</li>
<li>
<p>Interestingly, code LLM works both ways! Given a SQL query, you can ask code LLM to explain what it does. To do that I have formed the query as below. Feel free to copy the query and post it in your LLM application window.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#lab3-lab3-hands-on-guide-__codelineno-3-1"></a>What does the below SQL query do ?
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#lab3-lab3-hands-on-guide-__codelineno-3-2"></a>SQL Query:
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#lab3-lab3-hands-on-guide-__codelineno-3-3"></a>SELECT * FROM USERS u, ACCOUNTS a WHERE u.userid = a.userid AND a.balance &gt; 2000;
</code></pre></div>
<p><br></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/8576ca30-42b8-4647-bd92-e092d0c67aa8" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/8576ca30-42b8-4647-bd92-e092d0c67aa8"></a></p>
<p>That's a decent explanation of the SQL query.</p>
</li>
<li>
<p>Let's try one more example. Here I give it 2 conditions to match in the query.<br>
   NOTE: You don't have to repeat the whole schema in the prompt. LLMs can remember context.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#lab3-lab3-hands-on-guide-__codelineno-4-1"></a>Using the schema given above, generate SQL query to list all users whose account balance &gt; 2000 and user is of type employee.
</code></pre></div>
<p><br></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/2be00e6a-8d79-4f1b-8844-b6768b63d86a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/2be00e6a-8d79-4f1b-8844-b6768b63d86a"></a></p>
<p>Response received as below:</p>
<p></p><div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#lab3-lab3-hands-on-guide-__codelineno-5-1"></a>Here's the SQL query to list all users whose account balance is greater than 2000 and they are of type employee from the USERS and ACCOUNTS tables:
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#lab3-lab3-hands-on-guide-__codelineno-5-2"></a>SELECT * FROM USERS u, ACCOUNTS a WHERE u.user_id = a.user_id AND a.balance &gt; 2000 AND usertypeid='employee';
</code></pre></div>
 <br>
<ul>
<li>The response is not 100% correct.</li>
<li>The last part of the SQL query <code>usertypeid='employee'</code> is ambiguous as the DB won't know which <code>usertypeid</code> column to reference.</li>
<li>The correct SQL query would have <code>u.usertypeid='employee'</code> so that the DB knows that its part of the USERS (aliased as <code>u</code> in the query) table.</li>
</ul>
<p><strong>NOTE</strong>: AI-generated content may vary and may not always provide consistent answers. Your results may vary.<br></p>
</li>
<li>
<p>Re-iterating some of the points we learned in this lab:</p>
<ul>
<li>
<p>Code LLMs are not 100% correct, yet they can be immensely helpful for a developer as they can help generate near perfect code which can then be analyzed and tweaked to perfection by the developer.</p>
</li>
<li>
<p>There are many code LLMs available in HF with varied degree of accuracy and clients can choose the right one by experimenting with them in the context of their specific use case.</p>
</li>
<li>
<p>IBM also offers enterprise grade code LLMs via its watsonx Code Assistant (WCA) family of product offerings.</p>
</li>
</ul>
</li>
<li>
<p>IBM Power servers are best used as system of record (SoR) servers which means they hold a lot of enterprise specific data in different DBs (for example: Oracle on AIX, DB2 on AIX / IBM i, PostgreSQL on Linux etc). From an IBM Power solution point of view, an end to end solution to query DB records using natural language can be easily implemented which can help clients immensely. They don't need to depend on experts to generate DB reports. Even executives (with authorized access to the DB) can generate reports and/or view data using natural language queries.</p>
<ul>
<li>Check this ~2min short <a href="https://mediacenter.ibm.com/media/Infusing+AI+into+mission+critical+workloads+with+PowerVS+and+watsonx.ai/1_fzqutamr" target="_blank">video demo</a> on "Infusing AI into mission critical workloads with PowerVS and watsonx.ai" which uses natural language to query fraudulent transactions and list high value customers. Although this demo is based on PowerVS use case the same is applicable to on-premises as well.</li>
</ul>
</li>
</ol>
<p><strong>Summary</strong></p>
<p>Code LLMs democratize coding by making it more accessible to non-coders, accelerating the development process for smaller teams, and assisting both beginners and experienced developers in learning and improving productivity. By lowering technical barriers, they are transforming who can engage in software development and how innovation happens across industries.</p></div></section><h1 class='nav-section-title-end'>Ended: Lab 3 - Deploy code LLM on Power10</h1><section class="print-page" id="resources"><div><h1 id="additional-learning-assets-optional-but-recommended">Additional learning assets (optional but recommended)<a class="headerlink" href="#resources-additional-learning-assets-optional-but-recommended" title="Permanent link">¶</a></h1>
<h2 id="resources-read-and-learn">Read and learn<a class="headerlink" href="#resources-read-and-learn" title="Permanent link">¶</a></h2>
<ul>
<li><a href="https://ibm.seismic.com/Link/Content/DC4HhmXXFPpFdGcTJJJb996JFR7V" target="_blank">AI on IBM Power - Seismic sales kit</a></li>
<li><a href="https://ibm.seismic.com/Link/Content/DCFbHqRHpq9d88cHj2dMWFffVP4P" target="_blank">AI on IBM Power - One Pager</a></li>
<li><a href="https://ibm.seismic.com/Link/Content/DC3CbdBWFpgTX8CDCFmg9JWmbMpV" target="_blank">AI on Power - Conversation Starters and Opportunity Qualification Questions</a></li>
<li><a href="https://www.ibm.com/docs/en/watsonx/saas?topic=models-choosing-model" target="_blank">Choosing a generative AI model</a></li>
<li>Delivering an on-premises generative AI Chatbot with IBM Power - <a href="https://www.ibm.com/downloads/cas/YE3OVQNB" target="_blank">white paper</a></li>
<li><a href="https://community.ibm.com/community/user/powerdeveloper/blogs/sebastian-lehrig/2024/03/26/sizing-for-ai" target="_blank">Tutorial - Sizing and configuring an LPAR for AI workloads</a></li>
</ul>
<h2 id="resources-watch-and-learn">Watch and learn<a class="headerlink" href="#resources-watch-and-learn" title="Permanent link">¶</a></h2>
<ul>
<li>TechZone demos<ul>
<li><a href="https://techzone.ibm.com/collection/ai-inferencing-on-ibm-power10-mma" target="_blank">AI Inferencing on IBM Power10 with MMA</a></li>
<li><a href="https://techzone.ibm.com/collection/generative-ai-demos-on-ibm-power" target="_blank">Generative AI demos on IBM Power</a></li>
</ul>
</li>
<li>Red Hat Developer webinar hosted on YouTube - <a href="https://www.youtube.com/watch?v=qx6MHt24TrY" target="_blank">Implementing enterprise-ready generative AI</a></li>
<li><a href="https://ec.yourlearning.ibm.com/w3/playback/10467290" target="_blank">Panel discussion on AI performance on Power - Sept 2024</a> - IBMers only</li>
</ul></div></section><section class="print-page" id="credits"><div><h1 id="credits">Credits<a class="headerlink" href="#credits-credits" title="Permanent link">¶</a></h1>
<p>The following individuals contributed at various stages of creating this course and I would like to extend my appreciation to them:</p>
<ul>
<li>
<p><strong>Marvin Gießing</strong>: Lab 1 and Lab 2 are inspired by Marvin's existing work <a href="https://github.com/mgiessing/bcn-lab-2084/" target="_blank">here</a>.</p>
</li>
<li>
<p><strong>Jean Midot</strong>: For the support provided on the TechZone front.</p>
</li>
<li>
<p><strong>Sebastian Lehrig</strong>, <strong>David Spurway</strong> and <strong>Daniel Casali</strong>: For all the technical guidance and support that is provided during the course.</p>
</li>
</ul></div></section><section class="print-page" id="support"><div><h1 id="support">Support<a class="headerlink" href="#support-support" title="Permanent link">¶</a></h1>
<p>Think something is down? Check the applicable status pages for any known issues like a site or service not available:</p>
<ul>
<li><a href="https://techzone.status.io/" target="_blank">IBM Technology Zone</a></li>
</ul>
<p>For issues with provisioning the ITZ environment for this lab (for example, a failed reservation request due to insufficient quota capacity) open a ticket with ITZ support:</p>
<ul>
<li>
<p>Web:  <a href="https://ibmsf.force.com/ibminternalproducts/s/createrecord/NewCase?language=en_US" target="_blank">IBM Technology Zone</a></p>
</li>
<li>
<p>Email: <a href="mailto:techzone.help@ibm.com" target="_blank">techzone.help@ibm.com</a></p>
</li>
</ul>
<p>For issues related to specific steps found in the demonstration guide after the ITZ environment is provisioned, contact the author:</p>
<ul>
<li>Email: deepakcshetty@in.ibm.com</li>
</ul></div></section></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 IBM
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "/", "features": ["navigation.instant", "navigation.tracking", "content.code.annotate", "content.code.copy"], "search": "../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
    
  </body>
</html>