{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the AI on Power - Level 3 course - seller enablement demonstration. This Level 3 course will provide hands-on enablement on how to use Generative AI (GenAI) models on Power10 and will cover few use-cases to help understand the art of the possible.</p> <p>There are 4 main parts to this demonstration as you can see on the left hand side navigation pane:</p> <ul> <li> <p>Lab Setup - How to provision and setup the lab env for the hands-on demos</p> </li> <li> <p>LAB 1 \u2013 Deploy a LLM on Power10 - Deploy a LLM on Power10 and switch to a diff LLM</p> </li> <li> <p>LAB 2 \u2013 Deploy RAG on Power10 - RAG chatbot for domain specific Q&amp;A</p> </li> <li> <p>LAB 3 \u2013 Query DB using NL on Power10 - Generate SQL queries for a given DB schema, using NL</p> </li> </ul>"},{"location":"#generative-ai-and-llms-overview","title":"Generative AI and LLMs overview","text":"<p>Large Language Models (LLMs) and Generative AI (GenAI) have revolutionized industries by enabling machines to understand, generate, and interact with human language in ways never seen before. They have disrupted the market by:</p> <ul> <li> <p>Automating Content Creation: From drafting articles, reports, and code to generating creative content like music and artwork, GenAI has drastically reduced time and costs in creative industries.</p> </li> <li> <p>Enhancing Customer Experience: LLM-powered chatbots and virtual assistants are offering more personalized, human-like interactions, reshaping customer service and support.</p> </li> <li> <p>Transforming Decision-Making: With advanced data processing and natural language understanding, businesses are leveraging AI for smarter, faster insights and predictive analytics.</p> </li> <li> <p>Boosting Productivity: LLMs streamline workflows by handling complex tasks, like writing, summarizing, and translating, allowing professionals to focus on more strategic, creative efforts.</p> </li> </ul> <p>In short, LLMs and GenAI are reshaping industries by enhancing efficiency, creativity, and customer engagement, making them indispensable tools in the modern business landscape.</p>"},{"location":"#why-ibm-power-for-genai","title":"Why IBM Power for GenAI","text":"<p>IBM Power platform is a high-performance, enterprise-grade computing solution designed to handle demanding workloads such as AI, analytics, cloud, and mission-critical applications. With unmatched processing power, scalability, and reliability, IBM Power enables organizations to optimize performance, lower costs, and ensure security.</p>"},{"location":"#key-benefits","title":"Key Benefits:","text":"<ul> <li> <p>Unmatched Performance: Optimized for data-intensive applications, AI, and large-scale analytics, delivering faster insights and better decision-making.</p> </li> <li> <p>Scalability: Seamlessly scale workloads across on-prem, cloud, and hybrid environments to meet evolving business needs.</p> </li> <li> <p>Enterprise-Grade Security: Built-in security features provide robust protection for sensitive data and critical operations.</p> </li> <li> <p>Efficiency and Flexibility: Designed for diverse environments, supporting Linux, AIX, IBM i, and containerized applications, making it versatile for a range of industries.</p> </li> </ul> <p>IBM Power is the platform of choice for businesses seeking to future-proof their IT infrastructure, handle complex workloads, and accelerate innovation</p>"},{"location":"#ibm-power10-differentiation","title":"IBM Power10 differentiation","text":"<p>IBM Power10 offers several advantages for Generative AI (GenAI) use cases, providing the computational power, security, and efficiency required to support the demanding workloads of AI-driven applications. Here\u2019s how Power10 is uniquely positioned to accelerate GenAI:</p>"},{"location":"#key-advantages","title":"Key Advantages:","text":"<ol> <li> <p>High Performance for AI Workloads:</p> <ul> <li> <p>AI Acceleration: IBM Power10 includes AI inference acceleration directly on the chip, allowing faster processing of GenAI tasks such as model training, inference, and real-time AI applications.</p> </li> <li> <p>Massive Scalability: Power10 is designed to handle large-scale AI models, with enhanced memory bandwidth and processing power to efficiently manage vast datasets used in training LLMs and GenAI systems.</p> </li> </ul> </li> <li> <p>Memory and Bandwidth Optimization:</p> <ul> <li>The DDR5 memory and faster I/O bandwidth provide a significant performance boost, enabling GenAI models to run more efficiently.</li> </ul> </li> <li> <p>Energy Efficiency:</p> <ul> <li>Power10 processors are highly energy-efficient, offering more performance per watt compared to previous generations. This reduces operational costs and is especially beneficial for the resource-intensive training and deployment of GenAI models.</li> </ul> </li> <li> <p>Security:</p> <ul> <li> <p>IBM Power10 comes with end-to-end encryption and advanced memory protection features, ensuring that sensitive AI workloads are secure from potential breaches. This is particularly important in industries like healthcare and finance, where GenAI applications need strong data protection.</p> </li> <li> <p>Quantum-safe encryption helps future-proof AI workloads against quantum computing threats.</p> </li> </ul> </li> <li> <p>Flexible Cloud and Hybrid Deployments:</p> <ul> <li>Power10 integrates smoothly with hybrid cloud environments, allowing enterprises to run GenAI workloads both on-premises and in the cloud. This flexibility is key for scaling AI applications while maintaining control over data and infrastructure.</li> </ul> </li> <li> <p>Support for AI Ecosystems:</p> <ul> <li>IBM Power10 works seamlessly with AI frameworks like TensorFlow and PyTorch, making it easier for developers to build, train, and deploy GenAI models without needing major infrastructure changes.</li> <li>Support for OpenShift and Kubernetes enhances containerization, allowing efficient orchestration of AI workloads.</li> </ul> </li> </ol>"},{"location":"#summary","title":"Summary:","text":"<p>IBM Power10 is designed to meet the specific needs of GenAI use cases by offering high-performance processing, memory scalability, security, and energy efficiency. It empowers enterprises to handle complex AI workloads efficiently while maintaining flexibility and security, making it ideal for industries that rely on AI-driven innovation.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#credits","title":"Credits","text":"<p>Sincere and heartfelt thanks to the below people who helped in various stages of creating this course:</p> <ul> <li> <p>Marvin Gie\u00dfing: Lab1 and Lab2 are inspired by Marvin's existing work here.</p> </li> <li> <p>Jean Midot: For the support he provided on the TechZone front.</p> </li> <li> <p>Sebastian Lehrig, David Spurway &amp; DANIEL DE SOUZA CASALI: For all the technical guidance and support provided during the course.</p> </li> </ul>"},{"location":"lab-setup/","title":"Lab setup instructions","text":""},{"location":"lab-setup/#lab-setup","title":"Lab Setup","text":""},{"location":"lab-setup/#provisioning-the-environment","title":"Provisioning the environment","text":"<p>For the hands-on labs, we will be using the OpenShift on Power10 on-prem environment, optimized for AI workloads and hosted on IBM TechZone.</p> <p>Follow the steps below:</p> <ol> <li> <p>Head to this TechZone collection and provision the environment named \"OpenShift 4.14 ready for AI on IBM Power10 (Container PaaS)\" by clicking on Reserve and submitting the resulting form (select Education as purpose)    </p> </li> <li> <p>Watch your email for updates from TechZone and wait for your environment to be provisioned.</p> </li> <li>Once provisioned, head to my reservations page to ensure its ready.    </li> </ol>"},{"location":"lab-setup/#accessing-the-environment","title":"Accessing the environment","text":"<ol> <li>Since this is an on-prem environment, make sure you are connected to the IBM network to access the environment.(TODO: add details on VPN)</li> <li>In TechZone, click on your provisioned environment under \"My Reservations\" which will open up the details page.</li> <li>Scroll to the \"Reservation Details\" section of the page which has information on how to connect to OpenShift console    </li> </ol>"},{"location":"lab-setup/#accessing-openshift-guiconsole","title":"Accessing OpenShift GUI/console","text":"<ol> <li>In the \"Reservation Details\" section of the TechZone environment details page, click on the OpenShift console link</li> <li>This will open up a new browser tab/window and opens up the OpenShift console login page   <ul> <li>If you encounter any security exception, navigate to the bottom of the browser page, acccept the exception under Advanced and continue. This is ok as we are in a lab/demo environment and using self-signed certificates.</li> </ul> </li> <li>On the OpenShift console page, select the htpasswd login option.</li> <li> <p>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code></p> <ul> <li>TIP: Click on the copy icon provided under 'User Password' in the Reservation Details page to copy the password and paste it in the OpenShift console window</li> </ul> <p> </p> </li> <li> <p>You have successfully logged into the OpenShift cluster using the GUI/console. You should be able to see the dashboard (or the page you were on before logging off) of your OpenShift console. You should land in Administrator profile (or Developer profile if that was the last profile you were in when you logged off).</p> </li> <li> <p>TIP: Its a good idea to have 2 browser windows (or tabs per your preference) for OpenShift console access - one with Administrator profile and another with Developer profile because in the hands-on labs we will be needing to switch between these profiles and its easier and efficient to do so with 2 browser windows.</p> <ul> <li> <p>To do so, copy the URL from the browser address bar, open a new browser window (or tab) and paste the URL there. It should open up one more OpenShift console in the new window (or tab). In the new window/tab, switch to the Developer profile (aka Persona) by going to the top left corner and clicking on Administrator and selecting Developer (or vice-versa) in the drop down menu. In short, ensure you have 2 browser windows, one each with Administrator and Developer profile (aka Persona) and we will call this OpenShift Administrator console and Developer console respectively.</p> <p> </p> <p>!!! note \"RE-AUTHENTICATING in case you lose GUI/console access\"</p> <pre><code> In case you lose access to the OpenShift cluster and need to re-login to the GUI/console, which is possible in case your reservation expires and/or your GUI/console authentication timed-out, please follow the above steps again to re-login to your OpenShift GUI/console\n</code></pre> </li> </ul> </li> </ol>"},{"location":"lab-setup/#accessing-openshift-oc-cli","title":"Accessing OpenShift <code>oc</code> CLI","text":"<ol> <li>Go back to the \"Reservation Details\" section of the TechZone environment details page.</li> <li>Open your terminal window and use SSH utility to connect to the Bastion node of OpenShift cluster.<ul> <li><code>ssh -l cecuser &lt;your bastion hostname/IP as provided in Reservation Details section&gt;</code></li> <li>If <code>ssh</code> gives any warning, type <code>yes</code> and continue</li> <li>When prompted for password, copy the password from Reservation Details page by clicking on the copy icon and pasting it in the ssh terminal window</li> </ul> </li> <li>You have logged in successfully to the bastion node of your OpenShift cluster.<ul> <li>Keep this terminal window always open as you will be using it frequently to run CLI commands.</li> </ul> </li> <li> <p><code>oc</code> CLI is pre-installed on the bastion node and must be working already. You can check by running <code>oc version</code> command.</p> <ul> <li>Ignore the error part of the <code>oc version</code> for now. Its as expected since you have not yet logged into the cluster from the CLI.</li> </ul> <p> </p> </li> </ol>"},{"location":"lab-setup/#logging-in-to-openshift-cluster-using-oc-cli","title":"Logging in to OpenShift Cluster using <code>oc</code> CLI","text":"<p>Let's login to the OpenShift cluster via the <code>oc</code> CLI. This is needed as we will execute some CLI commands as part of the lab steps.</p> <ol> <li> <p>In the OpenShift console, top right section click on <code>cecuser</code> and select <code>Copy login command</code> option.</p> <p></p> </li> <li> <p>A new browser window (or tab, depending on your browser setting) opens up.</p> <ul> <li>If you encounter any security exception, navigate to the bottom of the browser page, acccept the exception under Advanced and continue. This is ok as we are in a lab/demo environment and using self-signed certificates.</li> </ul> </li> <li>You will be presented with another login screen. Click htpasswd option    </li> <li>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code><ul> <li>TIP: Click on the copy icon provided under 'User Password' in the Reservation Details page to copy the password and paste it in the OpenShift console window.</li> </ul> </li> <li> <p>Click Display token</p> <p></p> </li> <li> <p>Copy the <code>oc login --token=...</code> CLI and paste it in the bastion node terminal window.       </p> </li> <li>You have successfully logged into the OpenShift cluster using the <code>oc</code> CLI.</li> <li> <p>In case you lose access to the <code>oc</code> CLI, you will get an error as below, in which case you need to re-authenticate.    Refer to the call-out on how to re-authenticate.</p> <p></p> <p>!!! note \"RE-AUTHENTICATING in case you lose CLI access\"</p> <pre><code> In case you lose access to the OpenShift cluster and need to re-authenticate using the CLI, which is possible in case your reservation expires and/or your CLI window terminated for some reason, please follow the above steps again to get back your `oc` CLI authenticated to the OpenShift cluster\n</code></pre> </li> </ol>"},{"location":"lab-setup/#summary","title":"Summary","text":"<p>Efforts are made to keep the lab instructions simple and easy to follow to cater to audience of all skill levels. We strive to use OpenShift GUI/console as much possible, but in some scenarios OpenShift console doesn't yet support some functionality in which case we switch to the <code>oc</code> CLI. Hence this lab will use OpenShift GUI/console and <code>oc</code> CLI both as necessary.</p>"},{"location":"pre-req/","title":"Pre-requisites","text":""},{"location":"pre-req/#pre-requisites-for-the-course","title":"Pre-requisites for the course","text":"<p>The following are the pre-requisites for this course:</p> <ul> <li> <p>AI on Power - Level 2 course - IBM | BP</p> </li> <li> <p>Basic understanding of Generative AI (GenAI) and Large Language Models (LLM)</p> <ul> <li>If not, feel free to learn more by doing Google search :) </li> </ul> </li> <li> <p>Basic understanding of OpenShift and working with yaml files. </p> <ul> <li>In any case, appropriate instructions are provided through-out the course</li> </ul> </li> <li> <p>Laptop with ssh pre-installed to connect to lab environment.</p> <ul> <li>Windows: PowerShell should have ssh pre-installed.<ul> <li>Mac/Linux: ssh should be already available</li> <li>If not, Google is your friend :) </li> </ul> </li> </ul> </li> </ul>"},{"location":"Lab1/lab1-hands-on-guide/","title":"Hands-on guide","text":""},{"location":"Lab1/lab1-hands-on-guide/#deploy-a-llm-on-power10-hands-on-lab-guide","title":"Deploy a LLM on Power10 - Hands-on lab guide","text":""},{"location":"Lab1/lab1-hands-on-guide/#lab-ready-check","title":"Lab-ready check","text":"<p>Make sure you have the following items ready:</p> <ul> <li>In your browser, you have logged in as <code>cecuser</code> in the OpenShift GUI/console.</li> <li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI <code>oc</code>.</li> </ul> <p>Warning</p> <p>Do not proceed further unless the items mentioned above are ready. Please refer to \"Lab setup instructions\" section (see left hand side menu) to setup your browser and terminal windows.</p>"},{"location":"Lab1/lab1-hands-on-guide/#lab-guide","title":"Lab guide","text":"<p>Image zoom functionality</p> <p>Feel free to click on the images in the lab guide below to a view larger image</p>"},{"location":"Lab1/lab1-hands-on-guide/#create-project","title":"Create project","text":"<ol> <li>Go to OpenShift Administrator profile, click on Home -&gt; Projects and click Create Project </li> <li>Enter a project name: lab1-demo &amp; click Create </li> </ol>"},{"location":"Lab1/lab1-hands-on-guide/#setup-storage","title":"Setup storage","text":"<p>Let's setup storage for this lab which is needed for storing the downloaded AI models. This environment comes with NFS (file storage) pre-configured.  In OpenShift, you first request for the storage (aka PersistentVolumeClaim or PVC) and the actual storage (aka PersistentVolume or PV) gets alloted based on your request and the storage availability in the storage pool (NFS in this case).</p> <ol> <li>Go to OpenShift Administrator profile, click on Storage -&gt; PersistentVolumeClaims and click Create PersistentVolumeClaim </li> <li>In the resulting form, enter PVC name: model-storage and Size: 20 GB. Leave other fields as defaults and click Create </li> <li>Note that it shows PVC status as bound, which means storage was allotted    </li> <li> <p>Navigate to Storage -&gt; PersistentVolumes and view the actual storage (PV) bound to your storage request (PVC = model-storage)    </p> <p>This completes the storage setup.</p> </li> </ol>"},{"location":"Lab1/lab1-hands-on-guide/#setup-configmap","title":"Setup ConfigMap","text":"<p>In OpenShift, a ConfigMap is an object used to manage configuration data for applications. It allows you to decouple configuration details from application logic, which makes your applications more portable and easier to manage across different environments. Instead of hardcoding configuration values in your application, you store them in a ConfigMap and inject them into your application at runtime.</p> <p>We will use ConfigMap to store the model URL and model name, both of which will be used when we deploy the model. Using ConfigMap will help us switch to a different model very easily.</p> <ol> <li>Navigate to Workloads -&gt; ConfigMaps and click Create ConfigMap </li> <li>In the resulting form, enter a name: model-params, and fill the Key and Value fields as below:    - Key: MODEL_NAME    - Value: tinyllama-1.1b-chat-v1.0.Q8_0.gguf</li> </ol> <p>Click Add key/value which will open up one more Key/Value box.</p> <p> 4. In the newly created Key/Value box, enter: Key: MODEL_URL and Value: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf and click Create </p> <pre><code>This completes the ConfigMap setup.\n</code></pre>"},{"location":"Lab1/lab1-hands-on-guide/#deploy-first-model","title":"Deploy first model","text":"<ol> <li> <p>Navigate to the OpenShift Developer profile console window.</p> <ul> <li>Note: If you followed the TIP given in the \"Lab setup instructions\" page, you should already have a browser window/tab with the Developer profile. In case you didn't, go to top left corner of your console, click Administrator and select Developer.</li> </ul> </li> <li> <p>IMP: Ensure you are in the lab1-demo project in the Developer profile window. If not, goto Project and select lab1-demo.</p> <p></p> </li> <li> <p>Click on +Add and select Import YAML option.    </p> </li> <li> <p>In the resulting window, copy and paste the below deployment yaml into it and click Create </p><pre><code> apiVersion: apps/v1\n kind: Deployment\n metadata:\n   name: lab1-demo\n spec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: lab1-demo\n   template:\n     metadata:\n       labels:\n         app: lab1-demo\n     spec:\n       initContainers:\n         - name: fetch-model-data\n           image: ubi8\n           volumeMounts:\n             - name: llama-models\n               mountPath: /models\n           command:\n             - sh\n             - '-c'\n             - |\n               if [ ! -f /models/$MODEL_NAME ] ; then\n                 curl -L $MODEL_URL --output /models/$MODEL_NAME\n               else\n                 echo \"model /models/$MODEL_NAME already present\"\n               fi\n               rm -f /models/mymodel\n               ln -sf /models/$MODEL_NAME /models/mymodel\n           resources: {}\n       containers:\n         - name: llama-cpp\n           image: quay.io/daniel_casali/llama.cpp-mma:sep2024\n           args: [\"-m\", \"/models/mymodel\", \"-c\", \"4096\", \"--host\", \"0.0.0.0\"]\n           ports:\n             - containerPort: 8080\n               name: http\n           volumeMounts:\n             - name: llama-models\n               mountPath: /models\n       volumes:\n         - name: llama-models\n           persistentVolumeClaim:\n             claimName: model-storage\n</code></pre> </li> <li> <p>You should land in the Deployment details window. Click on Pods tab and you should see the Pod erroring out. This is expected as the yaml references MODEL_URL and MODEL_NAME environment variables which we haven't supplied yet! Remember we do have those in ConfigMap, so we use inject that in the next step.    </p> </li> <li> <p>Navigate to Environment tab, select fetch-model-data container and select model-params ConfigMap and click Save </p> <p>About LLama and tinyLLama models</p> <p>The ConfigMap currently points to the tinyLLaMa model. The LLaMA (Large Language Model Meta AI) model is a family of state-of-the-art large language models developed by Meta (formerly Facebook), specifically designed to perform various natural language processing tasks. TinyLLaMA is a compact variant of the LLaMA (Large Language Model Meta AI) model, designed for efficiency and accessibility, especially when deployed in smaller environments. Available via Hugging Face (HF), it focuses on reducing the size of large-scale language models while retaining strong performance across various natural language processing tasks.</p> </li> <li> <p>Switch back to Pods tab and you should see that a new pod has been launched by OpenShift as we changed the pod's environment, when we added ConfigMap.     </p> </li> <li> <p>The new pod will download the model and then start it. Since the configmap points to tinyllama model, it will be downloaded from HuggingFace and then started. When that happens the pod's status will change to Running.</p> <p>Model download will take time - Have patience!!</p> <p>This process will take a few minutes (in my case it took around 1-1.5 mims) and your mileage may vary! Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV)</p> <p></p> <p>Congratulations!, you have successfully deployed a LLM on Power10.</p> </li> <li> <p>Let's verify that the model running is tinyllama!. Click on the pod to enter the pod details view/page.     </p> </li> <li>In the pod details page, click on the Logs tab to see the pod logs     </li> <li> <p>In the log window, scroll upwards to see the name of the model against the attribute llm_load_print_meta: general.name </p> <p>This verifies that we have indeed deployed tinyllama.</p> </li> <li> <p>Let's access our model and interact with it. In OpenShift, you need to create a service and a route which generates the cluster internal and publicly accessible HTTP endpoints, respectively. To do that, navigate to the OpenShift Administrator profile, click Networking -&gt; Services and click Create Service</p> <p>If you are switching browser window/tab, make sure you are in the lab1-demo project in the new window/tab.</p> <p></p> </li> <li> <p>In the resulting Create Service yaml window, select all &amp; delete everything. Then copy the below service yaml, paste it in the yaml window and click Create </p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: \"lab1-service\"\n  labels:\n    app: \"lab1-service\"\nspec:\n  type: \"ClusterIP\"\n  ports:\n    - name: lab1-port\n      port: 8080\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: \"lab1-demo\"\n</code></pre> </li> <li> <p>You should land in service details view/page. You can see the ClusterIP which is accessible from inside the cluster only.     </p> </li> <li> <p>Navigate to Networking -&gt; Routes and click Create Route </p> </li> <li> <p>In the resulting Create Route window, select YAML view and clear everything from the yaml window. Copy the below yaml and paste it in the yaml window and click Create </p><pre><code>kind: Route\napiVersion: route.openshift.io/v1\nmetadata:\n  name: lab1-route\n  labels:\n    app: lab1-route\nspec:\n  to:\n    kind: Service\n    name: lab1-service\n  tls: null\n  port:\n    targetPort: lab1-port\n</code></pre> </li> <li> <p>You should land in the route details view. The URL mentioned under Location is the externally accessible URL of your application (which hosts the tinyllama LLM).     </p> </li> <li> <p>Click on the external URL in the route details view to access your model. A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:      </p> <p></p> </li> <li> <p>Scroll all the way down to the input field \"Say something...\" where you can interact with the LLM. You can ask any question you like, but keep in mind you're using a small model and there are more powerful models out there for general conversation.     </p> <p>Experimenting with model parameters</p> <p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Feel free to google and learn about them and experiment with it. You may want change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab</p> </li> <li> <p>Here are some questions I asked and the responses I got.</p> <p>Accuracy of LLM responses</p> <p>Large Language Models (LLMs), are trained on vast amounts of text data, but that training is limited to a specific cutoff date. This means that the model can only answer questions based on the information available up to that point in time. It cannot access real-time data or understand events, trends, or new information that occurred after the cutoff date. Consequently, their ability to provide accurate answers is constrained by the knowledge they were trained on.</p> <p></p> <p></p> <p>Congratulations, you were able to deploy a LLM, create a public endpoint to access it and take it for a run! In the next step, let's learn how to deploy a different LLM.</p> </li> </ol>"},{"location":"Lab1/lab1-hands-on-guide/#deploy-second-model","title":"Deploy second model","text":"<p>In this section, let's deploy IBM's granite model.</p> <p>About IBM's granite LLM</p> <p>The IBM Granite model family is designed as enterprise-grade AI models tailored for business applications. Granite models are available both as open-source models on platforms like Hugging Face and through IBM's watsonx.ai for more enterprise-specific needs.</p> <ol> <li>Navigate to your OpenShift developer profile window. Click ConfigMaps, then click model-params. This should open up the ConfigMap details page.    </li> <li>In the model-params ConfigMap details page, click Action and select Edit ConfigMap </li> <li> <p>In the resulting form, edit the key/value fields for MODEL_NAME and MODEL_URL as below and click Save</p> <ul> <li>Key: MODEL_NAME</li> <li>Value: granite-7b-lab.Q4_K_M.gguf</li> </ul> <ul> <li>Key: MODEL_URL</li> <li>Value: https://huggingface.co/RichardErkhov/instructlab_-_granite-7b-lab-gguf/resolve/main/granite-7b-lab.Q4_K_M.gguf</li> </ul> <p> 4. The existing pod won't see the changes right away as changing values of a ConfigMap doesn't cause a deployment (and hence pod) to restart. It needs to be done manually.</p> </li> <li> <p>Let's go to the deployment view. Click Topology, then click \"D lab1-demo\" part of the application icon, which will open up the deployment details pane (on the right hand side of the browser window). Click D lab1-demo in that pane which will then open up the deployment details view for lab1-demo deployment.</p> </li> </ol> <p></p> <ol> <li>Click Actions and select Restart rollout. This will restart the deployment which results in redeployment of the pod.</li> </ol> <p></p> <ol> <li> <p>Click on Pods tab, where you will see a new pod instantiated. The new pod will download the model and then start it. Since the configmap points to granite model, it will be downloaded from HuggingFace and then started. When that happens the new pod's status will change to Running and the existing pod will be terminated.</p> <p></p> <p>Model download will take time - Have patience!!</p> <p>This process will take a few minutes (in my case it took around 3-4 mins as this is a fairly large model compared to tinyllama) and your mileage may vary! Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV).</p> </li> <li> <p>If all goes well, you should see just 1 pod running.</p> <p></p> </li> <li> <p>Let's verify that the model running is granite!. Click on the pod to enter the pod details view/page.</p> <p></p> </li> <li> <p>In the pod details page, click on the Logs tab to see the pod logs.</p> <p></p> </li> <li> <p>In the log window, scroll upwards to see the name of the model against the attribute llm_load_print_meta: general.name.</p> <p></p> <p>This verifies that we have indeed deployed granite.   </p> </li> <li> <p>Let's access the model now!. As we did in the previous section of this lab, we need to find the external public endpoint. The beauty of OpenShift is that the endpoint remains same inspite of the pod being restarted. So either you can refresh the earlier page (if you have it opened in the browser) or follow the steps below to access the public URL of your application via the Topology view.</p> <p>Multiple ways to access public URL of your application</p> <p>If you are in Administrator profile, you can navigate to Networking -&gt; Routes to access your route resource and click on the URL to open your application, as we did in the previous section of this lab where we deployed our first model. Alternatively, if you are in Developer profile, you can go to Topology view and access the URL of your application as well. Let's use that method here...</p> <ul> <li>In Developer profile window, click Topology and you should see the icon representing your deployed application.   </li> <li>Click on the arrow (in top right corner of the icon) that says \"Open URL\"   </li> <li>A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:   </li> <li>Scroll all the way down to the input field \"Say something...\" where you can interact with the LLM. You can ask any question you like!   </li> </ul> <p>Experimenting with model parameters</p> <p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Feel free to google and learn about them and experiment with it. You may want change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab</p> </li> <li> <p>Here are some questions I asked and the responses I got.</p> <p>Accuracy of LLM responses</p> <p>Large Language Models (LLMs), are trained on vast amounts of text data, but that training is limited to a specific cutoff date. This means that the model can only answer questions based on the information available up to that point in time. It cannot access real-time data or understand events, trends, or new information that occurred after the cutoff date. Consequently, their ability to provide accurate answers is constrained by the knowledge they were trained on.</p> <p></p> <p></p> <p>Congratulations, in this section you learned how easy it is to switch to a different LLM, access it and take it for a run! This completes the lab.</p> </li> </ol>"},{"location":"Lab1/lab1-overview/","title":"Lab Overview","text":""},{"location":"Lab1/lab1-overview/#deploy-a-large-language-model-llm-on-ibm-power10-lab-education","title":"Deploy a Large Language Model (LLM) on IBM Power10 - Lab Education","text":"<p>Goal of this lab is to showcase the basics (and the ease) of deploying a LLM (aka foundation models) on Power and how easy it is to switch to a different LLM. But before we do that, let's understand the ecosystem around LLMs (aka foundation models or Generative AI) in the context of IBM Power.</p> <p>In this lab, we will deploy 2 LLMs from Hugging Face (HF) on IBM Power10 using the on-prem lab environment that has been provisioned.</p>"},{"location":"Lab1/lab1-overview/#what-is-hugging-face","title":"What is Hugging Face?","text":"<p>Hugging Face is an AI company that built an incredibly popular AI community around open source libraries, models, and data sets. It's an open source community with a large collection of AI models and datasets that are available for sharing and collaboration. The Hugging Face hub now hosts more than 950K models and over 210K data sets, and those numbers are growing quickly.</p> <p>What is Hugging Face?</p> <p>Hugging Face is an AI company that has become a major hub for Natural Language Processing (NLP) and Machine Learning (ML) tools. It is widely known for its open-source library, Transformers, and its collaborative platform, which provides a vast collection of pre-trained models and datasets. Hugging Face has made it easy for developers, researchers, and businesses to access, fine-tune, and deploy state-of-the-art machine learning models, especially those related to NLP tasks like text classification, translation, summarization, and more.</p> <p>We use HF to access LLMs in this lab as it has a vast collection of pre-trained models which are hosted publicly and can be accessed for free.</p>"},{"location":"Lab1/lab1-overview/#what-is-ibm-watsonx","title":"What is IBM watsonx?","text":"<p>Enterprise clients of IBM will use the IBM watsonx platform, an enterprise-ready AI and data platform. Watsonx is designed for the enterprise and is targeted for business domains, empowering value creators to transform business applications into AI-first applications.</p> <p>What is IBM watsonx?</p> <p>IBM Watsonx is IBM\u2019s next-generation AI and data platform, designed to help enterprises build, train, fine-tune, and deploy large-scale AI models efficiently. Watsonx enables businesses to harness the power of artificial intelligence and machine learning (AI/ML) for various tasks like generative AI, predictive analytics, and decision-making. It integrates advanced capabilities for handling large language models (LLMs), machine learning workflows, and AI governance.</p> <p>IBM watsonx support on IBM Power</p> <p>NOTE: At the time of writing this lab, IBM watsonx is not available to run on IBM Power yet. However, clients can use foundation models (either hosted by watsonx on IBM Cloud or running stand-alone on-premise on IBM Power) to infuse and harness the power of (Gen)AI into their on-prem applications running on IBM Power.</p>"},{"location":"Lab1/lab1-overview/#ibm-watsonx-vs-hugging-face","title":"IBM watsonx Vs Hugging Face","text":"<p>IBM Watsonx and Hugging Face both offer AI tools, but they serve different purposes:</p> <ul> <li> <p>IBM watsonx: An enterprise-grade AI platform designed for large-scale AI model training, fine-tuning, and deployment. It focuses on governance, compliance, and custom AI models for business use, offering robust tools for data management and AI governance. Ideal for enterprises needing secure, scalable AI with industry-specific solutions.</p> </li> <li> <p>Hugging Face: A community-driven platform offering pre-trained models (especially in NLP) and datasets for rapid experimentation and development. It's known for its open-source library and easy access to state-of-the-art models, making it great for research, startups, and developers.</p> </li> </ul> <p>In essence, IBM watsonx focuses on enterprise-grade AI with strong governance and scalability, while Hugging Face is designed for flexible, community-driven AI development with a focus on rapid prototyping and open access to pre-trained models.</p>"},{"location":"Lab1/lab1-overview/#ibm-watsonx-hugging-face","title":"IBM watsonx &amp; Hugging Face","text":"<p>With IBM watsonx, clients can run not just IBM-trained foundation models, but also open source models and models from Hugging Face as well!</p> <p>IBM watsonx and Hugging Face can work together by combining IBM's enterprise AI capabilities with Hugging Face's vast collection of pre-trained models and tools for rapid AI development:</p> <ul> <li>Model Access: IBM watsonx users can leverage Hugging Face's pre-trained models (like GPT, BERT, etc.) from its model hub to fine-tune or deploy in enterprise environments using Watsonx\u2019s scalable infrastructure.</li> <li>Fine-Tuning and Customization: Businesses can use Hugging Face models in Watsonx to fine-tune them with proprietary data while benefiting from watsonx\u2019s AI governance and compliance tools.</li> <li>Deployment: Hugging Face models can be integrated into IBM watsonx to deploy at scale on hybrid cloud or on-premise environments, ensuring enterprise-level security and performance.</li> </ul> <p>Together, Hugging Face provides the models, and watsonx offers the enterprise-ready infrastructure for secure, large-scale, compliant deployment. Read this blog to understand more about how IBM and Hugging Face are working to bring open source communities together for enterprise AI.</p>"},{"location":"Lab1/lab1-overview/#ai-and-watsonx-with-ibm-power","title":"AI and watsonx with IBM Power","text":"<p>Here is a quick 1-slider on what you can do with AI and watsonx on IBM Power, today. </p> <p>Clients can get started with AI and watsonx with IBM Power today.  IBM has made it simple by aligning the common use cases around 4 key areas that we see within our pilots and client activations.</p> <ul> <li>Pattern 1: Securely tune, deploy and manage foundation models. When it comes to task-specific use cases, it is a good idea to work with large open-source models in your own workspace that are under your control. Hugging Face is a large repository with over 950K pre-trained ML models and a platform where the AI ecosystem collaborates on models, datasets and applications. Download any model from Hugging Face and securely deploy at scale on IBM Power. Then, use best-of-breed software to help you tune, deploy and manage as many models as you need. Some examples of what enterprises can use this capability for: Customer service, knowledge workers to augment staff and fraud reporting. \u00a0</li> <li>Pattern 2: There are many new and existing business apps that are using foundation models integrated into the workflows. Deploy your foundation models anywhere, on Power, x86, cloud, and use the watsonx.ai software development kit (SDK) available in Python and embed directly into applications. On Power, enterprises can do this quickly so that services can be released to customers faster on a resilient 24/7 environment. Some examples of clients can embed AI into apps are generating the first draft of reports, citizen services for government end-clients and knowledge management. \u00a0</li> <li>Pattern 3: The ecosystem is important to enterprises that have long-standing investments in software that drives their core business workflows. Consume watsonx services from customers customized ecosystem apps. Enterprises can generate code for Ansible playbooks for IBM i or AIX to enhance the Ansible IT management experience. Additionally, SAP applications can be embedded with watsonx services within the SAP ABAP environments. These custom apps help clients deliver services much faster while taking advantage of existing and familiar investments, making this an attractive proposition for many. Some example use cases include asset management, code generation, accounting automation.  \u00a0</li> <li>Pattern 4: Lastly, we are bringing a full suite of AI capabilities to the Power platform to help clients train, tune and deploy models without purchasing GPUs. The lead time for GPUs is somewhere around a year and clients will miss out on opportunity if they can\u2019t get started today. Some of the popular use cases that enterprises will use are fraud detection, risk underwriting, and demand forecasting.</li> </ul>"},{"location":"Lab1/lab1-overview/#choosing-a-foundation-model","title":"Choosing a foundation model","text":"<p>There are many factors to consider when you choose a foundation model to use for inferencing from a generative AI project. Determine which factors are most important for you and your organization.</p> <ul> <li>Tasks the model can do</li> <li>Languages supported</li> <li>Tuning options for customizing the model</li> <li>License and IP indemnity terms</li> <li>Model attributes, such as size, architecture, and context window length</li> </ul> <p>After you have a short list of models that best fit your needs, you can test the models to see which ones consistently return the results you want.</p> <p>For more details on choosing a foundation model that support your use case / language / other factors, refer to this document. Although this document is part of the watsonx.ai product documentation, the information provided includes both IBM-trained models and open-source models as watsonx.ai support both types of models.</p>"},{"location":"Lab2/lab2-hands-on-guide/","title":"Hands-on guide","text":""},{"location":"Lab2/lab2-hands-on-guide/#deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-guide","title":"Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - lab guide","text":""},{"location":"Lab2/lab2-hands-on-guide/#lab-ready-check","title":"Lab-ready check","text":"<p>Make sure you have the following items ready:</p> <ul> <li>In your browser, you have logged in as cecuser in the OpenShift GUI/console.</li> <li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI oc.</li> </ul> <p>Warning</p> <p>Do not proceed further unless the items mentioned above are ready. Please refer to \"Lab setup instructions\" section (see left hand side menu) to setup your browser and terminal windows.</p>"},{"location":"Lab2/lab2-hands-on-guide/#lab-guide","title":"Lab guide","text":"<p>Image zoom functionality</p> <p>Feel free to click on the images in the lab guide below to a view larger image</p> <p>In this lab, we will focus on the below:</p> <ul> <li> <p>Deploy a vector DB - milvus</p> </li> <li> <p>Deploy a jupyter notebook where we will implement RAG pattern and learn about:</p> <ul> <li>Index the milvus DB with a sample PDF</li> <li>Query the DB to get relevant documents for the question asked.</li> <li>Create a prompt based on the relevant documents gotten from the previous step and send it to the LLM (granite model we deployed in Lab1) to get a domain specific answer.</li> </ul> </li> </ul>"},{"location":"Lab2/lab2-hands-on-guide/#create-project","title":"Create project","text":"<ol> <li> <p>Let's create a new OpenShift project that will hold all resources of this lan. Navigate to your browser window/tab which has the Administrator profile. Select Home -&gt; Projects and click Create Project </p> </li> <li> <p>In the resulting form, enter lab2-demo as the project name and click Create </p> </li> </ol>"},{"location":"Lab2/lab2-hands-on-guide/#deploy-milvus","title":"Deploy milvus","text":"<ol> <li> <p>Navigate to the terminal window where we had setup <code>oc</code> CLI on the bastion node.</p> <p>Ensure <code>oc</code> CLI is authenticated with the cluster</p> <p>Run a simple command: <code>oc project</code> and if it throws error you need re-authenticate with the cluster. Follow the steps mentioned in lab instructions to ensure <code>oc</code> is authenticated with the cluster.</p> </li> <li> <p>Make sure you are in the home directory and then run the command below to clone the github repository. Then switch to the newly cloned repository directory.</p> <ul> <li><code>cd</code></li> <li><code>git clone https://github.com/dpkshetty/bcn-lab-2084</code></li> <li><code>cd bcn-lab-2084/</code></li> </ul> <p></p> <p></p> <p></p> </li> <li> <p>Make sure `oc CLI is pointing to the lab2-demo project</p> <p><code>oc project lab2-demo</code></p> <p></p> </li> <li> <p>Run the below commands to deploy milvus DB      </p><pre><code>cd Part2-RAG/milvus-deployment\n\noc create configmap milvus-config --from-file=./config/milvus.yaml\n\noc apply -f .\n\ncd ..\n</code></pre> <p></p> </li> <li> <p>Monitor deployment using the below command until all pods are in Running state.     Hit Ctrl-C on the keyboard to exit and come back to the shell prompt.</p> <p><code>oc get pods -w</code></p> <p></p> </li> </ol>"},{"location":"Lab2/lab2-hands-on-guide/#deploy-jupyter-notebook","title":"Deploy jupyter notebook","text":"<ol> <li> <p>Run the below commands to deploy jupyter notebook (NB). Ignore any warnings if seen.</p> <p></p><pre><code>cd nb-deployment\n\noc apply -f .\n</code></pre> </li> <li> <p>Verify the notebook pod is running using the command below. Hit Ctrl-C on keyboard to exit and return back to shell prompt.</p> <p><code>oc get pods --selector=app=cpu-notebook -w</code></p> <p></p> </li> <li> <p>Once the notebook pod is deployed you should be able to access it using the link retrieved from the below command:</p> <p><code>oc get route cpu-notebook -o jsonpath='{.spec.host}'</code></p> <p></p> <p>In my case, the URL was:  <code>cpu-notebook-lab2-demo.apps.p1279.cecc.ihost.com</code>   but yours can be different! </p> <p>Alternate way to get the jupyter NB URL</p> <p>You can also goto OpenShift Administrator profile console window in your browser, navigate to Networking -&gt; Routes, select cpu-notebook route and click on the URL mentioned under Location field</p> </li> <li> <p>Copy and paste the URL in the browser. You should see the jupyter screen as below:     </p> </li> <li> <p>Now let's copy the jupyter NB (.ipynb file) present in the git repository to the NB pod.     In your <code>oc</code> CLI terminal window, navigate to the root of your git repository which has the RAG.ipynb file.</p> <p><code>cd /home/cecuser/bcn-lab-2084</code></p> <p></p> </li> <li> <p>List pods and copy the name of the cpu-notebook pod.</p> <p><code>oc get pods</code></p> <p></p> </li> <li> <p>Use oc cp ... command to copy the NB file from bastion server to /tmp/notebooks/ directory of the NB pod.</p> <p><code>oc cp ./RAG.ipynb cpu-notebook:/tmp/notebooks/</code></p> <p></p> </li> <li> <p>Go back to the jupyter NB application in your browser and hit refresh (F5 shortcut in keyboard). You should be able to see the RAG.ipynb file listed.</p> <p></p> </li> </ol>"},{"location":"Lab2/lab2-overview/","title":"Lab Overview","text":""},{"location":"Lab2/lab2-overview/#deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-education","title":"Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - Lab education","text":"<p>Goal of this lab is to get hands-on experience in deploying a RAG pattern on IBM Power10. Before we do that, lets understand what RAG is, its advantages, key usecases and how it fits in the IBM Power landscape.</p>"},{"location":"Lab2/lab2-overview/#what-is-rag","title":"What is RAG?","text":"<p>RAG (Retrieval-Augmented Generation) is an advanced technique that combines retrieval-based and generation-based approaches to improve the performance of large language models, especially in question-answering and knowledge-intensive tasks. In layman terms, clients can use RAG pattern to generate factually accurate output from LLMs, that is grounded in information in a knowledge base.</p> <p>Key Components of RAG:</p> <ol> <li>Retrieval: The model first retrieves relevant documents or information from a large knowledge base (like Wikipedia or custom databases) based on the user's query. This helps provide the model with factual and up-to-date information that it may not have learned during training.</li> <li>Augmented Generation: Once the relevant documents are retrieved, the model then generates a response based on the query and the retrieved documents. This ensures the generated answer is both relevant and factual, combining the reasoning power of the language model with the accuracy of external knowledge.</li> </ol>"},{"location":"Lab2/lab2-overview/#how-rag-works","title":"How RAG works?","text":"<p>RAG is a technique that uses vector databases to retrieve relevant information and improve the accuracy of Large Language Models (LLMs):</p> <ol> <li>Vector database storage: Text data is converted into vector embeddings using pre-trained models like BERT or GPT. These embeddings are then stored in a vector database.</li> <li>Query conversion: When a query is posed to the AI system, it is also converted into a vector</li> <li>Vector search: The vector database performs a vector search to find relevant embeddings from the stored dataset</li> <li>Information retrieval: The retrieved information is then integrated into the LLM's query input</li> <li>Response generation: The augmented query is sent to the LLM to generate an accurate answer.</li> </ol> <p>Why use vector database in RAG?</p> <p>Vector databases are used in RAG because they store data in a way that makes it easy to search and retrieve. Vector search techniques go beyond keyword matching and focus on semantic relationships, which improves the quality of the retrieved information.</p>"},{"location":"Lab2/lab2-overview/#need-advantages-of-rag","title":"Need &amp; Advantages of RAG","text":"<p>The need for Retrieval-Augmented Generation (RAG) arises from the limitations of current large language models (LLMs) and the growing demands for factual accuracy and knowledge scalability in AI applications. </p> <p>Here are the key reasons why RAG is necessary and its associated advantages:</p> <ol> <li>Handling Knowledge Gaps<ul> <li>LLMs are static: Traditional language models, once trained, cannot access new or external information. They can only generate text based on the data they were trained on, which means they might miss important or up-to-date knowledge.</li> <li>RAG dynamically retrieves information: By incorporating a retrieval step, RAG can pull in relevant, up-to-date documents from external sources to complement the model's output, making it more accurate and current.</li> </ul> </li> <li>Reducing Hallucinations<ul> <li>LLMs sometimes \"hallucinate\": LLMs can generate convincing but incorrect or fabricated answers because they are predicting text based on patterns rather than verifying facts.</li> <li>RAG grounds responses in real data: Since RAG retrieves factual documents before generating a response, it ensures that the output is based on real, verifiable information, reducing the risk of false or misleading content.</li> </ul> </li> <li>Scalability in Knowledge<ul> <li>LLMs are limited by training data: Even the largest models have limitations on how much they can remember from their training data, which might become outdated or incomplete.</li> <li>RAG scales with external data: By leveraging vast external knowledge bases or documents, RAG allows for almost unlimited knowledge expansion without retraining the model. This is particularly useful for enterprises or specific domains where continuous data updates are essential.</li> </ul> </li> <li>Improved Performance in Specific Domains<ul> <li>Specialized knowledge is often needed: Many applications require access to niche or domain-specific information, such as legal texts, scientific papers, or proprietary company data.</li> <li>RAG retrieves domain-specific documents: The retrieval step allows RAG to pull in domain-specific or proprietary documents, making the output more relevant for specialized tasks.</li> </ul> </li> <li>Efficiency and Adaptability<ul> <li>Model retraining is costly: Constantly retraining LLMs with new data is computationally expensive and time-consuming.</li> <li>RAG avoids retraining: By using real-time retrieval, RAG can access the latest information or new content without the need to retrain the entire model, making it more adaptable and cost-efficient.</li> </ul> </li> <li>Better Results for Open-Domain Question Answering<ul> <li>Complex queries require precise answers: In tasks like open-domain question answering, general models might struggle to provide precise answers for complex or rare questions.</li> <li>RAG enhances accuracy: By combining retrieval with generation, RAG can provide more accurate, context-rich answers, drawing from a wide range of documents.</li> </ul> </li> </ol> <p>In summary, RAG addresses limitations in current LLMs by improving factual accuracy, scalability, and adaptability, making it particularly useful for knowledge-intensive tasks and dynamic environments.</p>"},{"location":"Lab2/lab2-overview/#common-usecases-of-rag","title":"Common usecases of RAG","text":"<p>Below are some of the common usecases for RAG:</p> <ol> <li>Open-domain question answering: Where models need to answer questions about a wide range of topics, potentially beyond the training data.</li> <li>Customer support: Providing accurate answers by retrieving relevant documents from knowledge bases.</li> <li>Enterprise AI: Helping businesses with information retrieval, knowledge management, and research by retrieving and summarizing relevant documents.</li> </ol> <p>To summarize, RAG allows models to perform better in knowledge-intensive tasks by combining the strengths of both retrieval systems and generative language models.</p>"},{"location":"Lab2/lab2-overview/#ibm-power-and-rag","title":"IBM Power and RAG","text":""},{"location":"Lab2/lab2-overview/#ibm-power-systems-as-systems-of-record","title":"IBM Power Systems as Systems of Record","text":"<p>IBM Power Systems are renowned for their performance, reliability, and scalability, making them ideal for handling systems of record. A system of record (SOR) refers to a trusted source of truth that stores essential business data and transactions, such as financial systems, customer data, and inventory management. IBM Power Systems are often used for mission-critical applications in industries like banking, healthcare, and government due to their ability to manage large volumes of secure, transactional data.</p>"},{"location":"Lab2/lab2-overview/#how-ibm-power-systems-plays-well-with-rag","title":"How IBM Power Systems Plays Well with RAG","text":"<p>Retrieval-Augmented Generation (RAG) is a generative AI framework that enhances the performance of AI models by retrieving relevant documents from external knowledge bases before generating a response. This retrieval step ensures that the generated output is more accurate and fact-based, as it is grounded in real-time data from a reliable source.</p> <p>IBM Power Systems play exceptionally well with RAG due to the following reasons:</p> <ol> <li>High-Performance Data Handling<ul> <li>Power Systems are designed for high-volume data transactions and processing. This makes them perfect for storing and managing systems of record, which RAG relies on to retrieve real-time, relevant information.</li> <li>When RAG retrieves data from a system of record stored on IBM Power, it benefits from the fast data access and throughput that Power Systems offer, ensuring quick and efficient retrieval of documents for AI processing.</li> </ul> </li> <li>Data Security and Compliance<ul> <li>Power Systems are known for their robust security features, including end-to-end encryption and advanced data protection, making them ideal for storing sensitive data such as customer information, financial records, or healthcare data.</li> <li>In a RAG scenario, where retrieved data is used to generate answers, the ability of IBM Power Systems to ensure data privacy and regulatory compliance (e.g., HIPAA, GDPR) is crucial, especially for enterprises dealing with sensitive or regulated data.</li> </ul> </li> <li>Scalability and Reliability<ul> <li>RAG applications require scalable infrastructure to handle varying levels of computational demand, especially when dealing with large-scale document retrieval and real-time AI processing. IBM Power Systems are built to scale seamlessly, allowing RAG to handle larger datasets and more complex queries without performance degradation.</li> <li>Reliability is critical for systems of record, and Power Systems have a proven track record of uptime and resilience, ensuring that the data RAG retrieves is always available when needed, without risk of downtime affecting the retrieval process.</li> </ul> </li> <li>Integration with AI Workloads<ul> <li>IBM Power Systems are optimized for AI workloads, with features like accelerated AI processing (e.g., Power10's Matrix Math Accelerator (MMA)) that boost the performance of both retrieval and generation tasks in RAG.</li> <li>By running RAG-based applications on Power10, enterprises can take advantage of faster AI inference and improved data handling, resulting in more responsive and accurate AI systems.</li> </ul> </li> <li>Efficient Handling of Structured and Unstructured Data<ul> <li>Power Systems can efficiently handle both structured data (like databases) and unstructured data (such as documents and records), making them ideal for RAG, where both types of data may be retrieved from systems of record.</li> <li>RAG can retrieve structured data for quick reference (e.g., customer records or transactions) and unstructured data (e.g., reports, emails) for more complex, context-driven AI responses. Power Systems' capability to manage both types ensures efficient, accurate information retrieval.</li> </ul> </li> <li>Real-Time Analytics<ul> <li>Real-time data analytics capabilities in IBM Power Systems enable quick access to up-to-date information, which is essential for RAG when generating responses based on current data.</li> <li>This feature allows the RAG model to provide contextualized answers based on the latest transactions or data updates from the system of record, improving the relevance of AI-driven outputs.</li> </ul> </li> </ol> <p>Summary</p> <p>IBM Power Systems provide the speed, security, and reliability needed to store and manage systems of record that RAG models depend on for data retrieval. Power Systems\u2019 advanced capabilities in data handling, AI optimization, scalability, and security make them an ideal infrastructure for supporting RAG-based AI applications, ensuring that retrieved data is accurate, current, and secure, thus enhancing the quality of generative AI outputs.</p>"},{"location":"Lab2/lab2-overview/#rag-ibm-power10-solution-architecture","title":"RAG + IBM Power10 - solution architecture","text":"<p>Here is the high level solution architecture of a typical RAG use case on IBM Power10. This example is deployed on IBM Power10 end-to-end. The foundation model is simply downloaded from watsonx.ai or open-source repositories such as the Hugging Face model hub. The model does not require fine-tuning thanks to a domain adaptation technique called Retrieval Augmented Generation (RAG). </p> <p></p> <ol> <li>User asks a domain specific question in natural language.</li> <li>Q&amp;A app looksup in the knowledge base repository.</li> <li>Documents relevant to the question is retrieved from the repository.</li> <li>\"Question + Documents\" is passed as the context in a prompt to LLM.</li> <li>LLM generates the domain-specific answer.</li> </ol>"},{"location":"Lab3/lab3-hands-on-guide/","title":"Hands-on guide","text":""},{"location":"Lab3/lab3-hands-on-guide/#use-natural-language-to-generate-code-using-code-llm-on-ibm-power10-lab-guide","title":"Use natural language to generate code using code LLM on IBM Power10 - Lab guide","text":""},{"location":"Lab3/lab3-hands-on-guide/#lab-ready-check","title":"Lab-ready check","text":"<p>Make sure you have the following items ready:</p> <ul> <li>In your browser, you have logged in as cecuser in the OpenShift GUI/console.</li> <li>In your terminal, you have logged into the bastion server and authenticated to the OpenShift cluster using the OpenShift CLI oc.</li> </ul> <p>Warning</p> <p>Do not proceed further unless the items mentioned above are ready. Please refer to \"Lab setup instructions\" section (see left hand side menu) to setup your browser and terminal windows.</p>"},{"location":"Lab3/lab3-hands-on-guide/#lab-guide","title":"Lab guide","text":"<p>Image zoom functionality</p> <p>Feel free to click on the images in the lab guide below to a view larger image</p>"},{"location":"Lab3/lab3-hands-on-guide/#deploy-granite-code-llm","title":"Deploy granite code LLM","text":"<p>Pre-requisite</p> <p>This lab assumes you have finished Lab1. This lab uses the OpenShift resources deployed in Lab1 to optimize the usage of TechZone resources and to avoid re-deploying the same resources and re-learning the same concepts already taught in Lab1!</p> <ol> <li> <p>In this lab, we will deploy IBM's granite code LLM which is available on HF. Navigate to OpenShift developer profile window and ensure you are in lab1-demo project. Click Topology and ensure that your application is healthy and running (has a blue circle) before proceeding further.    </p> </li> <li> <p>Select ConfigMaps and click model-params </p> </li> <li> <p>Click on Actions -&gt; Edit ConfigMap </p> </li> <li> <p>In the resulting form, the key/value fields for MODEL_NAME and MODEL_URL as below and click Save</p> <ul> <li>Key: MODEL_NAME</li> <li>Value: granite-8b-code-instruct.Q4_K_M.gguf</li> </ul> <ul> <li>Key: MODEL_URL</li> <li>Value: https://huggingface.co/ibm-granite/granite-8b-code-instruct-4k-GGUF/resolve/main/granite-8b-code-instruct.Q4_K_M.gguf     </li> </ul> <p></p> </li> <li> <p>Use the deployment resource to restart the pods.</p> <p>ConfigMap update does not restart pods automatically</p> <p>The existing pod won't see the ConfigMap changes right away as changing values of a ConfigMap doesn't cause a deployment (and hence pod) to restart automatically. It needs to be done manually.</p> </li> <li> <p>Navigate to the deployment view. Click Topology, then click \"D lab1-demo\" part of the application icon, which will open up the deployment details pane (on the right hand side of the browser window). Click \"D lab1-demo\" in that pane which will then open up the deployment details view for lab1-demo deployment.    </p> </li> <li> <p>Click Actions and select Restart rollout. This will restart the deployment which results in re-deployment of the pod. This ensure the pod will now see the new ConfigMap changes.    </p> </li> <li> <p>Click on Pods tab, where you will see a new pod instantiated. The new pod will download the model and then start it. Since the configmap points to granite code model, it will be downloaded from HuggingFace and then started. When that happens the new pod's status will change to Running and the existing pod will be terminated.     </p> <p>Model download will take time - Have patience!!</p> <p>This process will take a few minutes (in my case it took around 3-4 mins as this is a fairly large model compared to tinyllama) and your mileage may vary! Remember, this is a demo environment and models are few GBs in size. Models once downloaded won't be downloaded again as long as you are using the same storage (PV).</p> </li> <li> <p>If all goes well, you should see just 1 pod running.     </p> </li> <li> <p>Let's verify that the model running is granite code LLM!. Click on the pod to enter the pod details view/page.     </p> </li> <li> <p>In the pod details page, click on the Logs tab to see the pod logs.     </p> </li> <li> <p>In the log window, scroll upwards to see the name of the model against the attribute llm_load_print_meta: general.name</p> <p></p> <p>This verifies that we have indeed deployed granite code LLM.</p> </li> <li> <p>Let's access our model. As we did in Lab1, head to Topology view, click on the Open URL icon of your application.     </p> </li> <li>A new browser window/tab where you will be able to interact with your newly deployed LLM. You should see a screen like this:     </li> <li> <p>Scroll all the way down to the input field \"Say something...\" where you can interact with the LLM. You can ask any question you like!     </p> <p>Experimenting with model parameters</p> <p>You can see a lot of model parameters or tunables (eg: Predictions, Temperature, etc.). Feel free to google and learn about them and experiment with it. You may want change some parameters, ask the same question and check how the response changes. We will not cover these parameters in this lab as its outside the scope of the lab</p> </li> </ol>"},{"location":"Lab3/lab3-hands-on-guide/#generate-python-code","title":"Generate python code","text":"<p>Now let's use the granite code model to generate python code using natural language queries.</p> <ol> <li> <p>Generating python code for printing the fibonacci series    </p> <ul> <li> <p>I ran this code on my local python environment and it ran without errors!</p> </li> <li> <p>Note that I spelled fibonacci incorrectly, yet it understood my question correctly.</p> </li> <li> <p>Also note that the answer it gave uses recursion (function fib is being called recursively).</p> </li> </ul> </li> <li> <p>Now let's try asking it to generate the same code without using recursion.    </p> <ul> <li> <p>As expected, it did give the code snippet that doesn't use recursion, so it did well there.</p> </li> <li> <p>Generated code is not 100% correct. I ran this code on my local python environment and it ran into some issues and had to fix some code to make it generate the right fibonacci sequence.</p> </li> <li> <p>The above shows that code LLMs can generate almost perfect code and in some cases it might need developer intervention to make it perfect!</p> </li> </ul> <p>Accuracy of code LLMs</p> <ul> <li>LLMs excel at generating simple or boilerplate code, often producing highly accurate results for common tasks such as sorting algorithms, basic input/output operations, or template-based functions.</li> <li>For routine tasks and widely-used languages like Python or JavaScript, accuracy rates can be high, sometimes upwards of 80-90% for straightforward problems.</li> <li>When dealing with more complex algorithms, nuanced edge cases, or multi-step logic, LLMs may struggle. The model can produce syntactically correct code that might not solve the problem as intended or might have logic errors.</li> <li>For complex use cases, accuracy can drop significantly, often requiring human intervention to correct the output.</li> </ul> </li> </ol> <p>Feel free to explore and try out more scenarios!</p>"},{"location":"Lab3/lab3-hands-on-guide/#generate-c-code","title":"Generate C code","text":"<p>Now let's use the granite code model to generate C code using natural language queries.</p> <ol> <li> <p>Generate C code for sorting a list of numbers.    </p> <ul> <li>I ran this code on my local C environment and it ran without errors! Ofcourse I had to fix the first line (which was incomplete) as <code>#include &lt;stdio.h&gt;</code></li> </ul> </li> <li> <p>Generate C code for swapping 2 numbers without using a temporary variable    </p> <ul> <li>I ran this code on my local C environment and it ran without errors! Ofcourse I had to fix the first line (which was incomplete) as <code>#include &lt;stdio.h&gt;</code></li> </ul> </li> </ol> <p>Feel free to explore and try out more scenarios!</p>"},{"location":"Lab3/lab3-hands-on-guide/#generate-sql-query","title":"Generate SQL query","text":"<p>Generating SQL query using natural langguage is a little different than C or python code since SQL is a language to query Databases (DBs). One needs to give enough context to the code LLM for it to understand the DB table schemas for which you want it to generate the SQL query.</p> <p>The context is given as a prompt to the code LLM which preceeds the natural language query and the code LLM answers the query using the context given in the prompt. The best way to understand is looking at some examples as depicted below.</p> <p>Let's take an super simple example of a bank which has information stored in 2 tables in a DB:</p> <ul> <li>USERS - This which has user specific info (user_id, name, age, dob etc)</li> <li>ACCOUNTS - This holds the account balance of each user with user_id being the common field between the 2 tables</li> </ul> <ol> <li>Given the above DB example, the prompt (aka context) we need to provide to the code LLM is as below:    <pre><code>You are a developer writing SQL queries given natural language questions. The database contains a set of 2 tables. The schema of each table with description of the attributes is given. Write the SQL query given a natural language statement with names being not case sensitive\n\n Here are the 2 tables :\n\n (1) Database Table Name: USERS\n Table Schema:\n Column Name # Meaning\n user_id # unique identifier of the user\n user_name # name of the user\n usertypeid # user is '\\''employee'\\'', '\\''customer'\\''\n gender_id # user'\\''s gender is 1 for female, 2 for male and 3 for other\n dob # date of birth of the user\n address # adress of the user\n state # state of the user\n country # country of residence of the user\n\n (2) Database Table Name: ACCOUNTS\n Table Schema:\n Column Name # Meaning\n acc_id # account number or account id of the user\n user_id # user id of the user\n balance # available balance in the account\n</code></pre></li> <li> <p>Natural language query can be something like:</p> <pre><code>With the above schema, please generate sql query to list all users whose balance is &gt; 2000\n</code></pre> </li> <li> <p>Let's send the \"Prompt + Query\" to the code LLM and see how it responds. Feel free to copy and paste it in your LLM application window.</p> <pre><code>You are a developer writing SQL queries given natural language questions. The database contains a set of 2 tables. The schema of each table with description of the attributes is given. Write the SQL query given a natural language statement with names being not case sensitive\n\n Here are the 2 tables :\n\n (1) Database Table Name: USERS\n Table Schema:\n Column Name # Meaning\n user_id # unique identifier of the user\n user_name # name of the user\n usertypeid # user is '\\''employee'\\'', '\\''customer'\\''\n gender_id # user'\\''s gender is 1 for female, 2 for male and 3 for other\n dob # date of birth of the user\n address # adress of the user\n state # state of the user\n country # country of residence of the user\n\n (2) Database Table Name: ACCOUNTS\n Table Schema:\n Column Name # Meaning\n acc_id # account number or account id of the user\n user_id # user id of the user\n balance # available balance in the account\n\n With the above schema, please generate sql query to list all users whose balance is &gt; 2000\n</code></pre> <p></p> <p></p> <ul> <li>The SQL query generated seems correct.</li> <li>Its joining both the tables using <code>user_id</code> as the key and selecting all records where the user's account balance is &gt; 2000</li> <li>For the sake of people who may want to analyse further, pasting the SQL query that was generated:  <code>SELECT * FROM USERS u, ACCOUNTS a WHERE u.user_id = a.user_id AND a.balance &gt; 2000;</code></li> </ul> </li> <li> <p>Interestingly, code LLM works both ways! Given a SQL query, you can ask code LLM to explain what it does. To do that I have formed the query as below. Feel free to copy the query and post it in your LLM application window.</p> <pre><code>What does the below SQL query do ?\nSQL Query:\nSELECT * FROM USERS u, ACCOUNTS a WHERE u.userid = a.userid AND a.balance &gt; 2000;\n</code></pre> <p></p> <p></p> <p>That's a decent explanation of the SQL query!</p> </li> <li> <p>Let's try one more example. Here I give it 2 conditions to match in the query.    NOTE: You don't have to repeat the whole schema in the prompt. LLMs can remember context.</p> <pre><code>Using the schema given above, generate SQL query to list all users whose account balance &gt; 2000 and user is of type employee\n</code></pre> <p></p> <p></p> <p>Response received as below:</p> <p></p><pre><code>Here's the SQL query to list all users whose account balance is greater than 2000 and they are of type employee from the USERS and ACCOUNTS tables:\nSELECT * FROM USERS u, ACCOUNTS a WHERE u.user_id = a.user_id AND a.balance &gt; 2000 AND usertypeid='employee';\n</code></pre> <ul> <li>The response is not 100% correct.</li> <li>The last part of the SQL query <code>usertypeid='employee'</code> is ambiguous as the DB won't know which <code>usertypeid</code> column to reference.</li> <li>The correct SQL query would have <code>u.usertypeid='employee'</code> so that the DB knows that its part of the USERS (aliased as <code>u</code> in the query) table.</li> </ul> </li> <li> <p>Re-iterating some of the points we learned in this lab:</p> <ul> <li> <p>Code LLMs are not 100% correct, yet they can be immensely helpful for a developer as they can help generate near perfect code which can then be analyzed &amp; tweaked to perfection by the developer.</p> </li> <li> <p>There are many code LLMs available in HF with varied degree of accuracy and clients can choose the right one by experimenting with them in the context of their specific use case.</p> </li> <li> <p>IBM also offers enterprise grade code LLMs via its watsonx Code Assistant (WCA) family of product offerings.</p> </li> </ul> </li> <li> <p>IBM Power servers are best used as system of record (SoR) servers which means they hold a lot of enterprise specific data in different DBs (e.g.: Oracle on AIX, DB2 on AIX / IBM i, PostgreSQL on Linux etc). From an IBM Power solution point of view, an end to end solution to query DB records using natural language can be easily implemented which can help clients immensely. They don't need to depend on experts to generate DB reports. Even executives (with authorized access to the DB) can generate reports and/or view data using natural language queries.</p> <ul> <li>Check this ~2min short video demo on \"Infusing AI into mission critical workloads with PowerVS and watsonx.ai\" which uses natural language to query fraudulent transactions and list high value customers. Although this demo is based on PowerVS use case the same is applicable to on-premise as well.</li> </ul> </li> </ol> <p>Summary</p> <p>Code LLMs democratize coding by making it more accessible to non-coders, accelerating the development process for smaller teams, and assisting both beginners and experienced developers in learning and improving productivity. By lowering technical barriers, they are transforming who can engage in software development and how innovation happens across industries.</p>"},{"location":"Lab3/lab3-overview/","title":"Lab Overview","text":""},{"location":"Lab3/lab3-overview/#generate-sql-query-using-code-llm-on-ibm-power10-lab-education","title":"Generate SQL query using code LLM on IBM Power10 - Lab education","text":"<p>Goal of this lab is to acquire some hands-on experience with code LLMs and understand their usecases. In this lab, we will use a code LLM to convert a natural language query into a SQL statement which can be used to query DBs. We will also to generate some python and C++ code and understand how better prompting can generate code closer to your expectations.</p> <p>But before we do that, lets understand what code LLM is, its benefits, key use cases and IBM products around code LLMs.</p>"},{"location":"Lab3/lab3-overview/#what-is-code-llm","title":"What is code LLM?","text":"<p>A Code LLM (Large Language Model) is a type of AI model specifically trained to understand, generate, and manipulate programming code. These models, built on architectures like GPT or similar transformers, are trained on vast datasets of code from various programming languages, enabling them to assist with coding tasks such as generating code snippets, debugging, refactoring, and even writing documentation.</p>"},{"location":"Lab3/lab3-overview/#key-features-of-code-llm","title":"Key Features of Code LLM","text":"<ul> <li>Code Generation: Code LLMs can generate new code based on prompts, such as writing functions, classes, or even entire programs.</li> <li>Code Completion: Similar to how text-based LLMs suggest completions for sentences, code LLMs can suggest code completions in real-time, aiding developers in writing code faster.</li> <li>Bug Detection and Fixes: These models can detect bugs or potential issues in code and suggest corrections.</li> <li>Multi-Language Support: Code LLMs are typically trained on multiple programming languages, allowing them to work across various tech stacks (e.g., Python, Java, JavaScript, C++, etc.).</li> <li>Code Explanation: Some models can explain how a piece of code works, making them useful for learning, documentation, and debugging.</li> </ul>"},{"location":"Lab3/lab3-overview/#use-cases","title":"Use Cases","text":"<ul> <li>Autocompletion in IDEs: Code LLMs like GitHub Copilot, powered by OpenAI's Codex, assist developers by offering code suggestions and completions.</li> <li>Code Refactoring: LLMs can suggest improvements or optimizations to existing code, helping improve performance or readability.</li> <li>Debugging and Error Resolution: Code LLMs can help identify issues in code and suggest potential fixes, streamlining the debugging process.</li> <li>Automated Documentation: They can generate comments or documentation for code, reducing the manual effort required for explaining code logic.</li> </ul>"},{"location":"Lab3/lab3-overview/#examples-of-code-llms","title":"Examples of Code LLMs","text":"<p>Below are few examples of code LLMs:</p> <ul> <li>OpenAI Codex: The LLM that powers GitHub Copilot, trained specifically on a large corpus of programming languages and capable of generating code.</li> <li>AlphaCode: DeepMind's LLM designed for competitive programming tasks.</li> <li>CodeT5: A transformer model from Hugging Face fine-tuned for code generation and understanding tasks.</li> <li>Code Llama: It is a large language model developed by Meta, optimized for coding tasks such as code generation, completion, and debugging, supporting multiple programming languages for enhanced developer productivity.</li> <li>IBM Granite code models: A family of open foundation models for code intelligence. It is an enterprise-focused large language model developed by IBM Research, designed to enhance productivity in software development by enabling code generation, debugging, and refactoring with a strong emphasis on security and performance.</li> </ul>"},{"location":"Lab3/lab3-overview/#benefits-of-code-llms","title":"Benefits of code LLMs","text":"<ul> <li>Boosts Developer Productivity: By automating repetitive coding tasks, Code LLMs help developers write code faster and reduce errors.</li> <li>Assists with Learning: Code LLMs are valuable for learners by explaining how code works and suggesting how to fix bugs.</li> <li>Cross-Language Compatibility: Supporting multiple languages, Code LLMs help developers work across different coding environments without needing expertise in all languages.</li> </ul> <p>In essence, Code LLMs bring AI-driven enhancements to the software development process, making it faster, more accurate, and more accessible.</p>"},{"location":"Lab3/lab3-overview/#ibm-watsonx-code-assistant","title":"IBM watsonx Code Assistant","text":"<p>IBM watsonx\u2122 Code Assistant (WCA) is a family of offerings from IBM that leverages generative AI to accelerate development while maintaining the principles of trust, security and compliance at its core. Developers and IT Operators can speed up application modernization efforts and generate automation to rapidly scale IT environments. </p> <p>WCA is powered by the IBM Granite foundation models that include state-of-the-art large language models designed for code, geared to help IT teams create high-quality code using AI-generated recommendations based on natural language requests or existing source code. Built on the Watsonx AI platform, this tool leverages IBM\u2019s expertise in AI and machine learning to enhance productivity across various industries.</p> <p>IBM watsonx Code Assistant (WCA) support on IBM Power</p> <p>NOTE: At the time of writing this lab, IBM WCA is not available to run on IBM Power yet. However, clients can use code specific foundation models (either hosted by WCA on IBM Cloud or running stand-alone on-premise on IBM Power) to infuse and harness the power of code LLMs into their on-prem applications running on IBM Power.</p>"},{"location":"Lab3/lab3-overview/#ibm-wca-architecture-offerings","title":"IBM WCA architecture &amp; offerings","text":"<p>Here is a high level architecture of IBM WCA:  At the bottom-most layer, WCA is powered by IBM Granite code models, specifically trained and fine-tuned by IBM for different programming languages.</p> <p>At the time of writing this lab, there are 2 products available under the IBM WCA offering family:</p> <ul> <li> <p>IBM watsonx Code Assistant for Red Hat Ansible Lightspeed: The Ansible-tuned model forms the basis of this product. This is fine-tuned for Ansible use cases. In addition to x86 endpoints, this supports generating Ansible tasks/playbooks for IBM Power endpoints (running AIX, IBM i and Linux) as well.</p> </li> <li> <p>IBM watsonx Code Assistant for Z: The Cobol to Java-tuned model forms the basis of this product. This is fine-tuned for COBOL to Java conversion. It can help with enterprise use cases around IBM Z application modernization.</p> </li> </ul> <p>More and more programming languages support is being added to WCA over time.</p>"},{"location":"Lab3/lab3-overview/#key-features-of-ibm-wca","title":"Key Features of IBM WCA","text":"<ol> <li>Code Generation: Automatically generates code snippets or complete functions based on natural language prompts or specific programming needs.</li> <li>Debugging Assistance: Helps identify bugs and suggests fixes, improving the efficiency of the development process.</li> <li>Multi-Language Support: Supports a variety of programming languages, making it versatile for different development environments.</li> <li>Security and Compliance: Designed with a strong focus on enterprise-level security, ensuring that generated code adheres to industry standards and compliance regulations.</li> <li>Customization: Tailors AI recommendations to the specific needs of a project, enabling more accurate code suggestions and personalized developer assistance.</li> </ol>"},{"location":"Lab3/lab3-overview/#benefits-of-ibm-wca","title":"Benefits of IBM WCA","text":"<ol> <li>Increased Developer Productivity: Automates routine coding tasks, allowing developers to focus on higher-level problem-solving.</li> <li>Enhanced Code Quality: Improves the accuracy of code with AI-powered insights, reducing the chances of errors or vulnerabilities.</li> <li>Enterprise-Grade Security: Ensures that code generation is secure and compliant with regulations, making it suitable for industries like finance and healthcare.</li> </ol>"}]}