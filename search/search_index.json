{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the AI on Power - Level 3 course - seller enablement demonstration. This Level 3 course will provide hands-on enablement on how to use Generative AI (GenAI) models on Power10 and will cover few use-cases to help understand the art of the possible.</p> <p>There are 4 main parts to this demonstration as you can see on the left hand side navigation pane:</p> <ul> <li> <p>Lab Setup - How to provision and setup the lab env for the hands-on demos</p> </li> <li> <p>LAB 1 \u2013 Deploy a LLM on Power10 - Deploy a LLM on Power10 and switch to a diff LLM</p> </li> <li> <p>LAB 2 \u2013 Deploy RAG on Power10 - RAG chatbot for domain specific Q&amp;A</p> </li> <li> <p>LAB 3 \u2013 Query DB using NL on Power10 - Generate SQL queries for a given DB schema, using NL</p> </li> </ul>"},{"location":"#generative-ai-and-llms-overview","title":"Generative AI and LLMs overview","text":"<p>Large Language Models (LLMs) and Generative AI (GenAI) have revolutionized industries by enabling machines to understand, generate, and interact with human language in ways never seen before. They have disrupted the market by:</p> <ul> <li> <p>Automating Content Creation: From drafting articles, reports, and code to generating creative content like music and artwork, GenAI has drastically reduced time and costs in creative industries.</p> </li> <li> <p>Enhancing Customer Experience: LLM-powered chatbots and virtual assistants are offering more personalized, human-like interactions, reshaping customer service and support.</p> </li> <li> <p>Transforming Decision-Making: With advanced data processing and natural language understanding, businesses are leveraging AI for smarter, faster insights and predictive analytics.</p> </li> <li> <p>Boosting Productivity: LLMs streamline workflows by handling complex tasks, like writing, summarizing, and translating, allowing professionals to focus on more strategic, creative efforts.</p> </li> </ul> <p>In short, LLMs and GenAI are reshaping industries by enhancing efficiency, creativity, and customer engagement, making them indispensable tools in the modern business landscape.</p>"},{"location":"#why-ibm-power-for-genai","title":"Why IBM Power for GenAI","text":"<p>IBM Power platform is a high-performance, enterprise-grade computing solution designed to handle demanding workloads such as AI, analytics, cloud, and mission-critical applications. With unmatched processing power, scalability, and reliability, IBM Power enables organizations to optimize performance, lower costs, and ensure security.</p>"},{"location":"#key-benefits","title":"Key Benefits:","text":"<ul> <li> <p>Unmatched Performance: Optimized for data-intensive applications, AI, and large-scale analytics, delivering faster insights and better decision-making.</p> </li> <li> <p>Scalability: Seamlessly scale workloads across on-prem, cloud, and hybrid environments to meet evolving business needs.</p> </li> <li> <p>Enterprise-Grade Security: Built-in security features provide robust protection for sensitive data and critical operations.</p> </li> <li> <p>Efficiency and Flexibility: Designed for diverse environments, supporting Linux, AIX, IBM i, and containerized applications, making it versatile for a range of industries.</p> </li> </ul> <p>IBM Power is the platform of choice for businesses seeking to future-proof their IT infrastructure, handle complex workloads, and accelerate innovation</p>"},{"location":"#ibm-power10-differentiation","title":"IBM Power10 differentiation","text":"<p>IBM Power10 offers several advantages for Generative AI (GenAI) use cases, providing the computational power, security, and efficiency required to support the demanding workloads of AI-driven applications. Here\u2019s how Power10 is uniquely positioned to accelerate GenAI:</p>"},{"location":"#key-advantages","title":"Key Advantages:","text":"<ol> <li> <p>High Performance for AI Workloads:</p> <ul> <li> <p>AI Acceleration: IBM Power10 includes AI inference acceleration directly on the chip, allowing faster processing of GenAI tasks such as model training, inference, and real-time AI applications.</p> </li> <li> <p>Massive Scalability: Power10 is designed to handle large-scale AI models, with enhanced memory bandwidth and processing power to efficiently manage vast datasets used in training LLMs and GenAI systems.</p> </li> </ul> </li> <li> <p>Memory and Bandwidth Optimization:</p> <ul> <li>The DDR5 memory and faster I/O bandwidth provide a significant performance boost, enabling GenAI models to run more efficiently.</li> </ul> </li> <li> <p>Energy Efficiency:</p> <ul> <li>Power10 processors are highly energy-efficient, offering more performance per watt compared to previous generations. This reduces operational costs and is especially beneficial for the resource-intensive training and deployment of GenAI models.</li> </ul> </li> <li> <p>Security:</p> <ul> <li> <p>IBM Power10 comes with end-to-end encryption and advanced memory protection features, ensuring that sensitive AI workloads are secure from potential breaches. This is particularly important in industries like healthcare and finance, where GenAI applications need strong data protection.</p> </li> <li> <p>Quantum-safe encryption helps future-proof AI workloads against quantum computing threats.</p> </li> </ul> </li> <li> <p>Flexible Cloud and Hybrid Deployments:</p> <ul> <li>Power10 integrates smoothly with hybrid cloud environments, allowing enterprises to run GenAI workloads both on-premises and in the cloud. This flexibility is key for scaling AI applications while maintaining control over data and infrastructure.</li> </ul> </li> <li> <p>Support for AI Ecosystems:</p> <ul> <li>IBM Power10 works seamlessly with AI frameworks like TensorFlow and PyTorch, making it easier for developers to build, train, and deploy GenAI models without needing major infrastructure changes.</li> <li>Support for OpenShift and Kubernetes enhances containerization, allowing efficient orchestration of AI workloads.</li> </ul> </li> </ol>"},{"location":"#summary","title":"Summary:","text":"<p>IBM Power10 is designed to meet the specific needs of GenAI use cases by offering high-performance processing, memory scalability, security, and energy efficiency. It empowers enterprises to handle complex AI workloads efficiently while maintaining flexibility and security, making it ideal for industries that rely on AI-driven innovation.</p>"},{"location":"lab-setup/","title":"Lab setup instructions","text":""},{"location":"lab-setup/#lab-setup","title":"Lab Setup","text":""},{"location":"lab-setup/#provisioning-the-environment","title":"Provisioning the environment","text":"<p>For the hands-on labs, we will be using the OpenShift on Power10 on-prem environment, optimized for AI workloads and hosted on IBM TechZone.</p> <p>Follow the steps below:</p> <ol> <li> <p>Head to this TechZone collection and provision the environment named \"OpenShift 4.14 ready for AI on IBM Power10 (Container PaaS)\" by clicking on Reserve and submitting the resulting form (select Education as purpose)    </p> </li> <li> <p>Watch your email for updates from TechZone and wait for your environment to be provisioned.</p> </li> <li>Once provisioned, head to my reservations page to ensure its ready.    </li> </ol>"},{"location":"lab-setup/#accessing-the-environment","title":"Accessing the environment","text":"<ol> <li>Since this is an on-prem environment, make sure you are connected to the IBM network to access the environment.(TODO: add details on VPN)</li> <li>In TechZone, click on your provisioned environment under \"My Reservations\" which will open up the details page.</li> <li>Scroll to the \"Reservation Details\" section of the page which has information on how to connect to OpenShift console    </li> </ol>"},{"location":"lab-setup/#accessing-openshift-guiconsole","title":"Accessing OpenShift GUI/console","text":"<ol> <li>In the \"Reservation Details\" section of the TechZone environment details page, click on the OpenShift console link</li> <li>This will open up a new browser tab/window and opens up the OpenShift console login page   <ul> <li>If you encounter any security exception, navigate to the bottom of the browser page, acccept the exception under Advanced and continue. This is ok as we are in a lab/demo environment and using self-signed certificates.</li> </ul> </li> <li>On the OpenShift console page, select the htpasswd login option.</li> <li>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code><ul> <li>TIP: Click on the copy icon provided under 'User Password' in the Reservation Details page to copy the password and paste it in the OpenShift console window</li> </ul> </li> </ol> <ol> <li>You have successfully logged into the OpenShift cluster using the GUI/console. You should be able to see the dashboard (or the page you were on before logging off) of your OpenShift console.</li> </ol> <p>RE-AUTHENTICATING in case you lose GUI/console access</p> <p>In case you lose access to the OpenShift cluster and need to re-login to the GUI/console, which is possible in case your reservation expires and/or your GUI/console authentication timed-out, please follow the above steps again to re-login to your OpenShift GUI/console</p>"},{"location":"lab-setup/#accessing-openshift-oc-cli","title":"Accessing OpenShift <code>oc</code> CLI","text":"<ol> <li>Go back to the \"Reservation Details\" section of the TechZone environment details page.</li> <li>Open your terminal window and use SSH utility to connect to the Bastion node of OpenShift cluster.<ul> <li><code>ssh -l cecuser &lt;your bastion hostname/IP as provided in Reservation Details section&gt;</code></li> <li>If <code>ssh</code> gives any warning, type <code>yes</code> and continue</li> <li>When prompted for password, copy the password from Reservation Details page by clicking on the copy icon and pasting it in the ssh terminal window</li> </ul> </li> <li>You have logged in successfully to the bastion node of your OpenShift cluster.<ul> <li>Keep this terminal window always open as you will be using it frequently to run CLI commands.</li> </ul> </li> <li> <p><code>oc</code> CLI is pre-installed on the bastion node and must be working already. You can check by running <code>oc version</code> command.</p> <ul> <li>Ignore the error part of the <code>oc version</code> for now. Its as expected since you have not yet logged into the cluster from the CLI.</li> </ul> <p> </p> </li> </ol>"},{"location":"lab-setup/#logging-in-to-openshift-cluster-using-oc-cli","title":"Logging in to OpenShift Cluster using <code>oc</code> CLI","text":"<p>Let's login to the OpenShift cluster via the <code>oc</code> CLI. This is needed as we will execute some CLI commands as part of the lab steps.</p> <ol> <li>In the OpenShift console, top right section click on <code>cecuser</code> and select <code>Copy login command</code> option.</li> </ol> <p></p> <ol> <li>A new browser window (or tab, depending on your browser setting) opens up.<ul> <li>If you encounter any security exception, navigate to the bottom of the browser page, acccept the exception under Advanced and continue. This is ok as we are in a lab/demo environment and using self-signed certificates.</li> </ul> </li> <li>You will be presented with another login screen. Click htpasswd option    </li> <li>Use Username: <code>cecuser</code> and Password: <code>&lt;as provided in the TechZone Reservation Details page&gt;</code><ul> <li>TIP: Click on the copy icon provided under 'User Password' in the Reservation Details page to copy the password and paste it in the OpenShift console window.</li> </ul> </li> <li>Click Display token</li> </ol> <p></p> <ol> <li>Copy the <code>oc login --token=...</code> CLI and paste it in the bastion node terminal window.     </li> <li>You have successfully logged into the OpenShift cluster using the <code>oc</code> CLI.</li> </ol> <p>RE-AUTHENTICATING in case you lose CLI access</p> <p>In case you lose access to the OpenShift cluster and need to re-authenticate using the CLI, which is possible in case your reservation expires and/or your CLI window terminated for some reason, please follow the above steps again to get back your <code>oc</code> CLI authenticated to the OpenShift cluster</p>"},{"location":"lab-setup/#summary","title":"Summary","text":"<p>Efforts are made to keep the lab instructions simple and easy to follow to cater to audience of all skill levels. We strive to use OpenShift GUI/console as much possible, but in some scenarios OpenShift console doesn't yet support some functionality in which case we switch to the <code>oc</code> CLI. Hence this lab will use OpenShift GUI/console and <code>oc</code> CLI both as necessary.</p>"},{"location":"pre-req/","title":"Pre-requisites","text":""},{"location":"pre-req/#pre-requisites-for-the-course","title":"Pre-requisites for the course","text":"<p>The following are the pre-requisites for this course:</p> <ul> <li> <p>AI on Power - Level 2 course - IBM | BP</p> </li> <li> <p>Basic understanding of Generative AI (GenAI) and Large Language Models (LLM)</p> <ul> <li>If not, feel free to learn more by doing Google search :) </li> </ul> </li> <li> <p>Basic understanding of OpenShift and working with yaml files. </p> <ul> <li>In any case, appropriate instructions are provided through-out the course</li> </ul> </li> <li> <p>Laptop with ssh pre-installed to connect to lab environment.</p> <ul> <li>Windows: PowerShell should have ssh pre-installed.<ul> <li>Mac/Linux: ssh should be already available</li> <li>If not, Google is your friend :) </li> </ul> </li> </ul> </li> </ul>"},{"location":"Lab1/lab1-overview/","title":"Lab Overview","text":""},{"location":"Lab1/lab1-overview/#deploy-a-large-language-model-llm-on-ibm-power10-lab-education","title":"Deploy a Large Language Model (LLM) on IBM Power10 - Lab Education","text":"<p>Goal of this lab is to showcase the basics (and the ease) of deploying a LLM (aka foundation models) on Power and how easy it is to switch to a different LLM. But before we do that, let's understand the ecosystem around LLMs (aka foundation models or Generative AI) in the context of IBM Power.</p> <p>In this lab, we will deploy 2 LLMs from Hugging Face (HF) on IBM Power10 using the on-prem lab environment that has been provisioned.</p>"},{"location":"Lab1/lab1-overview/#what-is-hugging-face","title":"What is Hugging Face?","text":"<p>Hugging Face is an AI company that built an incredibly popular AI community around open source libraries, models, and data sets. It's an open source community with a large collection of AI models and datasets that are available for sharing and collaboration. The Hugging Face hub now hosts more than 950K models and over 210K data sets, and those numbers are growing quickly.</p> <p>What is Hugging Face?</p> <p>Hugging Face is an AI company that has become a major hub for Natural Language Processing (NLP) and Machine Learning (ML) tools. It is widely known for its open-source library, Transformers, and its collaborative platform, which provides a vast collection of pre-trained models and datasets. Hugging Face has made it easy for developers, researchers, and businesses to access, fine-tune, and deploy state-of-the-art machine learning models, especially those related to NLP tasks like text classification, translation, summarization, and more.</p> <p>We use HF to access LLMs in this lab as it has a vast collection of pre-trained models which are hosted publicly and can be accessed for free.</p>"},{"location":"Lab1/lab1-overview/#what-is-ibm-watsonx","title":"What is IBM watsonx?","text":"<p>Enterprise clients of IBM will use the IBM watsonx platform, an enterprise-ready AI and data platform. Watsonx is designed for the enterprise and is targeted for business domains, empowering value creators to transform business applications into AI-first applications.</p> <p>What is IBM watsonx?</p> <p>IBM Watsonx is IBM\u2019s next-generation AI and data platform, designed to help enterprises build, train, fine-tune, and deploy large-scale AI models efficiently. Watsonx enables businesses to harness the power of artificial intelligence and machine learning (AI/ML) for various tasks like generative AI, predictive analytics, and decision-making. It integrates advanced capabilities for handling large language models (LLMs), machine learning workflows, and AI governance.</p> <p>IBM watsonx support on IBM Power</p> <p>NOTE: At the time of writing this lab, IBM watsonx is not available to run on IBM Power yet. However, clients can use foundation models (either hosted by watsonx on IBM Cloud or running stand-alone on-premise on IBM Power) to infuse and harness the power of (Gen)AI into their on-prem applications running on IBM Power.</p>"},{"location":"Lab1/lab1-overview/#ibm-watsonx-vs-hugging-face","title":"IBM watsonx Vs Hugging Face","text":"<p>IBM Watsonx and Hugging Face both offer AI tools, but they serve different purposes:</p> <ul> <li> <p>IBM watsonx: An enterprise-grade AI platform designed for large-scale AI model training, fine-tuning, and deployment. It focuses on governance, compliance, and custom AI models for business use, offering robust tools for data management and AI governance. Ideal for enterprises needing secure, scalable AI with industry-specific solutions.</p> </li> <li> <p>Hugging Face: A community-driven platform offering pre-trained models (especially in NLP) and datasets for rapid experimentation and development. It's known for its open-source library and easy access to state-of-the-art models, making it great for research, startups, and developers.</p> </li> </ul> <p>In essence, IBM watsonx focuses on enterprise-grade AI with strong governance and scalability, while Hugging Face is designed for flexible, community-driven AI development with a focus on rapid prototyping and open access to pre-trained models.</p>"},{"location":"Lab1/lab1-overview/#ibm-watsonx-hugging-face","title":"IBM watsonx &amp; Hugging Face","text":"<p>With IBM watsonx, clients can run not just IBM-trained foundation models, but also open source models and models from Hugging Face as well!</p> <p>IBM watsonx and Hugging Face can work together by combining IBM's enterprise AI capabilities with Hugging Face's vast collection of pre-trained models and tools for rapid AI development:</p> <ul> <li>Model Access: IBM watsonx users can leverage Hugging Face's pre-trained models (like GPT, BERT, etc.) from its model hub to fine-tune or deploy in enterprise environments using Watsonx\u2019s scalable infrastructure.</li> <li>Fine-Tuning and Customization: Businesses can use Hugging Face models in Watsonx to fine-tune them with proprietary data while benefiting from watsonx\u2019s AI governance and compliance tools.</li> <li>Deployment: Hugging Face models can be integrated into IBM watsonx to deploy at scale on hybrid cloud or on-premise environments, ensuring enterprise-level security and performance.</li> </ul> <p>Together, Hugging Face provides the models, and watsonx offers the enterprise-ready infrastructure for secure, large-scale, compliant deployment. Read this blog to understand more about how IBM and Hugging Face are working to bring open source communities together for enterprise AI.</p>"},{"location":"Lab1/lab1-overview/#ai-and-watsonx-with-ibm-power","title":"AI and watsonx with IBM Power","text":"<p>Here is a quick 1-slider on what you can do with AI and watsonx on IBM Power, today. </p> <p>Clients can get started with AI and watsonx with IBM Power today.  IBM has made it simple by aligning the common use cases around 4 key areas that we see within our pilots and client activations.</p> <ul> <li>Pattern 1: Securely tune, deploy and manage foundation models. When it comes to task-specific use cases, it is a good idea to work with large open-source models in your own workspace that are under your control. Hugging Face is a large repository with over 950K pre-trained ML models and a platform where the AI ecosystem collaborates on models, datasets and applications. Download any model from Hugging Face and securely deploy at scale on IBM Power. Then, use best-of-breed software to help you tune, deploy and manage as many models as you need. Some examples of what enterprises can use this capability for: Customer service, knowledge workers to augment staff and fraud reporting. \u00a0</li> <li>Pattern 2: There are many new and existing business apps that are using foundation models integrated into the workflows. Deploy your foundation models anywhere, on Power, x86, cloud, and use the watsonx.ai software development kit (SDK) available in Python and embed directly into applications. On Power, enterprises can do this quickly so that services can be released to customers faster on a resilient 24/7 environment. Some examples of clients can embed AI into apps are generating the first draft of reports, citizen services for government end-clients and knowledge management. \u00a0</li> <li>Pattern 3: The ecosystem is important to enterprises that have long-standing investments in software that drives their core business workflows. Consume watsonx services from customers customized ecosystem apps. Enterprises can generate code for Ansible playbooks for IBM i or AIX to enhance the Ansible IT management experience. Additionally, SAP applications can be embedded with watsonx services within the SAP ABAP environments. These custom apps help clients deliver services much faster while taking advantage of existing and familiar investments, making this an attractive proposition for many. Some example use cases include asset management, code generation, accounting automation.  \u00a0</li> <li>Pattern 4: Lastly, we are bringing a full suite of AI capabilities to the Power platform to help clients train, tune and deploy models without purchasing GPUs. The lead time for GPUs is somewhere around a year and clients will miss out on opportunity if they can\u2019t get started today. Some of the popular use cases that enterprises will use are fraud detection, risk underwriting, and demand forecasting.</li> </ul>"},{"location":"Lab2/lab2-overview/","title":"Lab Overview","text":""},{"location":"Lab2/lab2-overview/#deploy-retrieval-augmented-generation-rag-on-ibm-power10-lab-education","title":"Deploy Retrieval-Augmented Generation (RAG) on IBM Power10 - Lab education","text":"<p>Goal of this lab is to get hands-on experience in deploying a RAG pattern on IBM Power10. Before we do that, lets understand what RAG is, its advantages, key usecases and how it fits in the IBM Power landscape.</p>"},{"location":"Lab2/lab2-overview/#what-is-rag","title":"What is RAG?","text":"<p>RAG (Retrieval-Augmented Generation) is an advanced technique that combines retrieval-based and generation-based approaches to improve the performance of large language models, especially in question-answering and knowledge-intensive tasks. In layman terms, clients can use RAG pattern to generate factually accurate output from LLMs, that is grounded in information in a knowledge base.</p> <p>Key Components of RAG:</p> <ol> <li>Retrieval: The model first retrieves relevant documents or information from a large knowledge base (like Wikipedia or custom databases) based on the user's query. This helps provide the model with factual and up-to-date information that it may not have learned during training.</li> <li>Augmented Generation: Once the relevant documents are retrieved, the model then generates a response based on the query and the retrieved documents. This ensures the generated answer is both relevant and factual, combining the reasoning power of the language model with the accuracy of external knowledge.</li> </ol>"},{"location":"Lab2/lab2-overview/#how-rag-works","title":"How RAG works?","text":"<p>RAG is a technique that uses vector databases to retrieve relevant information and improve the accuracy of Large Language Models (LLMs):</p> <ol> <li>Vector database storage: Text data is converted into vector embeddings using pre-trained models like BERT or GPT. These embeddings are then stored in a vector database.</li> <li>Query conversion: When a query is posed to the AI system, it is also converted into a vector</li> <li>Vector search: The vector database performs a vector search to find relevant embeddings from the stored dataset</li> <li>Information retrieval: The retrieved information is then integrated into the LLM's query input</li> <li>Response generation: The augmented query is sent to the LLM to generate an accurate answer.</li> </ol> <p>Why use vector database in RAG?</p> <p>Vector databases are used in RAG because they store data in a way that makes it easy to search and retrieve. Vector search techniques go beyond keyword matching and focus on semantic relationships, which improves the quality of the retrieved information.</p>"},{"location":"Lab2/lab2-overview/#need-advantages-of-rag","title":"Need &amp; Advantages of RAG","text":"<p>The need for Retrieval-Augmented Generation (RAG) arises from the limitations of current large language models (LLMs) and the growing demands for factual accuracy and knowledge scalability in AI applications. </p> <p>Here are the key reasons why RAG is necessary and its associated advantages:</p> <ol> <li>Handling Knowledge Gaps<ul> <li>LLMs are static: Traditional language models, once trained, cannot access new or external information. They can only generate text based on the data they were trained on, which means they might miss important or up-to-date knowledge.</li> <li>RAG dynamically retrieves information: By incorporating a retrieval step, RAG can pull in relevant, up-to-date documents from external sources to complement the model's output, making it more accurate and current.</li> </ul> </li> <li>Reducing Hallucinations<ul> <li>LLMs sometimes \"hallucinate\": LLMs can generate convincing but incorrect or fabricated answers because they are predicting text based on patterns rather than verifying facts.</li> <li>RAG grounds responses in real data: Since RAG retrieves factual documents before generating a response, it ensures that the output is based on real, verifiable information, reducing the risk of false or misleading content.</li> </ul> </li> <li>Scalability in Knowledge<ul> <li>LLMs are limited by training data: Even the largest models have limitations on how much they can remember from their training data, which might become outdated or incomplete.</li> <li>RAG scales with external data: By leveraging vast external knowledge bases or documents, RAG allows for almost unlimited knowledge expansion without retraining the model. This is particularly useful for enterprises or specific domains where continuous data updates are essential.</li> </ul> </li> <li>Improved Performance in Specific Domains<ul> <li>Specialized knowledge is often needed: Many applications require access to niche or domain-specific information, such as legal texts, scientific papers, or proprietary company data.</li> <li>RAG retrieves domain-specific documents: The retrieval step allows RAG to pull in domain-specific or proprietary documents, making the output more relevant for specialized tasks.</li> </ul> </li> <li>Efficiency and Adaptability<ul> <li>Model retraining is costly: Constantly retraining LLMs with new data is computationally expensive and time-consuming.</li> <li>RAG avoids retraining: By using real-time retrieval, RAG can access the latest information or new content without the need to retrain the entire model, making it more adaptable and cost-efficient.</li> </ul> </li> <li>Better Results for Open-Domain Question Answering<ul> <li>Complex queries require precise answers: In tasks like open-domain question answering, general models might struggle to provide precise answers for complex or rare questions.</li> <li>RAG enhances accuracy: By combining retrieval with generation, RAG can provide more accurate, context-rich answers, drawing from a wide range of documents.</li> </ul> </li> </ol> <p>In summary, RAG addresses limitations in current LLMs by improving factual accuracy, scalability, and adaptability, making it particularly useful for knowledge-intensive tasks and dynamic environments.</p>"},{"location":"Lab2/lab2-overview/#common-usecases-of-rag","title":"Common usecases of RAG","text":"<p>Below are some of the common usecases for RAG:</p> <ol> <li>Open-domain question answering: Where models need to answer questions about a wide range of topics, potentially beyond the training data.</li> <li>Customer support: Providing accurate answers by retrieving relevant documents from knowledge bases.</li> <li>Enterprise AI: Helping businesses with information retrieval, knowledge management, and research by retrieving and summarizing relevant documents.</li> </ol> <p>To summarize, RAG allows models to perform better in knowledge-intensive tasks by combining the strengths of both retrieval systems and generative language models.</p>"},{"location":"Lab2/lab2-overview/#ibm-power-and-rag","title":"IBM Power and RAG","text":""},{"location":"Lab2/lab2-overview/#ibm-power-systems-as-systems-of-record","title":"IBM Power Systems as Systems of Record","text":"<p>IBM Power Systems are renowned for their performance, reliability, and scalability, making them ideal for handling systems of record. A system of record (SOR) refers to a trusted source of truth that stores essential business data and transactions, such as financial systems, customer data, and inventory management. IBM Power Systems are often used for mission-critical applications in industries like banking, healthcare, and government due to their ability to manage large volumes of secure, transactional data.</p>"},{"location":"Lab2/lab2-overview/#how-ibm-power-systems-plays-well-with-rag","title":"How IBM Power Systems Plays Well with RAG","text":"<p>Retrieval-Augmented Generation (RAG) is a generative AI framework that enhances the performance of AI models by retrieving relevant documents from external knowledge bases before generating a response. This retrieval step ensures that the generated output is more accurate and fact-based, as it is grounded in real-time data from a reliable source.</p> <p>IBM Power Systems play exceptionally well with RAG due to the following reasons:</p> <ol> <li>High-Performance Data Handling<ul> <li>Power Systems are designed for high-volume data transactions and processing. This makes them perfect for storing and managing systems of record, which RAG relies on to retrieve real-time, relevant information.</li> <li>When RAG retrieves data from a system of record stored on IBM Power, it benefits from the fast data access and throughput that Power Systems offer, ensuring quick and efficient retrieval of documents for AI processing.</li> </ul> </li> <li>Data Security and Compliance<ul> <li>Power Systems are known for their robust security features, including end-to-end encryption and advanced data protection, making them ideal for storing sensitive data such as customer information, financial records, or healthcare data.</li> <li>In a RAG scenario, where retrieved data is used to generate answers, the ability of IBM Power Systems to ensure data privacy and regulatory compliance (e.g., HIPAA, GDPR) is crucial, especially for enterprises dealing with sensitive or regulated data.</li> </ul> </li> <li>Scalability and Reliability<ul> <li>RAG applications require scalable infrastructure to handle varying levels of computational demand, especially when dealing with large-scale document retrieval and real-time AI processing. IBM Power Systems are built to scale seamlessly, allowing RAG to handle larger datasets and more complex queries without performance degradation.</li> <li>Reliability is critical for systems of record, and Power Systems have a proven track record of uptime and resilience, ensuring that the data RAG retrieves is always available when needed, without risk of downtime affecting the retrieval process.</li> </ul> </li> <li>Integration with AI Workloads<ul> <li>IBM Power Systems are optimized for AI workloads, with features like accelerated AI processing (e.g., Power10's Matrix Math Accelerator (MMA)) that boost the performance of both retrieval and generation tasks in RAG.</li> <li>By running RAG-based applications on Power10, enterprises can take advantage of faster AI inference and improved data handling, resulting in more responsive and accurate AI systems.</li> </ul> </li> <li>Efficient Handling of Structured and Unstructured Data<ul> <li>Power Systems can efficiently handle both structured data (like databases) and unstructured data (such as documents and records), making them ideal for RAG, where both types of data may be retrieved from systems of record.</li> <li>RAG can retrieve structured data for quick reference (e.g., customer records or transactions) and unstructured data (e.g., reports, emails) for more complex, context-driven AI responses. Power Systems' capability to manage both types ensures efficient, accurate information retrieval.</li> </ul> </li> <li>Real-Time Analytics<ul> <li>Real-time data analytics capabilities in IBM Power Systems enable quick access to up-to-date information, which is essential for RAG when generating responses based on current data.</li> <li>This feature allows the RAG model to provide contextualized answers based on the latest transactions or data updates from the system of record, improving the relevance of AI-driven outputs.</li> </ul> </li> </ol> <p>Summary</p> <p>IBM Power Systems provide the speed, security, and reliability needed to store and manage systems of record that RAG models depend on for data retrieval. Power Systems\u2019 advanced capabilities in data handling, AI optimization, scalability, and security make them an ideal infrastructure for supporting RAG-based AI applications, ensuring that retrieved data is accurate, current, and secure, thus enhancing the quality of generative AI outputs.</p>"}]}